


\documentclass[12pt]{article}

\usepackage[margin=1in]{geometry}

\input{../../variables.tex}

\title{%
    \Huge Numerical Algorithms \\
    \Large Lecture XIII
}
\date{2025-06-02}
\author{Michael Saba}

\begin{document}
\pagenumbering{gobble}
\maketitle
\newpage
\setlength{\parindent}{0pt}
\pagenumbering{arabic}

\subsection*{Inverse-Discrete Sampling}

Inverse sampling was done in the continuous case,
where we wanted to get rid of a term $p(x)$
in the intgeral. \\

In the discrete case, we have the probabilities:
\[ \{p_1, p_2 \dots p_n \} \]
Which sum up to $1$. \\
This is the discrete analog to $p(x)$. \\

If the sum of $p_i$ is $M \neq 1$, we just divide
all probabilities by $M$. \\

We define:
\[ P_i = \sum_{k=1}^i p_k \]
And we map the intervals:
\[ \brac{ P_i,  P_{i+1}} \to i+1 \]
to these indices. \\
This is the integration in the continuous case. \\

We then try to fit $u \in [0, 1]$ 
into one of the buckets we have,
using binary search for efficiency
(we can also use linear search), which is our
inversion that we did in the continuous case. \\

The funciton is not injective (piece-wise constant function), 
which is why we can't
invert it, which is why we have buckets that map to
indexes. \\

For the $2$ dimensional case, we have rows and columns,
we can then sum all the rows (or columns),
and get a $1$ dimensional array. \\

Video: My favorite samples | SIGGRAPH courses 1:42:20. \\

\newpage

\end{document}
