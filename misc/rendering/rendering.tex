
\documentclass[12pt]{article}

\usepackage[margin=1in]{geometry}

\input{../../variables.tex}

\title{%
    \Huge Path Tracing
}
\date{2025-07-29}
\author{Michael Saba}

\begin{document}
\pagenumbering{gobble}
\maketitle
\newpage
\setlength{\parindent}{0pt}
\pagenumbering{arabic}

\newpage

\subsection*{Introduction}

In Computer Graphics, several methods are used
in rendering scenes. \\

Rastarization is when a 3 dimensional scene is flattened
(using linear transformations) onto a 2 dimensional
screens. This is fast and efficient, but, since
the shader won't have access to the full 3 dimensional
scene, the lighting of the scene won't look accurate
to real life. \\

In ray tracing, the shader has access to the full 3
dimensional scene. A camera shoots rays onto a pixel
grid, and then traces those rays as they hit the scene. \\
Based on whether they hit an object, or a light source,
or nothing at all, the pixel or fragment is colored 
accordingly. \\

One type of ray tracing is called path tracing; it is
when the rays of light are traced and bounced around
a scene in a way that mimicks the way light bounces
in the real world. \\ 
A path tracer is any algorithm that converges towards
the rendering equation. While slow, it is the most accurate
method. \\

\newpage


\subsection*{The Rendering Equation}

The rendering equation calculates the amount of
outgoing radiance or light from a point $x$
in the outgoing direction $\omega_o$:
\[ L_{o}(x, \omega_o)
= L_{e}(x, \omega_o)
+ \int_\Omega L_{i}(x, \omega_i) \cdot
f(x, \omega_i, \omega_o) \cdot
(\omega_i \cdot n) \; d\omega_i \]
Here, the outgoing radiance is the emitted light
(if the point is a source of light)
plus the light reflected at that point (from other
light sources). \\

When a ray of light in direction $\omega_i$
hits a point $x$, it will be reflected in all 
outgoing directions $w_o$. \\
Likewise, the outgoing reflected ray of light in
direction $\omega_o$ is the result of many incoming
rays of light that hit $x$ and get reflected. \\

Here, $L_{e}(x, \omega_o)$ is the emitted light 
from point $x$ in direction $\omega_o$. \\
It is $0$ on a non-light source, and non-zero
on a light source. \\
 
On the other hand, the integral:
\[ \int_\Omega L_{i}(x, \omega_i) \cdot
f(x, \omega_i, \omega_o) \cdot
(\omega_i \cdot n(x)) \; d\omega_i \]
integrates over all incoming rays of light
of direction $\omega_i$ into point $x$.
Here $\Omega$ is the hemisphere above $x$. \\

So $L_{i}(x, \omega_i)$ is the incoming ray
of light in direction $-\omega_i$. \\

And $f(x, \omega_i, \omega_o)$ is the BRDF, 
which is the amount of light from direction $\omega_i$
that gets reflected in direction $\omega_o$
at point $x$.
This will depend on the material that $x$ is made of. \\
The BRDF allows us to achieve effects such as reflectivity,
roughness, glossiness... \\ 

Finally, $\omega_i \cdot n$ is the dot product,
which basically gives us the cosine of the angle $\theta$
between the incoming light direction and the normal
$n(x)$ at $x$, which scales how much light is reflected
(Lambert's cosine law).
It's the cosine of the angle because we 
assume both directions $n$ and $\omega_i$
have length $1$. \\
We can also sometimes write it as $\cos(\theta)$. \\

By convention, when we say $L_i(x, \omega_i)$,
we actually mean the coming into $x$
from direction $-\omega_i$
($\omega_i$ shoots out of $x$ towards the source). \\
This is by convention, and it is a bit confusing. \\

The scene, or geometry,
$\mcal$, is basically made up of surfaces defined
by some equations:
\[ \mcal = \unionof{i=1}{n} S_i \]
where $S$ is the set of the surfaces. \\

The function $N:\mcal \to \rbb^d$
gives the normal $N(x)$ at any point $x$ on $\mcal$. \\

We can imagine that the screen on which we view the
scene is the sensor, that captures rays of light 
$L_o(x, \omega_o)$ and then showcases the correct pixel
color depending on which rays hit which pixels. \\
The rays originate in the light sources and bounce around
until they hit the sensor. \\
However, the probability that rays will hit the sensor
is very low, so we usually do the operation in the
opposite direction, and shoot light rays from the sensor
until they hit the light sources,
and then do the calculation in reverse. \\

We have a ray tracing function $x_\mcal(x, \omega)$
or $r(x, \omega)$, which basically shoots a ray
from $x$ in direction $\omega$. We use it extensively
in order to figure out where light rays go and come from. \\

As mentioned earlier, the incoming light $L_i(x, \omega)$
is the ray of light that leaves $y$ in the direction $-\omega$
and hits $x$.
We can write $y$ as $r(x,\omega)$
since we can get $y$ by shooting a ray in the direction
$\omega$ from $x$ (recall that $L_i(x, \omega_i)$
has an incoming ray in direction $-\omega_i$). \\
So we can write:
\[ L_i(x, \omega) = L_o(r(x, \omega), -\omega) \]
Which means that we write $L_i$,
the light coming into a point $x$ from some direction,
as the light $L_o$, coming out of a point in
the opposite direction towards $x$. \\

This is called the conservation of enegery along the ray,
and it states that:
\[ L_i(x, \omega) = L_o(r(x, \omega), -\omega) \]
This tells us we can write the rendering equation recursively
in terms of outgoing light only (using a ray tracer). \\

Note that $L_i(x, \omega) \neq L_o(x, \omega)$,
since light leaving a point $x$ isn't
necessarily equivalent to the amount of light
entering it.
Outgoing light does depend on incident light,
but it also depends on things like emitted light $L_e$
and angle and surface as we saw in 
the rendering equation. \\

There is an issue with the rendering equation:
\[ L_{o}(x, \omega_o)
= L_{e}(x, \omega_o)
+ \int_\Omega L_{i}(x, \omega_i) \cdot
f(x, \omega_i, \omega_o) \cdot
(\omega_i \cdot n) \; d\omega_i \]
We define the light sources, 
which give us $L_{e}(x, \omega_o)$.
We also define the BRDF $f(x, \omega_i, \omega_o)$.
The geometry of the scene gives us $\omega_i \cdot n$. \\
But what about $L_{i}(x, \omega_i)$?
How can we calculate the incident light
at a point $x$? \\

Now, recall that we mentioned that:
\[ L_i(x, \omega) = L_o(r(x, \omega), -\omega) \]
Which means that the incoming light at a point
is just the outgoing light from the point
we get if we just follow the light ray
in the opposite direction until we hit
an intersection with a surface (light source
or otherwise). \\
This implies that we can write the rendering
equation as such:
\[ L_{o}(x, \omega_o)
= L_{e}(x, \omega_o)
+ \int_\Omega L_o(r(x, \omega_i), -\omega_i)  \cdot
f(x, \omega_i, \omega_o) \cdot
(\omega_i \cdot n) \; d\omega_i \]
Which means that the incident light at $x$
is just the outgoing (exitant) light 
from some point $r(x, \omega_i)$.
And that outgoing light $L_o(r(x, \omega_i), -\omega_i)$
can itself be calculated using the rendering
equation, since it by definition
gives us the value of the exitant randiance
in some direction $\omega_o$. \\

And this implies a sort of recursion,
where the outgoing light at one point $x$
depends on the outgoings lights from other points $y$,
which themselves depend on the outgoing light
from other points... \\
This creates a recursion, where each recursion
is a light bounce on a surface. \\

Each recursion of the rendering equation
is one bounce of light. \\
Energy needs to be conserved, since otherwise light would
bounce around forever and the scene keeps getting brighter;
this law is obeyed if the BRDF is well defined. \\
In partice, what this means is that with each bounce
away from the light, more and more light is asbosrbed,
and less light is being contributed; we can truncate
the sum after a few bounces, when the energy being
added is near $0$. \\

Now in order to explicitely write out the the rendering
equation as a series of bounces, we first have to
show we to resolve the recursions.
We have:
\[ L_{o}(x, \omega_o)
= L_{e}(x, \omega_o)
+ \int_\Omega L_o(r(x, \omega_i), -\omega_i)  \cdot
f(x, \omega_i, \omega_o) \cdot
(\omega_i \cdot n) \; d\omega_i \]
We can replace $L_o(r(x, \omega_i), -\omega_i)$
by the rendering equation:
\[ L_o(r(x, \omega_i), -\omega_i)
= L_{e}(r(x, \omega_i), -\omega_i)
+ \int_\Omega L_i(r(x, \omega_i), \omega_j) \cdot
f(x, \omega_j, -\omega_i) \cdot
(\omega_j \cdot n) \; d\omega_j \]
And then repeat, until we have:
\multiline{
L_{o}(x, \omega_o)
& = L_{e}(x, \omega_o) \\
& + \int_\Omega \bigg( L_e(r(x, \omega_i), -\omega_i)
+ f(x, \omega_i, \omega_o) \cdot 
(\omega_i \cdot n) \\
& + \int_\Omega 
\bigg( L_e(r(r(x, \omega_i), \omega_j), -\omega_j) 
+ f(x, \omega_j, -\omega_i) \cdot 
(\omega_j \cdot n) \\
& + \int_\Omega \bigg( 
L_e(r(r(r(x, \omega_i), \omega_j), \omega_k), -\omega_k)
+ f(x, \omega_k, -\omega_j) \cdot 
(\omega_k \cdot n) \\ 
& + \int_\Omega \dots \bigg)
\; d\omega_k \bigg) \; d\omega_j \bigg) \; d\omega_i }
Each integral expresses a bounce of the light. \\

\newpage

\subsection*{Neumann Series}

Generally speaking, if we have an integral:
\[ f(x) = \integral{a}{b}{K(x, y)g(y)}{dy} \]
We can approximate it as a Riemann sum,
that is, discretize it
by taking a sample of size $n$ of columns,
which we multiply by small steps $\Delta y$:
\[ f(x_j) \sim \sumof{i = 1}{n} (K(x_j, y_i)g(y_i) \Delta y) \]
Note here that if we envision
a vector $g$ that contains all values of $g(y_i)$
for each $i \in \{1, 2, \dots n\}$,
then we can rewrite this as a matrix multiplication:
\[ f = K \cdot g \]
And this is how we discretize an integral and write
it as a matrix multiplication. \\

For the rendering equation, we introduce the term $T$,
which is the integral operator. When the space
is discretized, this becomes a matrix:
\[ TL_o =  \int_\Omega L_o(r(x, \omega_i), -\omega_i)  \cdot
f(x, \omega_i, \omega_o) \cdot
(\omega_i \cdot n) \; d\omega_i \]
But notice that if we were to use $T$
to express the result we found in the last section, we
would get:
\[ L_o = L_e + T( L_e + T( L_e + T (L_e + \dots))) \]
\[ L_o = L_e + TL_e + T^2L_e + T^3L_e \dots \]
Which is called the Neumann series of the rendering equation. \\

Here each term $T^kLe$ expresses one bounce of light. \\

An alternative way to derive the Neumann series
is to note that, since $T$ behaves like a matrix
when the space is discretized, we can rewrite
the rendering equation like this:
\[ L = L_e + TL \]
\[ L - TL = L_e \]
\[ (I - T)L = L_e \]
\[ L = (I-T)\inv L_e \]
Here, the Neumann expansion of $(I-T)\inv$ is:
\[ (I-T)\inv = \sum_{i=0}^\infty T^k \]
So we end up with:
\[ L = \sum_{i=0}^\infty T^kL_e \]
Which is the same Neumann series we derived earlier. \\

To prove that the Neumann expansion is correct,
we can use the eigendecomposion of $T$:
\[ T = P\Lambda P\inv \]
Which means that:
\multiline{ (I - T)\inv = (I - P\Lambda P\inv)\inv
& = (PIP\inv - P\Lambda P\inv)\inv 
= (P (I - \Lambda) P\inv)\inv \\
& = P (I - \Lambda)\inv P\inv }
Note that $I - \Lambda$
is a diagonal matrix with diagonal entries
$1 - \lambda_i$. This means that
we can distribute the inverse (since it is
diagonal) and get:
\[ P \bmat{ 
\dfrac{1}{1-\lambda_1} & 0 & \dots & 0 \vs{5} \\
0 & \dfrac{1}{1-\lambda_2} & \dots & 0 \vs{5} \\
\vdots & \ddots & \ddots & \vdots \vs{5} \\
0 & \dots & 0 & \dfrac{1}{1-\lambda_n} } P\inv \]
On the other hand, we have:
 \[ \sum_i^{\infty} T^i
= \sum_i^{\infty} (P\Lambda P\inv)^i \]
Notice that because this is the eigen decomposition
and $P$ is orthogonal,
that $(P\Lambda P\inv)^i = P\Lambda^i P\inv$.
So we have:
\[ \sum_i^{\infty} P\Lambda^i P\inv
= P \para{\sum_i^{\infty}\Lambda^i} P\inv \]
Notice that because $\Lambda$ 
is diagonal, we can distribute both the 
sum and the power to the diagonal elements:
\[ P \bmat{ 
\sum_i^{\infty} \lambda_1^i & 0 & \dots & 0 \vs{5} \\
0 & \sum_i^{\infty} \lambda_2^i & \dots & 0 \vs{5} \\
\vdots & \ddots & \ddots & \vdots \vs{5} \\
0 & \dots & 0 & \sum_i^{\infty} \lambda_n^i } P\inv \]
If we assume that $|\lambda_i| < 1$,
we can write the geometric sums
as the value they converge to:
\[ P \bmat{ 
\dfrac{1}{1-\lambda_1} & 0 & \dots & 0 \vs{5} \\
0 & \dfrac{1}{1-\lambda_2} & \dots & 0 \vs{5} \\
\vdots & \ddots & \ddots & \vdots \vs{5} \\
0 & \dots & 0 & \dfrac{1}{1-\lambda_n} } P\inv \]
Which proves  $(I-T)\inv =  \sum_i^{\infty} T^i$,
so long as $|\lambda_i| < 1$ for all $i$. \\

Note that the condition $|\lambda_i| < 1$ is automatically
satisfied when the BRDF is enegery conserving, that is,
when the scene conserves energy and bounces gradually
lose radiance. \\

If after $K$ bounces, we have:
\[ \sum_{i=0}^{\infty} T^i \approx \sum_{i=0}^{K} T^i \]
We can truncate the sum and stop bouncing light. \\

\newpage

\subsection*{The Area Formulation}

Instead of integrating over all the directions
$\omega$ in $\Omega$, the hemisphere
above $x$,
then we can use an area formulation;
The idea behind it is to integrate
in terms of all the points $y$
from which a light ray can be projected
towards $x$. \\

We can take each small area $d\sigma(\omega)$
on the hemisphere and project them.
They will hit points on the surfaces
of the scene $\mcal$. \\

We would then have $d\sigma(\omega)
= \dfrac{\cos(\theta)y}{\|x-y\|^2}dA(y)$
where $y$ is the point intersected
when we project from $x$ in the $\omega$
direction, and $dA(y)$ is a small area around it
on the surface in $\mcal$. \\

We can absorb these terms into the
geometry term $G(x, y)$:
\[ G(x, y) = \dfrac{((y-x)\cdot N(x))((x-y)\cdot N(y))}
{\|x-y\|^4} \]
Note that we also added the term $(y-x)\cdot N(x)$
which also appears in the original integral. \\

When using solid angles, we have a ray tracing 
function to check the first point intersected
by a ray of light, so we could always be sure
that the point $r(x, \omega_i)$ is visible
from $x$. \\
This guarantee does not exist in the area formulation,
where we pick two points $x$ and $y$ from
the scene. \\
We instead use a visibility function $V(x, y)$:
\[ V(x, y) = \piecewise{
    1 \qquad \IF r(x, ) \\
    0 \qquad \ELSE
} \]
It ensures that $y$ is visible from $x$
without obstruction. \\

So instead of:
\[  L_{o}(x, \omega_o)= L_{e}(x, \omega_o) +
\int_\Omega L_{i}(x, \omega_i) \cdot
f(x, \omega_i, \omega_o) \cdot
(\omega_i \cdot n) \; d\sigma(\omega_i) \]
We will have:
\[  L_{o}(x \to z)= L_{e}(x \to z) +
\int_\mcal L_{i}(x \leftarrow y) \cdot
f(x, \vec{xy} \to \vec{xz}) G(x, y) V(x, y) \; dA(y) \]
So we now have the same integral
for the rendering equation, but instead
of thinking in terms of directions, we think
in terms of points on the scene $x$ and $y$,
and light bouncing between them. \\
So we can get the outgoing light $L_o$
at a point $x$ in the direction towards a point $y$,
which is $0$ if the visibility function
determines we can't see $x$ from $y$ and vice-versa. \\

Everything we said in regards to solid angles still
applies in the area formulation of the rendering
equation. \\

Using the area formulation, we can express
each bounce of light $T^kL_e$ as:
\multiline{
    T^k L_e &= \int_{\mathcal{M}^k} 
    L_e(x_{k+1} \to x_k) \\
    & \prod_{k=1}^{n} 
    \left[ f(x_{k-1}, \overset{\rightarrow}{x_k x_{k+1}} 
    \to \overset{\rightarrow}{x_k x_{k-1}}) 
    \cdot G(x_k, x_{k+1}) \cdot V(x_k, x_{k+1})
    \; dA(x_{k+1}) \right]
}
Where $\int_{\mcal^k}$ is $n$ nested integrals
each over $\mcal$. \\

\newpage

\subsection*{Monte-Carlo Integration}

Monte-Carlo integration is a numerical
algorithm that allows us to approximate
integrals efficiently. \\

Why do we use Monte-Carlo
integration, instead of just using any
other integration method, like Riemann
Sums or the Trapezoidal rule? \\
These methods work fine for a small
dimension, but as the dimension of a scene
increases, they become less and less efficient. 
For example, Riemann sums use rectangles in
$\rbb^2$, but columns in $\rbb^3$.
Recall that some of our integrals
$T^kL_e$ have dimensions $k$,
which can grow very large.
Monte-Carlo integration is probabilistic
and has far less of a dependency
on the dimension. \\

Now for some motivation;
we know that the average is just the
sum divided by the number of elements:
\[ \mu(x) = \dfrac{1}{n}\sum_{i=1}^n x_i \]
Intuitively, this means that we can write
the sum as the average multilplied the number
of elements:
\[ n\mu(x) = \sum_{i=1}^n x_i \]
This is the intuition behind Monte-Carlo
integration. \\

As we know, integration is just a sum,
over an interval instead of a number of elements.
So we can do the same thing here:
\[ \mu(x) = \dfrac{1}{b-a}\int_a^b x \; dx \]
Which means that:
\[ (b-a)\mu(x) = \int_a^b x \; dx \]
Here we use the measure (length)
of the interval, since we don't have a number
of elements, in order to calculate the intergal
using the average. \\

In a more general case, if we have:
\[ \int_\dcal f(x) \; dx \]
Then we would need the generalization
of the volume or length to $\dcal$,
whatever dimension $d$ it may be,
which we can write as $v_d(\dcal)$. \\
So we can approximate the integral as:
\[ \int_\dcal f(x) dx
= v_d(\dcal)\mu(f(x)) \]
Where we can get the average by just taking
$N$ random samples and dividing by $N$:
\[ \int_\dcal f(x) dx
= \dfrac{v_d(\dcal)}{N} \sum_{i=1}^N f(x_i) \]
This is how Monte-Carlo integration works. \\

However, one trick we can use
is to map the coordinates int $\dcal$
to a $d$-dimensional unit hypercube,
which always has a measure of $1$.
In $\rbb^2$ this is a unit square,
and in $\rbb^3$ this is a unit cube. \\
We can write the unit hypercube in $\rbb^d$ 
as $[0, 1]^d$. \\
So if we can transform the variables
such that they span a hypercube,
we will have:
\[ \int_{[0, 1]^d} f(u) du
= \dfrac{1}{N} \sum_{i=1}^N f(u_i)  \]
Where the volume of the space is just $1$. \\

So, how can we do this mapping?
We can show it using an example. \\
Suppose we have an integral over a hemisphere:
\[ \int_\Omega f(\omega) \; d\sigma(\omega) \]
Which uses solid angles ($\sigma(\omega)$). \\
Now, we can use a mapping:
\[ T: [0, \sfrac{\pi}{2}] \times [0, 2\pi] \to \Omega \]
Which maps the angles to parametric coordinates:
\[ T(\phi, \theta) = \pmat{
    \cos(\phi)\sin(\theta) \\
    \sin(\phi)\sin(\theta) \\
    \cos(\theta)
} \]
Notice that the new coordinate space is a rectangle,
since we have $\theta \in [0, \sfrac{\pi}{2}]$
and $\phi \in [0, 2\pi]$. \\
Now note that the infinitessimal $d\sigma(\omega)$
will change to this when we perform the transformation:
\[ d\sigma(\omega) = |\det(J_T)| \; d\phi \; d\theta \]
Where $J_T$ is the Jacobian of the transformation,
and $\det(J_T)$ is calculated the same way
we calculate the determinant of any rectangular
matrix:
\[ \det(J_T) = \sqrt{(J_T)^TJ_T}
\qquad \AND \qquad J_T = \pmat{
-\sin\theta \sin\phi & \cos\theta \cos\phi  \\
\sin\theta \cos\phi  & \cos\theta \sin\phi\\
0 & -\sin\theta } \]
So in total, we get:
\[ \int_\Omega f(\omega) \; d\sigma(\omega)
= \int_0^{2\pi} \int_0^{\sfrac{\pi}{2}} 
f(T(\phi, \theta)) \cdot |\det(J_T)| 
\; d\theta \; d\phi\]
Which is the integration with a change
of variables. \\

Now, this is a rectangle with length $2\pi$
and width $\dfrac{\pi}{2}$,
and we need a unit square,
so we can just do a second transformation,
this time:
\[ F: [0, 1]^2 \to [0, \sfrac{\pi}{2}] \times [0, 2\pi] \]
Where:
\[ F(u, v) = \pmat{2\pi u \vs{10} \\ \dfrac{\pi}{2}v} \]
Which will also have a Jacobian $J_F$,
and which gets us a new integral:
\[ \int_0^{1} \int_0^{1} 
f(T(F(u, v))) \cdot
|\det(J_T)| \cdot
|\det(J_F)| \; du \; dv \]
Which we can also write as:
\[ \int_{[0, 1]^2}
f(T(F(u, v))) \cdot
|\det(J_T)| \cdot
|\det(J_F)| \; d(u, v) \]
Where $[0, 1]^2$
has area that is just $1$. \\

Note that the transformation has to be 
differentiable for this change of variables
to work. \\

So in short, we have:
\multiline{ 
\int_\Omega f(\omega) \; d\sigma(\omega)
& = \int_0^{1} \int_0^{1} 
f(T(F(u, v))) \cdot
|\det(J_T)| \cdot
|\det(J_F)| \; du \; dv \\
&\approx \dfrac{1}{N} \sumof{i = 1}{N}
f(T(F(u_i, v_i))) \cdot
|\det(J_T)| \cdot |\det(J_F)| }
Which is Monte-Carlo integration in a nutshell. \\

Note that sampling points from $[0, 1]^d$
is very easy; we just find $d$ random numbers
between $0$ and $1$. \\

Monte-Carlo integration has a convergence rate
of $O\para{\dfrac{1}{\sqrt{N}}}$, which makes it
rather efficient when the number of samples
and dimensionality is high. \\
Other methods, like the Trapezoidal rule or Riemann
sums scale far worse in higher dimensions. \\

\newpage

\subsection*{Surfaces}

Surfaces of any dimension, include two,
can be expressed parametrically or implicitely. \\

A parametric function has the form $g(\alpha)$,
where $\alpha$ is a set of $n$ $d$-dimensional 
parameters. Every point $g(\alpha)$ for any 
$\alpha \in \rbb^{n \times d}$ is a point on the
surface $S$ defined by $g$. \\

An implicit function on the other hand,
has the form $f(p) = 0$,
where $p$ is a $d$-dimensional point. 
Every point $p$ that satisifes 
$f(p) = 0$ $p$ is on the surface $S$ defined by $f$. \\

Parametric functions are great for generating
points, though it can be difficult to check 
if a point belongs to the surface 
(we would need to check if a set of
parameters $\alpha$ generate the point). \\

On the other hand, implicit functions are great
for checking for membership (we just need to
plug the point into the function and check
if it evaluates to $0$). But it can be difficult
to generate points on the surface, as we
can't know before evaluating the function
whether or not a point belongs to the surface. \\

A ray can be parametrically defined as:
\[ t(t) = o + td \]
Where $o \in \rbb^d$ is the origin of the ray,
$d \in \rbb^d$ is the direction (assumed to be
normalized with $\|d\| = 1$), and $t \in \rbb$
is a parameter that signifies how far along
the ray a point is. \\

In order to check if ray intersects a
surface defined by a parametric function
$g(\alpha)$, recall that both the ray and
surface are parametrically defined, so
they are both functions that generate points
on their respective surfaces. \\
So to check for an intersection, we just try to
solve the equation:
\[ g(\alpha) = r(t) \]
for the parameters $\alpha$ and $t$.
For instance, this may lead to linear system,
where $\alpha$ and $t$ are unknowns. \\
If we can solve the equation (for example,
if the linear system has a unique solution),
then we know there is an intersection, and
we have the parameters that will give us the
intersection point if we plug them into
$r(t)$ or $g(\alpha)$. \\

In order to check for an intersection between
a ray and an implicitely defined surface,
given by $f(p) = 0$,
we can just plug the ray's parametric equation
(which generates points) into the implict function:
\[ f(r(t)) = 0 \]
This equation may be a polynomial equation with $t$
as the variable. \\
We know that if the above equation is satisfied
for some $t = t_0$, then the point $r(t)$
must be on the surface, and as such must be
an intersection between the ray and the surafce. \\
If the equation is not solvable, then there
is no intersection. \\
Generally, if the shapes allow for more than one
intersection, such as a sphere, then the equation
will reflect that (the equation could be
quadratic with two possible solutions). \\

As a general rule, we only consider solutions
with $t > 0$, since rays have a forward direction,
and we only want intersections that take place
in that direction (in front of the ray). \\

Sometimes, we need to transform objetcs in a
scene, which we can do by transforming the surfaces,
using both a linear transformation encoded in
a matrix, $A$, and a translation $b$, \\
We can encode both in a square homogeneous matrix:
\[ \bmat{A & b \\ 0^\top & 1} \bmat{p \\ 1} \]
Which we can apply to a point $p$ by multiplying
the homogeneous matrix by a padded vector
(in homogeneous coordinates). \\

Suppose we have
a parametrically defined set (surface):
\[ \mathcal{M}_\varphi = 
\{ \varphi(s) \in \rbb^n \mid 
s \in \mathcal{D} \subseteq \rbb^k \} \]
In order to transform this set,
we just transform every point in it.
As we know, parametric equations are great at
generating the points of the surface,
so this is as simple as just transforming the
generated points. \\
We can thus apply the affine transformation $Ap + b$:
\[ A\mathcal{M}_\varphi + b
= \{ A\varphi(s) + b\in \rbb^n \mid 
s \in \mathcal{D} \subseteq \rbb^k \} \]
So we just apply the transformation
to each point on the set.
We denote the transformed set or surface
$\mathcal{M}_{A\varphi + b}$. \\

Now, if we want to intersect the resulting
transformed surface with a line,
we would solve:
\[ r(t) = A\mathcal{M}_\varphi + b \]
Another way we could have solved the same
equation was apply the inverse transformation
to the line $r(t)$
and intersect it with the original surface:
\[ A\inv(r(t) - b) = \mathcal{M}_\varphi \]
All we've done is applied the inverse transformation
to both sides. \\

On the other hand, suppose we have an implicitely
defined surface:
\[ \mathcal{M}_{f\inv(0)}
= \{ p \in \rbb^n \mid f(p) =0 \} \]
which is just the set of points $p$
such that $f(p) = 0$ is satisfied. \\
This makes it easy to check for membership,
but harder to generate points,
which makes transforming it slightly more
complicated:
\[ A\mathcal{M}_{f\inv(0)} + b
= \{ Ap + b \in \rbb^n \mid f(p) =0 \} \]
Basically it is the set of transformed points 
$Ap + b$ from the original surface definition,
so the condition to satisfy is still $f(p)$. \\
The only issue is that this defintion isn't
really implicit the way we defined one. \\
If we define $q = Ap + b$
and $p = A\inv(q - b)$ can thus rewrite
the set in the original format of
an implicit surface:
\[ A\mathcal{M}_{f\inv(0)} + b
= \{ q \in \rbb^n \mid f(A\inv(q - b)) = 0 \} \]
but this time with a new function,
which is the composition of $f$ and the inverse of the
affine transformation into one function:
\[ f \circ A\inv \circ T(-b) \]
which means that:
\[ A\mathcal{M}_{f\inv(0)} + b
= \{ q \in \rbb^n \mid 
(f \circ A\inv \circ T(-b))(q) = 0 \} \]
We can thus denote the transformed set or surface
$\mathcal{M}_{(f \circ A\inv \circ T(-b))\inv(0)}$. \\

The same thing applies regarding the
light intersection. \\
We could just replace $p$
by $r(t)$
inside of the transformed set 
$A\mathcal{M}_{f\inv(0)} + t$. \\
Or we could use the original set
$\mathcal{M}_{f\inv(0)}$
and replace $p$
by the line transformed by the inverse of
the affine transformation: 
$A\inv(o + \tau d - t)$. \\

One last thing to note regarding surfaces
is that, if we have disjoint sets (surfaces):
\[ S_1, S_2, \dots S_n \]
And defined $S$ as the union of these sets:
\[ S = \bigcup_{i = 1}^n S_i \]
Then an integral over $S$
can be decomposed into a sum of integrals
over the original list of sets:
\[ \int_S f(t)\;dt
= \int_{S_1} f(t)\;dt + \int_{S_2} f(t)\;dt +
\dots \int_{S_n} f(t)\;dt \]
This can be used when we have a scene
$\mcal$ which is made up of several surfaces
$S$, be they parametric or implicit. \\

Usually, scene data is encoded as a vector
of triplets of vertices representing triangles. \\
These triangles can be seen as our disjoint
sets of surfaces. \\
We can also procedurally generate the surface
data in the shaders themselves, though that is
less efficient. \\

\newpage

\subsection*{Full Path Tracer}

Light travels from the source, bounces around the
scene accumulating colors, and finally hits a pixel
on the sensor, whcih determines the final color
of that part of the screen. \\

As mentioned, in practice, we send rays of
light out of the sensor, and then bounce it around
the scene until we truncate it. \\

The scene is made up of multiple objects, which
can be stored as an array of implicit or parametric
surfaces, along with a ray-surface intersection
routine that we will need to bounce rays
around the scene. We will also need the
surface normals at every point on the object. \\

Then, for each pixel on the sensor (screen), we will
send the first ray in the direction of of the view ray
(that is, following the direction from the camera position
to the pixel on the camera sensor/screen), and then choose
random points in the scene to bounce the light rays
around, which we will repeat until we have
enough bounces, hopefully hitting some light sources in
the process. \\

Using the area formulation and the Neumann expansion, 
each bounce will have the form of an integral:
\multiline{
    T^k L_e &= \int_{\mathcal{M}^k} 
    L_e(x_{k+1} \to x_k) \\
    & \prod_{k=1}^{n} 
    \left[ f(x_{k-1}, \overset{\rightarrow}{x_k x_{k+1}} 
    \to \overset{\rightarrow}{x_k x_{k-1}}) 
    \cdot G(x_k, x_{k+1}) \cdot V(x_k, x_{k+1})
    \; dA(x_{k+1}) \right]
}
The first step will be to parmatrize the bounds of the
integrla such that they form a series of unit hypercubes
with a volume of $1$, using some change of variables
\[ T: \mcal^k \to [0, 1]^d \]
This will allow us to more easily sample points
for the Monte-Carlo estimator we will be using. \\

Note that in pratice, we won't have $\mcal$;
instead, we will have a sum of integrals 
over disjoint sets each
representing a surface, such as a triangles:
\[ \mcal^k = \bigcup_{i=1}^l S_i^n \]
So the integral will look like:
\[ \int_{\mcal^k} f(t)\;dt
= \int_{S_1^n} f(t)\;dt + \int_{S_2^n} f(t)\;dt +
\dots \int_{S_l^n} f(t)\;dt \]
We can then parametrize each set $S_i^n$ individually. \\

Note that when we have the scene parametrized
with parameters in $[0, 1]^d$, it also becomes
much easier to sample random points on the scene,
as we can just generate $d$ random floating point
numbers between $0$ and $1$. \\

When we have multiple surfaces,
and we are currently on surface $S_i$,
what we can do is first choose one random surface
$S_j$ such that $j \neq i$, and then randomly
sample a point on that surface. \\

This will give us an integral:
\multiline{
    T^k L_e &= \int_{[0, 1]^d} 
    \sqrt{\det(J_T^\top J_T)}L_e(u_{k+1} \to u_k) \\
    & \prod_{k=1}^{n} 
    \left[ f(u_{k-1}, \overset{\rightarrow}{u_k u_{k+1}} 
    \to \overset{\rightarrow}{u_k u_{k-1}}) 
    \cdot G(u_k, u_{k+1}) \cdot V(u_k, u_{k+1})
    \; dA(u_{k+1}) \right]
}
Where $u_i = T(x_i)$ for all $i$, and $J_T$
is the jacobian of the transformation. \\

This gives us an estimator:
\multiline{
    \widehat{I_k} &= \dfrac{1}{N}\sum_{i=0}^{N}
    \sqrt{\det(J_T^\top J_T)}L_e(u_{k+1, i} \to u_k) \\
    & \prod_{k=1}^{n} 
    \left[ f(u_{k-1}, \overset{\rightarrow}{u_k u_{k+1}} 
    \to \overset{\rightarrow}{u_k u_{k-1}})  
    \cdot G(u_k, u_{k+1, i}) \cdot V(u_k, u_{k+1, i})
    \right]
}
For the $\nth{k}$ bounce. \\

So to calculate the color or radiance of a single pixel,
we can use the full Neumann Series, with each term
being estimated using Monte-Carlo integration:
\[ L = \sum_{k=0}^\infty \widehat{I_k} \]
\multiline{
    L &= \sum_{k=0}^\infty \bigg[ \; 
    \dfrac{1}{N}\sum_{i=0}^{N} \bigg(
    \sqrt{\det(J_T^\top J_T)}L_e(u_{k+1, i} \to u_k) \\
    & \prod_{k=1}^{n} 
    \left[ f(u_{k-1}, \overset{\rightarrow}{u_k u_{k+1, i}} 
    \to \overset{\rightarrow}{u_k u_{k-1}}) 
    \cdot G(u_k, u_{k+1, i}) \cdot V(u_k, u_{k+1, i})
    \right] \bigg) \bigg]
}
Conceptually, what this means is that, for each
bounce $k$, we generate $N$ ray samples,
each representing a different $\nth{k+1}$ bounce,
and then for each of those, we generate
$N$ ray samples representing the $\nth{k+2}$ bounce... \\

The issue with this approach is that it is inefficient
and not especially well suited for optimization.
By the linearity of expectation, we can flip
the two sums, and still converge towards the same 
result:
\multiline{
    L &= \dfrac{1}{N}\sum_{i=0}^{N} \bigg( \; 
    \sum_{k=0}^\infty \bigg[
    \sqrt{\det(J_T^\top J_T)}L_e(u_{k+1, i} \to u_k) \\
    & \prod_{k=1}^{n} 
    \left[ f(u_{k-1}, \overset{\rightarrow}{u_k u_{k+1, i}} 
    \to \overset{\rightarrow}{u_k u_{k-1}}) 
    \cdot G(u_k, u_{k+1, i}) \cdot V(u_k, u_{k+1, i})
    \right] \bigg] \bigg)
}
In other words, each Monte-Carlo sample is one full
path of a single ray; for each sample $i$,
we shoot a ray, bounce it around the scene,
truncate the bounces, and then return the final answer. \\

The reason why this is more efficient is that we 
don't have to generate all $N$ needed samples in a 
single frame; for instance we can generate $4$
Monte-Carlo samples per frame, and then add
this frame's samples to the last, meaning the lighting
becomes more and more accurate as time goes on.
We can use this formula to add $n$ samples to an average
which had previously been calculated using $N$
older samples:
\[ \text{new average} = 
\dfrac{\text{average} \times N + 
\text{new samples}}{N + n} \]
Note however that if the scene changes, such as by moving
the camera, many if not most of the accumulated
pixel averages will have to be recalculated,
as the old averages are no longer relevant. \\

Note that the term:
\[ \prod_{k=1}^{n} 
\left[ f(u_{k-1}, \overset{\rightarrow}{u_k u_{k+1, i}} 
    \to \overset{\rightarrow}{u_k u_{k-1}})  \cdot 
G(u_k, u_{k+1, i}) \cdot V(u_k, u_{k+1, i}) \right] \]
which is called the Throughput,
does not need to be recomputed each bounce;
we can just save the product in a variable
and then multiply it by the $\nth{k}$
operands each bounce. \\

Note that the first ray of light, called the primary
ray, is not randomly sampled; it is always
the ray shot from the camera position onto the scene
in the direction of the current pixel on the grid. \\
This is equivalent to the first bounce in the Neumann
expansion, $T^0L_e$, where we only have the contribution
of $L_e$, that is, no BRDF or geometry term. \\

If at any point along a single path, the visibility
function evaluates to $0$, the rest of the light
bounces along this path starting from the current one
will all also be $0$. \\
We can truncate the path at this point, and count only
the previous bounces. \\
A similar thing happes when using solid angles;
while we don't run the risk of having an obstruction,
we do run the risk of shooting a ray into the void,
in which case we also truncate the path. \\

Recall that we reparametrized integrals over each
surface as an integral over a $d$-dimensional hypercube,
allowing us to easily randomly sample points on
said surface. \\
However, we migth have a union of multiple surfaces.
Sampling a point thus requires first choosing a surface,
then sampling it. To ensure true randomness, 
the likelihood of choosing a surface should be
proportional to the surface area itself. \\
We can achieve this by calcualting the surface areas
of each object, as well as the total surface area,
to build a probability density function. \\
If we then sample a random number from say, $0$ to $1$,
we can use the dencity function to determine which
surface the number points to. \\

Note that sampling using angles on the hemisphere 
instead of picking random can lead to bias. \\

\newpage

\subsection*{BRDF}

There are multiple algorithms that can be used define
BRDFs. \\
The one I will be using is the Cook-Torrance BRDF.
It combines diffuse and specular components, 
and accounts for surface roughness, 
fresnel effects, and geometric shadowing and masking. \\

The BRDF, in the area formulation, will take as input
three points $x_1$, $x_2$, and $x_3$,
where $x_3$ is the point of contact, and
the normalized incident and exitant directions are:
\[ \omega_i = \dfrac{(x_2 - x_1)}{\|x_2 - x_1\|} 
\qquad \AND \qquad 
\omega_o = \dfrac{(x_2 - x_3)}{\|x_2 - x_3\|} \]
We will use these in the rest of the algorithm. \\

Given the surface point $x_2$ with normal $n$, 
an incoming light direction $\omega_i$, 
and an outgoing direction $\omega_o$, 
we define:
\[ h = \frac{\omega_i + \omega_o}
{\|\omega_i + \omega_o\|} \]
as the halfway vector (in between the two). \\

The full Cook-Torrance BRDF is given by
\[f_r(\omega_i, \omega_o) = (1 - F(\omega_i, h)) 
\cdot \frac{c_d}{\pi}
+ \frac{D(h) \cdot F(\omega_i, h) \cdot 
G(\omega_i, \omega_o)}{4 (n \cdot \omega_i)
(n \cdot \omega_o)} \]
where $c_d$ is the base color or texture at $x_2$, 
and $ F(\omega_i, h)$ is the fresnel term, 
$D(h)$ is the normal distribution function, 
and $ G(\omega_i, \omega_o) $ is the geometry function. 
All dot products such as $ n \cdot \omega $ 
are clamped to zero from below. \\

The term $(1 - F(\omega_i, h)) 
\cdot \dfrac{c_d}{\pi}$ is the diffuse term. \\

The Fresnel term can be approximated using 
Schlick's approximation:
\[ F(\omega_i, h) = F_0 + (1 - F_0)
(1 - \omega_i \cdot h)^5 \]
where $F_0$ is the reflectance at normal incidence. 
For dielectrics, $F_0 \approx 0.04$, 
and for metals, $F_0 = c_d$. \\

The GGX normal distribution function is defined as
\[ D(h) = \frac{\alpha^2}{\pi \left[ 
(n \cdot h)^2 (\alpha^2 - 1) + 1 \right]^2} \]
where $\alpha = \text{roughness}$ 
is the remapped roughness parameter. \\

The geometry function uses the Smith GGX approximation:
\[ G(\omega_i, \omega_o) = 
G_1(\omega_i) \cdot G_1(\omega_o) \]
with
\[ G_1(\omega) = \frac{2 (n \cdot \omega)}
{(n \cdot \omega) + \sqrt{\alpha^2 + (1 - \alpha^2)
(n \cdot \omega)^2}} \]
Which is called the geometry shadowing-masking 
function for a single direction $\omega$. \\

For metallic materials, 
the diffuse component is zero, and 
$F_0 = c_d$. \\ 
For dielectrics, the BRDF includes both 
diffuse and specular components, 
and $F_0 \approx 0.04$. \\

This model ensures energy conservation and 
realism, and is often the BRDF of choice
in rendering and physics simulations. \\

\newpage

\subsection*{Random Number Generator}

We need a pseudo-random number generator that
can generate several 
floating-point numbers between $0$
and $1$ for each pixel of each frame. \\

These numbers need to be completely independent
from each other; meaning that we will need to
seed the generator using a combination of the frame,
pixel coordinate, and random number index. \\

We will use a hash based pseudo-random number
generator; this allows us to generate completely
independent samples while ensuring reproducibility,
that is, given the same seed, the generator returns
the same number, which will help with debugging. \\

In order to implement anti-aliasing, we could introduce
some jitter for the position on the pixel grid before
tracing the primary-ray. When we average out Monte-Carlo
samples, this provides a smoother surface.
We can use the pseudo-random number generator
to generate some random jitter, scaled accordingly. \\

\newpage

\subsection*{Multiple Importance Sampling}

Suppose an integral has the form:
\[ \int_a^b f(x)g(x)\;dx \]
Where $g(x)$ has a large variance,
while $f(x)$ is mostly flat. \\

If we wanted to estimate this using
Monte-Carlo integration, the samples would have
too high a variability. \\

In order to cancel out the $g(x)$,
we will use a special kind of change of variable
called multiple importance sampling. \\

Recall that when we transform the variables
using some transformation:
\[ T: [c, d] \to [a, b] \]
We end up adding a change of variable term:
\[ \sqrt{\vs{100}\det(J_T^\top J_T)} \]
We can design a transformation such that the
change of variable term cancels out $g(x)$. \\

If we define a transformation:
\[ \varphi(x) = \dfrac{\int_a^x g(x) \;dx}
{\int_a^b g(x) \;dx}
= \dfrac{G(x) - G(a)}
{G(b) - G(a)} \]
This induces an inverse transform:
\[ \varphi\inv: [0, 1] \to [a, b] \]
Note that we divide by $G(b) - G(a)$
specifically to ensure the transformed parameters
are from $0$ to $1$. \\

Now, by the Fundamental Theorem of Calculus:
\[ \sqrt{\det(J_{\varphi\inv}^\top J_{\varphi\inv})}
= \sqrt{\det((J_{\varphi}\inv)^\top J_{\varphi}\inv)}
= \dfrac{1}{\sqrt{\det((J_{\varphi})^\top J_{\varphi})}}
= \dfrac{1}{g(x)}\]
Which means that, having applied the
change of variables, the term g(x)
will cancel out. \\

We thus end up with the integral:
\[ \int_0^1 f(\varphi\inv(t)) \; dt \]
Samples from $f(\varphi\inv(t))$
have far less variance. \\

Note that if we have more than one variable,
and the variables are not separable,
inverse sampling becomes significantly harder. \\

In practice, the high variance term is
the geometry term, while the BRDF is the flat term. \\

While multiple importance sampling is not necessary, 
it is highly recommended for faster convergence. \\

\newpage

\subsection*{Next Event Estimation}

We mentioned earlier that, because the camera
sensor is small and unlikely to be hit by light
bounces, it is advisable to instead start the
ray's path from the sensor and then bounce
it around the scene. \\

However, if the light sources are small, it is unlikely
they will be hit either. \\

One solution to this problem is Bidirectional path
tracing, where we start with two paths;
one that originates on the sensor and another on
a light source, bounce both of them around the scene,
and then finally connect the two. \\

A better approach is called Next Event Estimation;
at each bounce in our path, we directly sample
from the light source:
\[ L_k^{\text{NEE}} = \int_{A_L} 
L_e(x_L \to x_k) \cdot 
f(x_k, \overrightarrow{x_k x_L} \to 
\overrightarrow{x_k x_{k-1}}) \cdot 
G(x_k, x_L) \cdot 
V(x_k, x_L) \; dA(x_L) \]
We can add this term $L_k^{NEE}$
to our regular $T^k L_e$ bounce term. \\

Note that $A_L$ is the set of all light sources;
we can randomly pick one light sounrce, and then
randomly sample a point on said light source. \\
Again, we can break this integral down
as a sum of individual light sources (surfaces),
though all except the sampled one will be $0$. \\

It can then be approximated using Monte-Carlo
estimation same as the normal bounce integral. \\

Now, when adding the two, we weight them differently:
\[ L_k = w_L \cdot L_k^\text{NEE} 
+ w_B \cdot (T^k L_e) \]
The weights add up to one:
\[ w_L + w_B = 1 \]
We can use the weight heuristic in order 
to ensure unbiased results:
\[ w_L = \dfrac{p_L}{p_L + p_B}
\quad \AND \quad
w_B = \dfrac{p_B}{p_L + p_B} \]
Where $p_B$ and $p_L$ are the probability density 
functions for picking a point on the light source
and the scene respectively. \\

Next-event estimation is not necessary but is
highly recommended. \\

\end{document}