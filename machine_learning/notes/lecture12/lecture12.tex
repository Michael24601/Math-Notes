
\documentclass[12pt]{article}

\usepackage[margin=1in]{geometry}

\input{../../../variables.tex}

\title{%
    \Huge Machine Learning \\
    \Large Lecture XII, XIII
}
\date{2025-06-12}
\author{Michael Saba}

\begin{document}
\pagenumbering{gobble}
\maketitle
\newpage
\setlength{\parindent}{0pt}
\pagenumbering{arabic}

\section*{Kernel Methods}
\subsection*{Motivation}


We start with quadratic regularized empirical risk
risk minimization with a margin-based loss 
function $\tilde{\ell}$:
\[ \min_{\theta \in \mathbb{R}^d} \frac{1}{n} 
\sum_{i=1}^n \tilde{\ell}((\Phi \theta)_i y_i) + 
\frac{\lambda}{2} \|\theta\|_2^2 \] 
Where $\Phi \in 
\mathbb{R}^{n \times d}$ is the feature matrix,
$y_i \in \mathbb{R}^n$ is an output label in the
training dataset, $\theta \in \mathbb{R}^d$
is the weights vector, and  $\lambda > 0$
is the regularization parameter. \\
Since $\tilde{\ell}$ is margin based, it could for 
example, be the loss for the hard-margin SVM,
$\delta_{\rbb_-}(1-t)$. \\

We can place all $y_i$ labels in the diagonals of
a matrix, and call it $\Upsilon$,
which allows us to write the regularized risk
minimization as:
\[ \min_{\theta \in \mathbb{R}^d} \frac{1}{n} 
\sum_{i=1}^n \tilde{\ell}((\Upsilon \Phi \theta)_i) + 
\frac{\lambda}{2} \|\theta\|_2^2\]
This loss is not nice enough for proximal gradient
descent, so we use the Fenchel-Rockafellar Duality
to associate a dual problem to our problem with the
same objective function:
\[ \min_{\theta} f(A\theta) + g(\theta) = 
\max_{\xi} -g^*(-A^\top \xi) - f^*(\xi) \]
We then take:
\[ A = \Upsilon \Phi \qquad 
f(\eta) = \frac{1}{n} \sum_{i=1}^n \tilde{\ell}(\eta_i) 
\qquad g(\theta) = \frac{\lambda}{2} \|\theta\|^2_2\]
Which have conjugates:
\[ f^*(\xi) = \frac{1}{n} \sum_{i=1}^n 
\tilde{\ell}^*(n\xi_i) \qquad 
g^*(\xi) = \frac{1}{2\lambda} \|\xi\|^2\]
Which yields the dual:
\[ \min_{\xi} \frac{1}{n} \sum_{i=1}^n 
\tilde{\ell}^*(n\xi_i) + \frac{1}{2\lambda} 
\|\Phi^\top \Upsilon \xi\|^2\] 
\[ \min_{\xi} \frac{1}{n} \sum_{i=1}^n 
\tilde{\ell}^*(n\xi_i) + \frac{1}{2\lambda} \langle 
\Upsilon \xi, \Phi \Phi^\top \Upsilon \xi \rangle \]
Which we can minimize to get the optimal $\xi$. \\

Now we know that, by the primal-dual relation:
\[ \theta = \nabla g^{-1}(-A^\top \xi) \]
\[ \theta = -\frac{1}{\lambda} \Phi^\top \Upsilon \xi \]
We can substitute in here the optimal $\xi$
to get the optimal $\theta$. \\

Now notice that in the minimization
of the fual problem,
we have the $n$ by $n$ matrix $K = \Phi\Phi^T$.
Notice that it is dimension independent,
meaning that it does not depend on the dimension
of the feature map $\varphi(x)$. \\
It is called the Gram Matrix. \\

This matrix will have entries with inner products
of the feature vector of the input:
\[ K = \Phi \Phi^\top = 
\pmat{\langle \varphi(x_1), \varphi(x_1) \rangle 
& \cdots & \langle \varphi(x_1), \varphi(x_n) \rangle \\
\vdots & \ddots & \vdots \\
\langle \varphi(x_n), \varphi(x_1) \rangle & \cdots 
& \langle \varphi(x_n), \varphi(x_n) \rangle} \]
Notice that it only has the inner product
of the feature maps in it, not the features 
themselves. \\

Now recall that our predictor $g_\theta(x)$
looks like:
\[ g_\theta(x) = \sum_{i=1}^n \theta_i \phi_i(x) = 
\langle \varphi(x), \theta \rangle \]
Now, if we replace $\theta$ by the optimal one we found,
which we can rewrite as a sum:
\[ \theta = -\frac{1}{\lambda} \Phi^\top \Upsilon \xi
= -\frac{1}{\lambda} \sum_{i=1}^n y_i \xi_i x_i = 
-\frac{1}{\lambda} \sum_{i=1}^n y_i \xi_i \varphi(x_i) \]
We get:
\[ g_{\theta}(x)
= - \frac{1}{\lambda} \sum_{i=1}^n y_i \xi_i 
\langle \varphi(x_i), \varphi(x) \rangle \]
Notice that again, we only need to know the
inner product of the feature map, not the
feature map itself. \\

A kernel function $k(x_i, x_j)$
return these exact inner products,
with respect to some feature map.
It measures similarity between $\varphi(x_i)$ 
and $\varphi(x_j)$. \\

The advantage is that instead of constructing
a feature map, which can be difficult, it may be
easier to instead define a function that measures
similarity intsead. And since we only ever need
the inner product, and not the feature map itself,
we can avoid defining it if we have the inner product,
or kernel. \\

So we will define kernel functions without
knowing what the feature map is; it is a measures
of similarity, but no longer an inner product. \\

Unlike feature maps, kernels do not depend on the
dimensionality of the feature space
(it is just an inner product, a single scalar output
given vectors $x$ and $x_i$ with arbitrary dimensions). \\

\newpage

\subsection*{Kernels}

We can't just replace inner products by kernels,
we need to analyze what is happening under the hood. \\

Positive Definite Kernels have a one to one
association with a Hilbert space 
$\hcal_k$, which more specifically,
is a Reproducing Kernel Hilbert Space,
or RKHS. \\

For now it is enough to know that a Hilbert Space
contains functions, including the kernel. \\

The Hilbert class will be our hypothesis
class for learning. The norm of this class
yields a natural regularization term
$\rcal(g) = \|g\|^2_{\hcal_k}$. \\

One advantage is that regularization penalties
are not applied to weights, but to the function 
directly. \\

For example, for the Guassian kernel:
\[ k(x, z) = e^{-{(x - z)^2/2\sigma^2}} \]
If we were to find its associated Hilbert Space,
we could prove that its norm is of the form:
\[ \|g\|^2_{\hcal_k} = \int_{\mathbb{R}} \sum_{s=0}^{\infty} 
\frac{\sigma^{2s}}{s! \, 2^s} \big( (\partial_x^s g)(x) 
\big)^2 \, dx\]
This associated RKHS contains only smooth functions. \\

The kernel is nothing more than an inner product
in the Hilbert Space:
\[ k(x, z) = \ang{\varphi(x), \varphi(z)} \]
Where $\varphi: \xcal \to \hcal$. \\
It is a similarity measure. \\

We also have a dissimilarity measure between $x$
and $z$ in the Hilbert Space, given by the norm,
which makes sense since it measure the distance:
\[ d(x, z) = \| \phi(x) - \phi(z) \| 
= \sqrt{ k(x, x) + k(z, z) - 2k(x, z) } \]
Norm are defined in terms of the inner product,
which we can replace by the kernel here
since it is the inner product. \\

A kernel is a function:
\[ k: \xcal \times \xcal \to \rbb \]
It is called positive definite if for all
$m \geq 1$, and for any two sequences:
\[ x_1, x_2,\dots, x_m \in \xcal 
\qquad c_1, c_2, \dots, c_m \in \rbb \]
The following inequality holds:
\[ \sum_{i=1}^m \sum_{j=1}^m c_i c_j k(x_i, x_j) \geq 0 \]
If the sum is strictly larger than $0$
for all distinct sequences that aren't jsut $0$,
we say the kernel is strictly positive definite. \\

A kernel matrix is the Gram Matrix we saw at the start,
with the inner product entries, which in the kernel
matrix are replaced by the kernel of $x_i$
and $x_j$:
\[ K = \big( k(x_i, x_j) \big)_{i,j} \]
A kernel is positive-definite, if every
kernel matrix for all sequences $x_1, \dots, x_m$
are all positive 
semi-definite (any sequence from $\xcal$). \\
We can use this to check that a kernel is positive
definite:
\[ w^TKw \geq 0 \]
Notice that checking that this is $\geq 0$
is pretty much the same as checking that the
inequality above holds. \\

The same applies for strictly positive definite 
kernels; we just need to prove that the Kernel matrix 
is positive definite intsead of positive
semi-definite. \\

Now, if we have two kernels $k_1$ and $k_2$
on $\xcal \times \xcal$, both of which are
positive definite, then these kernels $k$
are also positive definite:
\begin{enumerate}
    \item \( k(x, z) = \alpha \, k_1(x, z) \), 
    for any \(\alpha > 0\).
    \item \( k(x, z) = k_1(x, z) + k_2(x, z) \) 
    (pointwise addition).
    \item \( k(x, z) = k_1(x, z) k_2(x, z) \) 
    (pointwise multiplication).
    \item The pointwise limit \( k \) 
    of a sequence of positive definite kernels 
    \( k_i \) as \( i \to \infty \).
    \item \( k(x, z) = f(x) f(z) k_1(x, z) \) 
    for any \( f : X \to \mathbb{R} \) 
    (especially \( k(x, z) = f(x) f(z) \)).
    \item \( k(x, z) = \langle \phi(x), \phi(z) 
    \rangle \) for any \( \phi : X \to H \) with 
    Hilbert space \( \hcal \).
\end{enumerate}
The first two are trivial to prove, the third
the lecturer proved in class. \\

\newpage

\subsection*{Hilbert Spaces}

A Hilbert Space is just a complete vector space with
an inner product. \\

One example of a Hilbert space is the 
space of square integrable functions $L_2(\xcal)$:
\[ L^2(\xcal) = \left\{ f : \xcal \to \mathbb{R} \ \bigg| 
\ \int_\xcal |f(x)|^2 \, dx < \infty \right\} \]
Together with its inner product:
\[ \langle f, g \rangle := \int_\xcal f(x) g(x) \, dx \]
The issue with this space for example,
is that it is not a space of pointwise defined 
functions; there is no precise value 
of points, just the integral, which is a deal breaker
for us, since we need to be able to make
pointwise predictions. \\

This is why we need the RKHS family for the
kernels. \\

A Reproducing Kernel Hilbert Space $\hcal$
on $\xcal$ is a Hilbert Space of functions
$f: \xcal \to \rbb$ along with a reproducing kernel
$k$ on $\xcal \times \xcal$ which satisfies:
\[ \forall x \in \xcal: \quad k(x, \cdot) \in \hcal \]
\[ \forall f \in \hcal: \quad 
\ang{f, k(x, \cdot)}_\hcal = f(x) \]
The first condition statest that
if we fix one of the inputs of $k$,
it becomes a function on $\xcal$, and must
be part of the Hilbert Space. \\
The second is the reproducing property. \\

This makes the association clear;
given a Hilbert Space with kernel $k$,
$k$ is the kernel of the Hilbert Space,
which is always going to be positive definite. 
This side is trivial. \\
On the other hand, if we have a positive definite
kernel, then we can always construct a Hilbert
Space using that kernel, that satisfies the
above conditions. We can prove this
Hilbert Space is unique. \\

So we have a one to one association between
positive definite kernels and RKHS. \\
Each Hilbert Space is assoicated with one positive
definite kernel, and given a positive definite
kernel, we may construct one unique Hilbert Space. \\

For the first side of the association,
we can prove that the kernel of a RKHS 
is always positive definite, which the 
lecturer now does. \\

For the other side, 
we can show how to contrusct a RKHS using
some kernel $k$, which induces an inner product,
which we then show is independent of
our specific representation we use for $g$
and $f$ (the choice of $\alpha$ and $\beta$ vectors). \\
We also check the reproducing property holds. \\
And finally we check that the norm induced from the
inner product is that whenever the norm is $0$,
the function is $0$: if $\|f(x)\|^2_{\hcal} = 0 \implies
|f(x)| = 0$. We use the Cauchy-Schwartz inequality. \\
The Hilbert Space is a complete Vector Space,
so we should show that each cauchy sequence
converges, but this isn't very insightful. 
So we need a standard completion argument. \\
Finally, we need to show the uniqueness of the space;
if there is another RKHS with the same kernel, we
can prove that it can't be smaller, since we
used the span, so the other hilbert space must be
larger, but we can show the gap between the two
is 0, so they are the same Hilbert Space. \\

Basically, the main steps of the
constructive proof are:
\begin{enumerate}
    \item Construct the Hilbert Space
    from the span of the kernel:
    \[\ucal := 
    \left\{ \sum_{i=1}^{m} \alpha_i k(x_i, \cdot) 
    \,\middle|\, m \in \mathbb{N},\ x_i \in X,\ 
    \alpha_i \in \mathbb{R},\ i = 1, \ldots, 
    m \right\}\]
    \item Define the inner product of the Hilbert
    Space to be:
    \[ \langle f, g \rangle_{\ucal} =
    \sum_{i,j=1}^{m} \alpha_i \beta_j k(x_i, x_j) 
    \FOR f = \sum_{i=1}^{m} \alpha_i 
    k(x_i, \cdot)\AND g = \sum_{j=1}^{m} 
    \beta_j k(x_j, \cdot) \]
    \item Make sure everything is 
    well-defined and compatible with completion.
\end{enumerate}
This construction gives us a nice Hilbert space,
with functions that can be evaluated pointwise;
note that all the functions are just combinations
of the kernel at different points. \\

\newpage

\subsection*{More on RKHS}

The main objective of kernels
is that we can avoid having to define
a feature map explicitely. \\

As we mentioned in the last lecture,
there is a unique RKHS associated with each
kernel, which is a complet vector space
of functions at pointwise defined points. \\
This makes it very useful for us, and we
will use the associated Hilbert Space
as our hypothesis class. \\
We can also use the norm of this
Hilbert Space as our regularizer.
It usually connects with the smootheness
of $g$ directly, instead of the parameters
$\theta$, which makes it very natural
and suitable. \\

So for a kernel $k$, we have the minimization problem:
\[ \min_{g \in \hcal_k} \dfrac{1}{n}\sum_{i=1}^n 
\ell(g(x_i), y_i) + \lambda \| g \|_{\hcal_k}^2 \]
when trying to find a classifier $g$. \\

Being able to minimize and optimize the smootheness
of $g$ directly and not have to involve the parameters
will be invaluable. \\

But isn't the Hilbert Space huge? How
can we optimize the above term?
By the representer theorem, 
the solution has to lie on an $n$-dimensional
linear subspace of $\hcal_k$. \\

We already showed how to construct a Hilbert Space
given a kernel. \\
But there exists an alternative characterization
of RKHS. \\
If $\hcal$ is a Hilbert Space from $\xcal$ to $\rbb$,
then $\hcal$ is an RKHS if and only if all evaluation
functionals $\delta_x: \hcal \to \rbb$, 
$\delta_x(f) = f(x)$ are continuous. \\
Equivalently, for all $x \in \xcal$,
there exists an $M_x < \infty$ such that:
\[ \forall f \in \hcal: \quad |f(x)| \leq 
M_x\|f\|_{\hcal}  \]
This implies that:
\[ \forall x \in \xcal, 
\quad \|f\|_{\hcal} = 0 \implies f(x) = 0 \]
This show that if $\|f-g\|_{\hcal} = 0$,
then $f(x) = g(x)$ for any $f, g \in \hcal$
and $x \in \xcal$. This is what causes the RKHS
to have pointwise defined functions. \\

The same would not apply for the space of 
square integrable functions $L_2(\xcal)$. \\

\newpage

\subsection*{The Representer Theorem}

Since Hilbert Spaces are usually high dimensional,
functions from $\hcal$ are in danger of overfitting.
This is why we use the norm to regularize. \\
For the regularization, we can use a wrapper
around the norm:
\[ \lambda \tilde{\rcal}(\|g\|^2_{\hcal_k}) \]
which is monotonically increasing. \\

The representer theorem tells us that the
solution of:
\[ \min_{g \in \hcal_k} \dfrac{1}{n}\sum_{i=1}^n 
\ell(g(x_i), y_i) + \lambda 
\tilde{\rcal}(\|g\|^2_{\hcal_k}) \]
is on an $n$-dimensional subspace of $\hcal_k$,
that is:
\[ \hat{g}(x) = \sum_{i=1}^n \theta_i k(x_i, x) \]
Here each $x_i$ is one of our training data inputs.
Our solution is a linear combination of $n$
kernels, for $n$ data points in our training data. \\ 

We can then derive a new optimization problem,
using the fact that:
\[ g(x) = g_\theta(x) = 
\sum_{i=1}^{n} \theta_i k(x_i, x) \]
Which is:
\[ \min_{\theta \in \mathbb{R}^n} \ \frac{1}{n} 
\sum_{i=1}^{n} \ell\left( \sum_{j=1}^{n} 
\theta_i k(x_i, x_j),\ y_i \right) + \lambda\, 
\tilde{R} \left( \sum_{i,j=1}^{n} \theta_i 
\theta_j k(x_i, x_j) \right) \]
We can more compactly write this as:
\[ \min_{\theta \in \mathbb{R}^n} \frac{1}{n} 
\sum_{i=1}^{n} \ell((K\theta)_i, y_i) + \lambda\, 
\tilde{R}(\langle \theta, K\theta \rangle) \]
Where $K$ is the Gram matrix of the kernel $k$. \\

Note that the representer theorem only applies 
if we have 
$\tilde{\rcal}: \rbb^+ \to \rbb^+$ 
is strictly increasing. \\

So while we have an infinite dimensional
optimization problem,
the solution is finite dimensional, 
which alleviates the danger of overfitting. \\

But how can we prove this result? \\
We use the fact that in any Euclidian vector space, 
including Hilbert spaces, 
we can decompose each element into
a part that lies on some $n$-dimensional
subspace, and a part that is orthogonal to
the subspace. \\

So to prove the statement,
we let:
\[ G = \text{span}\{k(x_i, \cdot) 
\mid i = 1, 2, \dots, n\}\] 
be a finite dimensional subspace of a hilbert 
space $\hcal$. \\
We then take any $f \in \hcal$,
and decompose it into:
\[ f = f_0 + f_1 \]
Where $f_0$ is on $G$, and $f_1$ is orthogonal
to $G$. \\
Here $f_0$ can be written as a linear combination
of each kernel $k(x_i, \cdot)$ that span $G$. \\
And we know that $\ang{f_1, g}_{\hcal} = 0$
for any element of $g \in G$, which are 
themselves just linear combinations of
$k(x_i, \cdot)$. \\
But, since $\hcal$ is a Repreoducing Kernel
Hilbert space, this implies that:
\[ f_1(x_i) = 0 \]
Since $f_1(x_i) = \ang{f_1, k(x_i, \cdot)}_{\hcal}$,
where $k(x_i, \cdot) \in G$. \\
So instead of $\ell(f(x_i), y_i)$,
we can just write $\ell(f_0(x_i), y_i)$,
as the orthogonal part is just $0$. \\
Now, $f_1$ will nonetheless be part of the $f$
in the regularization term. 
But, by the strict monotonicity of $\tilde{\rcal}$,
we will see that $f_1$
also becomes $0$:
\[ \tilde{\rcal}(\|f\|^2_{\hcal})
= \tilde{\rcal}(\|f_0\|^2_{\hcal}
+ \|f_1\|^2_{\hcal}
+ 2\ang{f_0, f_1)}_{\hcal}) \]
The third term is $0$ as they are othrogonal,
so we have:
\[ \tilde{\rcal}(\|f_0\|^2_{\hcal}
+ \|f_1\|^2_{\hcal}) \]
Our goal is to minimize the regularized
empirical risk, but $f_1$
does not appear in the loss term,
only the regularizer, and the regularizer
is strictly increasing, which means that
it is always in our best interest
to set $f_1$ to $0$
when minimizing.
We can't say the same for $f_0$,
since it features in the loss $\ell$,
so we can't say for sure that setting it to $0$
would minimize the term as a whole. \\

So our solution is always some $f_0$,
which is in $G$, the finite dimensional subspace. \\
The $\theta_i$ terms are used in the representation
of $g$ as a linear combination of the kernels 
in the $n$-dimensional subspace. \\

We can use this to solve kernel least 
squares, kernel logistic regression, kernel
hard and soft SVM... \\

\newpage

\subsection*{Translation Invariant Kernels
and the Fourrier Transform}

The Guassian kernel can be used to linearlly seperate
data that is not linearlly seperable in the input space,
similar to a feature map, which can be hard to
find. \\

A translation invariant kernel is one that does
not depend on the absolute positions of inputs $x$
and $z$, but rather, on their relative position. \\
That is to say, there exists a $k'$ function
such that:
\[ k(x, z) = k'(x - z) \]
Notice that $k'$ returns the same measure of
similarity as $k$, but only needs the difference
between $x$ and $z$, implying that $k(x + a, z+a)$
for any $a \in \rbb^d$ would yield the same
result:
\[ k(x + a, z+a) = k'(x+a - (z + a)) 
= k'(x - z) = k(x, z) \]
So, translation invariant kernels measure 
relative properties instead of absolute ones. \\

Translation invariant kernels can just be
expressed as $k(x -z)$, with one input only,
expected to be the difference. \\

A Fourrier Transform is a way to represent a function
using frequencies. \\
One property we care about is that
derivatives becomes multiples in the Fourrier domain;
so derivatives are easier in teh Fourrier domain;
we can then go back using the inverse Fourrier 
Transform. \\

So we have the fourrier transform,
which takes our function and expresses it as
a sum of frequencies:
\[ \hat{f}(\omega) = \frac{1}{\sqrt{2\pi}} 
\int_{\mathbb{R}} f(x)\, e^{-i x \omega} \, dx\] 
Where $\hat{f}$ is the function in the Fourrier
domain. \\
And the inverse fourrier transform then
transforms the function back to its original
form, possibly after some modifications to it
in the Fourrier domain:
\[ f(x) = \frac{1}{\sqrt{2\pi}} \int_{\mathbb{R}} 
\hat{f}(\omega)\, e^{i x \omega} \, d\omega\]
This is used extensively in signal processing. \\

One property we care about is that the
Fourrier Transformed function $g(\omega)$
can turn derivatives into multiplications:
\[ \dd{g(x)}{x} = \frac{d}{dx} 
\left( \frac{1}{\sqrt{2\pi}} 
\int_{\mathbb{R}} \hat{g}(\omega)\, e^{i x \omega} 
\, d\omega \right) = \frac{1}{\sqrt{2\pi}} 
\int_{\mathbb{R}} i \omega\, \hat{g}(\omega)\, 
e^{i x \omega} \, d\omega \]
So the derivative of $g(x)$
is $i\omega \hat{g}(\omega)$ in the Fourrier
domain. \\

Bochner's theorem tells us that a translation
invariant kernel is positive definite
if and only if it can be expressed
as the inverse Fourrier transform of a
positive, semmytric function $v(\omega)$:
\[ k(x - z) = \frac{1}{\sqrt{2\pi}} 
\int_{\mathbb{R}} v(\omega)\, 
e^{-i (x - z)\omega} \, d\omega \]
This gives us a way to define a translation
invariant kernel using a poisitive summetric
function $v(\omega)$. \\

This also gives us that:
\[ \|g\|_{\mathcal{H}_k}^2 = \frac{1}{2\pi} 
\int_{\mathbb{R}} \frac{|\hat{g}(\omega)|^2}
{v(\omega)} \, d\omega \]
Which is how we can express the norm squared
of a kernel given by $v(\omega)$. \\

For example, using:
\[ v(\omega) = \sigma e^{-\frac{\sigma^2 \omega^2}{2}} \]
We get the kernel:
\[ k(x - z) = e^{-\frac{(x - z)^2}{2\sigma^2}} \]
With the norm:
\[ \|g\|_{\mathcal{H}_k}^2 = \int_{\mathbb{R}} 
\sum_{s=0}^{\infty} \frac{\sigma^{2s}}{s! \, 2^s} 
\left( (\partial_x^s g)(x) \right)^2 \, dx \]
Which is the Guassian kernel
(written as a translation invariant kernel). \\

\newpage

\subsection*{Kernels on structured domains}

We can define kernels on any set,
not just $\rbb^d$. \\

It can be quite hard to define
a similairty measure on an unstructured set,
with the intersection between two parts being one
of the few. \\

\end{document}
