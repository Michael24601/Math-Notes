
\documentclass[12pt]{article}

\usepackage[margin=1in]{geometry}

\input{../../../variables.tex}

\title{%
    \Huge Machine Learning \\
    \Large Lecture V
}
\date{2025-05-12}
\author{Michael Saba}

\begin{document}
\pagenumbering{gobble}
\maketitle
\newpage
\setlength{\parindent}{0pt}
\pagenumbering{arabic}

\section*{Learning in Practice}

\subsection*{Linear Algebra}

Some linear algebra properties that
will be helpful for this chapter. \\

Diagonal matrices commute with other matrices:
\[ DA = AD \]
for any matrix $A$ so long as $D$ is diagonal. \\

First note that
\[ \ebb[\ang{u, v}] = \ang{\ebb[u], \ebb[v]} \]
holds. \\

Moreover, a matrix $M$ such that:
\[ wMw^T \geq 0 \]
for all vectors $w \in \rbb^n$ is called
positive semi-definite. \\
A positive semi-definite matrix
has non-negative eigenvalues. \\

A positive semi-definite matrix is positive-definite
if it is also invertible. \\

All matrices of the form $A^TA$
are positive semi-definite since:
\[ wA^TAw^T = (Aw^T)^TAw^T = \|wA\|^2 \]
is always positive (norm squared). \\

Moreover, a matrix of the form $A^TA$
is invertible (positive-definite)
if $A$ has full column rank (all columns
are linearly independent). \\

All matrices of the form $A^TA$ are symmetric. \\

A positive semi-definite matrix
that is also symmetric is always
diagonalizable, which means we can
get its eigen decomposition $P\Lambda P^T$
where $P$ is the orthogonal eigenspace. \\

If $M$ is a symmetric positive-semi definite matrix,
then the matrix:
\[ M + aI \]
for any positive number $a \in \rbb$
is positive definite. \\

A positive-definite matrix has positive
eigenvalues and is always diagonalizable. \\

The trace of a matrix $\tr(M)$ is the sum of its
diagonal entries. \\

The trace of a scalar is just the scalar
$\tr(a) = a$. \\

The trace has a circular property:
\[ \tr(ABC) = \tr(CAB) = \tr(BCA) \]
Which applies for vectors as well as matrices. \\

The inverse of the transpose $(M^T)\inv$
is equal to the transpose of the inverse 
$(M\inv)^T$. \\

This property of the transpose holds for vectors:
\[ (Av)^T = v^TA^T \]
where $v$ is any column vector. \\

The inverse of a scalar $b$ 
is just one over the scalar:
\[ (Ab)\inv = \dfrac{1}{b}A\inv \]
where $b \in \rbb$ is any scalar. \\

If the eigenvalues of a matrix $M$
are all smaller or equal to $\dfrac{1}{2}$,
then:
\[ w^TMw \leq \dfrac{1}{2} \|w\|^2 \]
for any vector $w \in \rbb^n$.
This is because:
\[ w^TMw = w^T(P\Lambda P^T)w
= (w^TP)\Lambda(P^Tw)
= (P^Tw)^T\Lambda(P^Tw) \]
Now, we can take $z = P^Tw$, and write:
\[ (P^Tw)^T\Lambda(P^Tw) 
= z^T \Lambda z = \sum_i \lambda_i z_i^2 \]
where $\lambda_i$ is the $\nth{i}$ eigenvalue. \\
Notice that because $\lambda_i \leq \dfrac{1}{2}$,
this means that:
\[\sum_i \lambda_i z_i^2 
\leq \dfrac{1}{2} \sum_i z_i^2 = \dfrac{1}{2} \|z\|^2 \]
Note that because $P$ is orthogonal:
\[ \|w\|^2 = \|z\|^2 \]
Which means that:
\[w^TMw = \sum_i \lambda_i z_i^2 
\leq \dfrac{1}{2} \|w\|^2\]
Proving the theorem. \\

\newpage

\subsection*{Linear Regression}

For linear gression we assume that
we have $\xcal$ and $\ycal = \rbb$
(the output is continuous). \\

For now we assume that $\rcal(f) \equiv 0$. \\

We will use the squared loss $\ell = (z- y)^2$. \\

Roughly, what we mean by linear regression is that
the prediction function is linear in its parameters. \\
It can still be non-linear with repsect to
the input! \\

The first step of Linear Regression is 
to take the input data $\xcal$ and use a feature
map that maps the data to $\rbb^d$:
\[ \varphi: \xcal \to \rbb^d \]
This is representation of $\xcal$ 
that eases learning while carrying the same
information. \\

The hypothesis class $\ffrak$ is a family of
linear functions $f_\theta$ with
repsect to its parameters $\theta \in \Theta$:
\[ f_\theta(x) = \ang{\theta, \varphi(x)}
= \sum_{i = 1}^d \theta_i \varphi_i(x) \]
Where $f_\theta$ is linear in terms of the
$\theta$ parameters, but not $x$,
since we don't know what $\varphi$ does. \\

Note that instead of having $\varphi(x)$
have all $d$ dimensions, we can instead define
a lower dimension $\tilde{\varphi}(x)$
that is in $\rbb^{d-1}$,
and then set $\varphi(x) = (\tilde{\varphi}(x), 1)$. \\
Basically, we want $\varphi(x)$ to have
a single constant $1$ at the end. \\
Note that this allows that the last parameter,
$\theta_d$, is just a shift in this case;
a $b$ offset,
turning the function into an affine function
(since it now has a simple shift element constant). \\
So if we have:
\[ \varphi(x) = \pmat{x \\ 1} \qquad
\theta = \pmat{w \\ b} \] 
Then $f_\theta(x) = \ang{x, w} + b$. \\

One example of a $\varphi(x)$ is:
\[ \varphi(x) = (1, x, x^2, x^3, \dots, x^{d-1})^T \]
This gives us a polynomial prediction function:
\[ f\theta(x) = \sumof{i=1}{d}\theta_i x^i \]
This is a monomial basis, where the function
is polynomial in terms of $x$,
but linear or affine in terms of $\theta$. \\

If we define $\xcal = [0, 2\pi]$,
then we can also have a fourrier basis:
\[ f_\theta(x)
= \sum_{j = 1}^{m} \theta_{2j-1}\sin(jx) 
+ \theta_{2j}\cos(jx) \]
Where $\varphi(x) = (\sin(x), \cos(x),
\sin(2x), \cos(2x), \sin(3x), \dots)$. \\

We should be careful not to use a basis with too
high a dimension as it would increase the number
of features by a lot. \\

When we are using squared loss; we can think
of the result as finding the values of $\theta$
that averages out over all $y$ values.
This makes sense when you look at the
empirical risk and what it calculates. \\

Note that we can write $\riskEmpirical{\ell}{n}
(f_\theta)$ as $\riskEmpirical{\ell}{n}
(\theta)$,
since $\theta$ is the only changing thing
(the one thing we optimize in $f_\theta$). \\

We can rewrite risk in vector and matrix
notation. \\
We define $\Phi$ to be the matrix
where each row is $\varphi(x_i)^T$
of the $\nth{i}$ input in the dataset $x_i$:
\[ \Phi = \pmat{
\varphi_1(x_1) & \varphi_1(x_2) & \dots & \varphi_1(x_n) \\
\varphi_2(x_1) & \varphi_2(x_2) & \dots & \varphi_2(x_n) \\
\vdots & \vdots & \ddots & \vdots \\
\varphi_d(x_1) & \varphi_d(x_2) & \dots & \varphi_d(x_n)} \]
So we get $\Phi \theta - \bl{y}$
where $\bl{y}$ is the vector of all
outputs $(y_1, y_2, \dots y_n)^T$. \\
Which means that the risk would look like:
\[ \dfrac{1}{n}\|\Phi \theta - \bl{y}\|^2 \]
Which makes it clear we are finding the $\theta$
that minimizes the distance expressed above. \\

Now how do we solve:
\[ \dfrac{1}{n}\|\Phi \theta - \bl{y}\|^2 = 0 \]
We know that we can just differentiate and finding
the gradient of the expression and setting
it to $0$. \\
We know that:
\[ \nabla \riskEmpirical{\ell}{n}(f_{\theta}) 
= \nabla \dfrac{1}{n}
\|\Phi \theta - \bl{y}\|^2 
= \dfrac{2}{n}\Phi^T(\Phi \theta - \bl{y}) \]
So setting it to $0$ we get:
\[ \hat{\theta} = (\Phi^T\Phi)^{-1} \Phi^T 
\bl{y} \]
This $\hat{\theta}$ is the set of arguments
that minimizes the empitical risk.
It is called the ordinary least square
estimator.
It is also called the empirical risk minimizer. \\
Note that this only works if $\Phi$
is full column rank to begin with;
that is, all its columns must linearly
independent. \\
We call $(\Phi^T\Phi)^{-1} \Phi^T$
the pseudo inverse. \\

So if $\Phi$ has full column rank,
we have a unique closed form solution. \\
Note that $\Phi$ having full column rank
makes the $\riskEmpirical{\ell}{n}$
coercive, meaning it has a global minimizer
$\hat{\theta}$. \\
Pratically, $\Phi^T\Phi$ is invertible
if all the columns in $\Phi$
are linearly independent. \\
The full proof is on the slides. \\

Note that 
\[ \dfrac{1}{n}\Phi^T\Phi \] 
is the empirical covariance matrix of the feature
vectors $(\varphi_1(x_i), \varphi_2(x_i),
\dots, \varphi_d(x_i))^T$. \\

Instead of using matrix and vector gradients,
we can try finding the partial derivatives
of the original function and setting the gradient
to $0$ as an exercise. \\

Now that we have a unique closed form solution,
we can add regularization back. \\

We now have to minimize:
\[ \min_{\theta} = \riskEmpirical{\ell}{n}(f_\theta)
+ \lambda \rcal(f_\theta) \] \\
Note that we can also write $\rcal(f_\theta)$
as $\rcal(\theta)$. \\

One example of a regularizer is shrinkage,
or quadratic regularization:
\[ \rcal(\theta) = \|\theta\|^2 \]
What this means is that this regularizer
will favour small parameters $\theta_i$,
and it can be thought of as a measure of
simplicity to ensure the regressor doesn't
become too complex. \\

Note that in certain case, $\rcal(f_\theta)$
and $\rcal(\theta)$ could be different. \\
Sometimes imposing regularization on function
or parameters is not the same. \\

Putting everything together, we have Ridge Regression:
\[ \min_{\theta} = \dfrac{1}{n}
\|\Phi \theta - \bl{y}\|^2 
+ \lambda \|\theta\|^2 \]
We can also minimize this,
and it does have a unique closed form solution that
we can find analytically:
\[ \hat{\theta}_\lambda = 
(n\lambda I + \Phi^T\Phi)\inv \Phi^T\bl{y} \]
Note that $n\lambda I + \Phi^T\Phi$ is always
invertible, no need to impose any column rank
rules on $\Phi$. \\
The proof comes from the fact that:
\[ \left\| \pmat{x \\ y} \right\|^2 
= \|x^2\| + \|y^2\| \]
Which we will see used later. \\

Note that we can take our regularized
Ridge Regression risk, and rewrite as an 
unregularized regression problem with a modified
$\tilde{\Phi}$, which we know is solved by 
$\hat{\theta}_\lambda$. \\
We at some point arrive at a matrix:
\[ \Phi^T\Phi + n \lambda I \]
which we need to invert. \\
Note that this matrix is always invertible,
since it is positive definite (the first
part is positive semi-definite,
then we add the diagonal matrix to it,
making it positive-definite),
which is always invertible. \\
That is why Ridge Regression always has a
solution regardless of $\Phi$'s column rank. \\

The way it works is that we have:
\[ \dfrac{1}{n}
\|\Phi \theta - \bl{y}\|^2 
+ \lambda \|\theta\|^2 \]
First we can enter the coefficients into the norm:
\[ \left\| \sqrt{\dfrac{1}{n}} (\Phi \theta 
- \bl{y}) \right\|^2
+ \| \sqrt{\lambda} \theta \|^2  \]
Using the fact we mentioned above, we can
rewrite this as a single norm:
\[ \left\| \pmat{\sqrt{\dfrac{1}{n}} 
(\Phi \theta - \bl{y})
\vspace{10pt} \\ \sqrt{\lambda} \theta } \right\|^2
= \left\| \pmat{\sqrt{\dfrac{1}{n}} \Phi
\vspace{10pt} \\ \sqrt{\lambda}I} \theta
- \pmat{\sqrt{\dfrac{1}{n}} \bl{y} 
\vspace{10pt} \\ 0} \right\|^2 \]
Finally, we can take out $\sqrt{\dfrac{1}{n}}$
from inside the norm:
\[ \dfrac{1}{n} \left\| 
\pmat{\Phi \vspace{5pt} \\ \sqrt{n\lambda}I} \theta
- \pmat{\bl{y} \vspace{5pt} \\ 0} \right\|^2 \]
Which is just another normal equation we can solve,
highlighting the fact that this is just
some unregularized least square problem with:
\[ \tilde{\Phi} = 
\pmat{\Phi \vspace{5pt} \\ \sqrt{\lambda}I}
\qquad \AND \qquad 
\bl{\tilde{y}} = 
\pmat{\bl{y} \vspace{5pt} \\ 0} \]
As the new feature matrix and output vector. \\

\newpage

\subsection*{Statistical Analysis of Least Square
Regression}

This section is about analyzing how close we can
get using real data to the optimal classifiers
and risk (Baye's). \\

So we will basically measure how close we
can get to the theoretical best output. \\

We have two settings. \\
\begin{enumerate}
    \item 
    \textbf{Random design:} 
    We assume we have a random
    input and output data set.
    \item 
    \textbf{Fixed design:} 
    We assume we have a fixed input dataset that
    we want to analyze and try to best predict the 
    outputs of these fixed inputs. 
\end{enumerate}

We will make our life a little easier and
focus on the fixed design setting.
Note that while the input is fixed,
the output is still a random dataset $\bl{Y}$. \\

The main goal of this section is to
find Baye's optimal risk for our hypothesis
$\ffrak$ class. \\
We want to compare the risk of some
set of parameters $\theta$ we have to
the optimal risk we can get
for our hypothesis class (given a fixed
input data). \\
That way, we can compare Baye's optimal risk
with that of some chosen parameter $\theta$,
which would then give us the rate of convergence
(how far off we are),
which is only possible if Baye's optimal risk
was for the same family of learning rules. \\
That means that we will need to find
Baye's optimal risk for the linear least square
problem, which means it will be given
by a set of optimal parameters $\theta^*$. \\

We can calculate the true (non-empirical)
risk:
\[ \risk{\ell}{}(f_\theta) = 
\dfrac{1}{n}\ebb[\|\Phi \theta - \bl{Y}\|^2] \]
Where $\bl{Y}$
is the random variable of the vector of outputs:
\[ \bl{Y} = (Y_1,Y_2, \dots, Y_n)^T \]
where each $\bl{Y_i}$
is the random variable of one output in the dataset. \\
As mentioned, the goal of this section
is to compare this risk with the optimal risk
(restricted to our linear hypothesis class). \\

We assume that $\Phi^T\Phi$ is invertible. \\

We will write $\hat{\Sigma} = \dfrac{1}{n}\Phi^T\Phi$
as a shorthand (the covariance matrix of the feature
vectors). This is a deterministic quantity since we
fixed the input data. \\

Now to find Baye's optimal risk,
restricted to our linear hypothesis class,
we will first assume (and then prove)
that the optimal vector of parameters $\theta^*$,
is given by:
\[ \bl{Y} = \Phi \theta^* + \bl{\eps} \]
where $\bl{\eps}$ is some random noise vector,
where for each input $Y_i$, 
we've got some noise $\varepsilon_i$. \\
We assume thatfor all $i$,
that is, $\ebb[\varepsilon_i] = 0$,
and we take have a variance
\[ \ebb[(\eps_i - \ebb[\eps_i])^2]
= \ebb[\eps_i^2] = \sigma^2 \]
for any one random noise variable. \\

At first glance, this makes intuitive sense;
the optimal set of parameters $\theta^*$
is the one such that $\Phi \theta^*$
returns the correct set of labels,
which are given by the random dataset
of output labels $\bl{Y}$.
Of course, in non-ideal settings,
like the real world, we would also have noise,
which is where the randomness of $\bl{Y}$
comes from ($\Phi$ and $\theta^*$ are fixed). \\
But we still need to prove it. \\

Recall that in the case of square loss
$\ell(z, y) = \|z - y\|^2$,
Baye's optimal predictor would output
the mean of $y$, which is just the expected
value when we have a random $\bl{Y}$. \\
We can do that here and rewrite $\bl{Y}$
as $\Phi \theta^* + \bl{\eps}$:
\[ f_{\theta}^*(x_i)
= \ebb[\bl{Y} \mid X = x_i] \]
However, because $X$, the input, is fixed,
as not random, we can get rid of the condition:
\[ \ebb[\bl{Y}] = \ebb[\Phi \theta^* + \bl{\eps}]
= \Phi \theta^* + \ebb[\bl{\eps}] = \Phi \theta^* 
= f_{\theta^*}(x_i) \]
So this proves that Baye's 
optimal predictor $f_{\theta}^*(x_i)$
is equal to $f_{\theta^*}(x_i)$,
which proves that $\theta^*$
is the optimal set of parameters. \\

We can also recalculate the risk:
\[ \risk{\ell}{}(\theta) = 
\dfrac{1}{n}\ebb[\|\Phi \theta - \bl{Y}\|^2]
= \dfrac{1}{n}\ebb[\|\Phi \theta 
- (\Phi \theta^* + \bl{\eps}) \|^2]
= \dfrac{1}{n}\ebb[\|\Phi (\theta - \theta^*) 
- \bl{\eps} \|^2] \]
This time with respect to the randomness of 
$\bl{\eps}$ instead of that of $\bl{Y}$,
and in terms of the parameters $\theta^*$
we defined. \\

Now that we know that $\theta^*$ is the
optimal set of parameters, it means that:
\[ \riskOptimal{\ell} = \risk{\ell}(\theta^*) \]
And we know that since:
\[ \risk{\ell}{}(\theta) =
\dfrac{1}{n}\ebb[\|\Phi (\theta - \theta^*) 
- \bl{\eps} \|^2] \]
This means that:
\[ \risk{\ell}{}(\theta^*) =
\dfrac{1}{n}\ebb[\|\Phi (\theta^* - \theta^*) 
- \bl{\eps} \|^2] = \dfrac{1}{n}\ebb[\|\bl{\eps}\|^2]
= \dfrac{1}{n}\ebb\brac{\sum_{i=1}^{n}\eps_i^2} \]
Using the linearity of the expected value:
\[ \dfrac{1}{n}\sum_{i=1}^{n} \ebb[\eps_i^2]
= \dfrac{1}{n}\sum_{i=1}^{n} \sigma^2
= \dfrac{1}{n} n\sigma^2 = \sigma^2 \]
So $\riskOptimal{\ell} = \sigma^2$,
the variance of each random noise. \\

Now we can compare
the risk of some parameter $\theta$
to Baye's optimal risk (restricted
to the same hypothesis class of
linear least square):
\[ \risk{\ell}(\theta) - \riskOptimal{\ell}
= \ang{\theta - \theta^*, 
\hat{\Sigma}(\theta - \theta^*) }
= \|\theta - \theta^* \|_{\hat{\Sigma}}^2 \]
This gives us how far off our set
of parameters are from being optimal. \\

We can prove that the excess is
what we said it was by calculating
the risk $\risk{\ell}(\theta)$:
\[ \risk{\ell}(\theta) =
\dfrac{1}{n}\ebb[\|\Phi (\theta - \theta^*) 
- \bl{\eps} \|^2]
= \dfrac{1}{n}\ebb[\|\Phi (\theta - \theta^*) \|^2
+ \|\bl{\eps} \|^2 
- 2\ang{\theta - \theta^*, \bl{\eps}}] \]
Then using the linearity of the expected value:
\[ \dfrac{1}{n} (\|\Phi (\theta - \theta^*) \|^2
+ \ebb[\|\bl{\eps} \|^2 ]
- 2\ebb[\ang{\theta - \theta^*, \bl{\eps}}])
= \dfrac{1}{n} (\|\Phi (\theta - \theta^*) \|^2
+ \sigma^2 - 
2\ang{\theta - \theta^*, \ebb[\bl{\eps}]}) \]
The last term is just $0$, so:
\[ \dfrac{1}{n} \|\Phi (\theta - \theta^*) \|^2
+ \sigma^2
= \ang{\theta - \theta^*,
\dfrac{1}{n}\Phi^T\Phi(\theta - \theta^*)}
+ \sigma^2 \]
When we subtract the two risks,
the variance vanishes, and we are left
with the excess risk we described above:
\[ \ang{\theta - \theta^*,
\dfrac{1}{n}\Phi^T\Phi(\theta - \theta^*)}
= \ang{\theta - \theta^*,
\hat{\Sigma}(\theta - \theta^*)} \]
Which completes the proof. \\

However, as we know, when derive a set
of parameters $\tilde{\theta}$ in the process
of fitting the data, we usually use the output
(such was the case when we calculated the
optimal estimator $\hat{\theta}$). \\
So in the case where our estimator depends
on the randomness of $\bl{Y}$,
we would have to use the expected value
of the risk, in order to get the convergence
rate, and we have:
\[ \ebb[\risk{\ell}(\tilde{\theta})] 
- \riskOptimal{\ell} = \underbrace{
\| \ebb[\tilde{\theta}] - \theta^*
\|^2_{\hat{\Sigma}}}_{\text{Bias}}
+ \underbrace{
\ebb[\|\tilde{\theta} - \ebb[\tilde{\theta}] 
\|^2_{\hat{\Sigma}}]}_{\text{Variance}} \]
We will prove this later. \\

First, note that the bias is the 
distance between $\tilde{\theta}$
and $\theta^*$.
We have $\ebb[\tilde{\theta}]$
because it is random (depends on $\bl{Y}$),
meaning we need to put an expected value
around it to get its mean value. \\
A high bias means that we have too simple a
learning rule, and we have an underfit.\\

The variance is pretty self explanatory;
it's just the formula for the variance
of $\tilde{\theta}$, the random variable.
If it's too high, it means we have overfitting. \\

Now to show the value is true:
\[ \ebb[\risk{\ell}(\tilde{\theta})] 
- \riskOptimal{\ell}
= \ebb[\risk{\ell}(\tilde{\theta}) - \riskOptimal{\ell}] \]
Using the result we got earlier, we have:
\[ \ebb[\|\tilde{\theta} - \theta^* \|_{\hat{\Sigma}}^2] \]
We can now add and subtract $\ebb[\tilde{\theta}]$:
\[\ebb[\|\tilde{\theta} - \ebb[\tilde{\theta}]
- \ebb[\tilde{\theta}] + \theta^* \|_{\hat{\Sigma}}^2]
= \ebb[\|\ebb[\tilde{\theta}] - \theta^* \|_{\hat{\Sigma}}^2]
-\ebb[\|\tilde{\theta} - \ebb[\tilde{\theta}] \|_{\hat{\Sigma}}^2] \]
Note that in $\ebb[\|\ebb[\tilde{\theta}] 
- \theta^* \|_{\hat{\Sigma}}^2]$,
both $\theta^*$ and $\ebb[\tilde{\theta}]$
are deterministic ($\tilde{\theta}$ is random,
but it already has an expected value around it
$\ebb[\tilde{\theta}]$),
so we can drop the outer expected value, getting:
\[ \|\ebb[\tilde{\theta}] - \theta^* \|_{\hat{\Sigma}}^2
-\ebb[\|\tilde{\theta} - \ebb[\tilde{\theta}] \|_{\hat{\Sigma}}^2 ] \]
Which completest the proof of the decomposition. \\

Now that we've given formulas that calculate
the convergence rate or excess risk
of any parameters we have (random or not),
what parameters can we evaluate? \\

\newpage

\subsection*{Analysis of the Empirical Risk Minimizer}

One very natural choice that we expect
to do well is linear least squares
estimator:
\[ \hat{\theta} = (\Phi^T\Phi)\inv\Phi^T\bl{Y}
= \dfrac{1}{n}\hat{\Sigma}\inv \Phi^T\bl{Y} \]
Which we got by minimizing the empirical risk
in the last section. \\
Note that
\[ \hat{\Sigma} = \dfrac{1}{n}\Phi^T\Phi \]
Which means that:
\[ {\hat{\Sigma}}\inv 
= \para{\dfrac{1}{n}\Phi^T\Phi}\inv
= n\para{\Phi^T\Phi}\inv \]
Which is why we replace $(\Phi^T\Phi)\inv$
by $\dfrac{1}{n}\hat{\Sigma}\inv$. \\

First notice that it is unbiased, meaning that:
\[ \ebb[\hat{\theta}] = \theta^* \]
What this means is that the bias term in
the excess risk decomposition will be $0$. \\
So the only term left is the variance. \\

We can prove that $\ebb[\hat{\theta}] = \theta^*$.
Note that:
\[ \ebb[\hat{\theta}]
= \dfrac{1}{n} \hat{\Sigma}\inv \Phi^T \ebb[\bl{Y}] \]
Recall now that:
\[ \ebb[\bl{Y}] 
= \ebb[\Phi \theta^* + \bl{\eps}]
= \Phi \theta^* + \ebb[\bl{\eps}] = \Phi \theta^* \]
Which means that:
\[ \ebb[\hat{\theta}] = 
\dfrac{1}{n} \hat{\Sigma}\inv \Phi^T \Phi \theta^*
= (\Phi^T\Phi)\inv \Phi^T \Phi \theta^* 
= \theta^* \]
Which concludes the proof. \\

This means that the excess risk will have no bias
term. \\

We can also calculate the variance
of $\hat{\theta}$, which will be helpful
when calculating the variance term in the 
excess risk. \\

First note that $\ebb[\bl{\eps} \bl{\eps}^T]
= \sigma^2I$, which is a diagonal matrix. \\
In fact, this is the covariance
matrix of $\bl{\eps}$:
\[ \ebb[\bl{\eps} \bl{\eps}^T]
= \ebb\brac{\pmat{\eps_1\eps_1 & \eps_1\eps_2 & \dots 
& \eps_1\eps_n \\ 
\eps_2\eps_1 & \eps_2\eps_2 & \dots & \eps_2\eps_n \\ 
\vdots &\vdots &\ddots & \vdots \\
\eps_n\eps_1 & \eps_n\eps_2 & \dots 
& \eps_n\eps_n }}
= \pmat{\ebb[\eps_1^2] & \ebb[\eps_1\eps_2] 
& \dots & \ebb[\eps_1\eps_n] \\ 
\ebb[\eps_2\eps_1] & \ebb[\eps_2^2] & 
\dots & \ebb[\eps_2\eps_n] \\ 
\vdots &\vdots &\ddots & \vdots \\
\ebb[\eps_n\eps_1] & \ebb[\eps_n\eps_2] & \dots & 
\ebb[\eps_n^2] } \]
Now note that since 
$\ebb[\eps_i] = \ebb[\eps_j] = 0$:
\[ \ebb[\eps_i\eps_j] = \ebb[(\eps_i - \ebb[\eps_i])
(\eps_j - \ebb[\eps_j])] = \cov(\eps_i, \eps_j) = 0 \]
when $i \neq j$ since $\eps_i$ and $\eps_j$
are unrelated. \\
On the other hand, we have:
\[ \ebb[\eps_i^2] = \var(\eps_i) = \sigma^2 \]
So to conclude:
\[ \ebb[\bl{\eps} \bl{\eps}^T] 
= \pmat{\sigma^2 & 0 & \dots & 0 \\ 
0 & \sigma^2 & \dots & 0 \\ 
\vdots &\vdots &\ddots & \vdots \\
0 & 0 & \dots & \sigma^2 } = \sigma^2I \]
Which concludes our proof. \\

The variance of $\hat{\theta}$ is given by:
\[ \var(\hat{\theta})
= \ebb[(\hat{\theta} - \ebb[\hat{\theta}])
(\hat{\theta} - \ebb[\hat{\theta}])^T]
= \ebb[(\hat{\theta} - \theta^*)
(\hat{\theta} - \theta^*)^T] \]
Since it is a vector. \\
Note that:
\[ \hat{\theta} - \theta^*
= \dfrac{1}{n}\hat{\Sigma}\inv 
\Phi^T\bl{Y} - \theta^*
= (\Phi^T\Phi)\inv \Phi^T
(\Phi \theta^* + \bl{\eps}) - \theta^*
= (\Phi^T\Phi)\inv \Phi^T \bl{\eps} \]
So we have:
\multiline{ \var(\hat{\theta})
& = \ebb[((\Phi^T\Phi)\inv \Phi^T \bl{\eps})
((\Phi^T\Phi)\inv \Phi^T \bl{\eps})^T] \\
& = \ebb[(\Phi^T\Phi)\inv \Phi^T \bl{\eps}
\bl{\eps}^T \Phi ((\Phi^T\Phi)\inv)^T ] \\
& = (\Phi^T\Phi)\inv \Phi^T 
\ebb[\bl{\eps}\bl{\eps}^T] 
\Phi ((\Phi^T\Phi)\inv)^T \\
& = (\Phi^T\Phi)\inv \Phi^T \sigma^2 I
\Phi ((\Phi^T\Phi)\inv)^T \\
& = \sigma^2 (\Phi^T\Phi)\inv (\Phi^T\Phi) 
((\Phi^T\Phi)\inv)^T \\
& = \sigma^2 ((\Phi^T\Phi)\inv)^T \\
& = \sigma^2((\Phi^T\Phi)^T)\inv \\
& = \sigma^2 (\Phi^T\Phi)\inv \\
& = \dfrac{\sigma^2}{n}\hat{\Sigma}\inv }
Whcih gives us that 
$\var(\hat{\theta}) 
= \dfrac{\sigma^2}{n}\hat{\Sigma}\inv$. \\

Now we are finally ready to 
calculate the excess risk of $\hat{\theta}$:
\[ \ebb[\risk{\ell}(\hat{\theta})] 
- \riskOptimal{\ell}
= \| \ebb[\hat{\theta}] - \theta^*
\|^2_{\hat{\Sigma}} +
\ebb[\|\hat{\theta} - \ebb[\hat{\theta}] 
\|^2_{\hat{\Sigma}}] \]
The first term, the bias, is just $0$. \\
The second term, the variance, we have to calculate.

So we have:
\[ \ebb[\|\hat{\theta} - \ebb[\hat{\theta}] 
\|^2_{\hat{\Sigma}}]
= \ang{\hat{\theta} - \ebb[\hat{\theta}],
\hat{\Sigma}(\hat{\theta} - \ebb[\hat{\theta}])}
= (\hat{\theta} - \ebb[\hat{\theta}])^T
\hat{\Sigma}(\hat{\theta} - \ebb[\hat{\theta}]) \]
Notice that this is just a scalar,
and since the trace of a scalar is just the scalar
itself, we can take the trace of this:
\[ \tr\para{(\hat{\theta} - \ebb[\hat{\theta}])^T
\hat{\Sigma}(\hat{\theta} - \ebb[\hat{\theta}])}\]
However, the trace has a cyclical property,
where:
\[ \tr(ABC) = \tr(CAB) = \tr{BCA} \]
So we can rewrite the expression:
\multiline{\tr\para{\hat{\Sigma}
(\hat{\theta} - \ebb[\hat{\theta}])
(\hat{\theta} - \ebb[\hat{\theta}])^T}
& = \tr\para{\hat{\Sigma} \var(\hat{\theta})}
= \tr\para{\hat{\Sigma} 
\dfrac{\sigma^2}{n}\hat{\Sigma}\inv} \\
& = \tr\para{\dfrac{\sigma^2}{n}
\hat{\Sigma}\hat{\Sigma}\inv}
= \tr\para{\dfrac{\sigma^2}{n}I} \\
& = \dfrac{\sigma^2 d}{n}}
Note that $d$ showed up here
since the trace is the sum of the diagonal
elements, and $\Sigma$
is a $d\times d$ matrix (the number of features). \\

So the excess risk, or convergence rate,
which tells us how good $\hat{\theta}$
is compared to the optimal parameters $\theta^*$,
is the value:
\[ \dfrac{\sigma^2 d}{n} \]
The smaller the convergence rate is,
the better $\hat{\theta}$ is. \\

This convergence rate implies that the
the estimator $\hat{\theta}$
is closer to being optimal if $n$ is large;
if it is trained on more data, 
it will be more accurate. \\

Also the more dimensions we have $d$,
the worse the convergence is.
We can think of this as the data becoming
more complex to learn. \\
If $d$ is large, then $n$ would need to be extremly
large in order to compensate for that. \\
This makes sense; the more complex our data is,
the more of it we need to learn it. \\

The empirical risk estimator
$\hat{\theta}$ is actually the best
one out of all unbiased estimators. \\

Note that we imposed on $\riskOptimal{\ell}$
that it be the optiaml risk for functions
from our linear hypothesis class. \\
We also assumed that we had a loss $\ell$
that is the quadratic, or squared loss. \\
All of the statistical analysis that we did 
here, including the convergence rate we got,
only apply if our assumptions hold. \\

\newpage

\subsection*{Statistical Analysis of Ridge Regression}

This part is the same as last part, but this time
with ridge regression. \\

Recall that ridge regression is just
a linear least square problem.

We have our ridge regression estimator 
$\hat{\theta}_\lambda$,
which also depends on the output
(which is random in this setting). \\

There may be the a $\lambda$ sweet spot,
since it creates a tradeoff between bias
and variance. \\

The excess risk will give us the best
tradeoff that we can find. \\

Once we have the optimal value of $\lambda$,
we can use it to write the convergence
rate again, but better. \\

Note that $d$, the number of dimensions of the feature
vector, does not affect the convergence rate. \\

Because we have $\sigma$ instead of $\sigma^2$
in the convergence rate, the bound depends less
on the noise in the data than in non-regularized 
risk. \\

Note that we have an upper bound for the excess risk,
at the end, not an equality, because of the way
we bounded $\lambda$ (typo on power point). \\

\newpage

\subsection*{Statistical Analysis in the Random 
Design Setting}

This is an optional section. \\

We can repeat the same steps we did in the last
two sections, but in the random design setting
instead of the fixed design setting. \\
where we have random datasets $\bl{X}$ and $\bl{Y}$. \\

I will not do this section. \\

\newpage

\subsection*{LASSO}

This section introduces a new regularizer. \\

1-norm regularization penalizes small and large
weights equally. \\

We compare 2-norm and 1-norm regularization. \\
Visually, it will be a square shape,
while the other is more of a circle. \\

It is not smooth, and does not have
a closed form solution. \\
We can still solve it, using iterative
optimization algorithms. \\

It is very sparse, meaning that it does
feature selection, such that $\Phi \theta$
will have many missing terms (less overhead). \\

Many other (non-smooth) regularizers also
induce sparsity. \\
They belong to a whole family
depending on norms used. \\

Typo (first regularizer should be 
$|x|^2 + |y|^2$). \\

Infinity norm does not induce sparsity
(looks like a square, but straight). \\

Note that many of these "norms"
when $p < 1$, it is not actually a norm
mathematically (triangle inequality
not satisfied). \\

We now just give the convergence rate of
several different norm regularizers,
no proofs, just the value. \\

Another type of sparsity besides
norms, we have group sparsity.
Then we just have a very high level
description. \\
They are also given by regularizers. \\

\newpage

\end{document}
