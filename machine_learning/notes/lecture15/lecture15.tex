
\documentclass[12pt]{article}

\usepackage[margin=1in]{geometry}

\input{../../../variables.tex}

\title{%
    \Huge Machine Learning \\
    \Large Lecture XVII
}
\date{2025-07-03}
\author{Michael Saba}

\begin{document}
\pagenumbering{gobble}
\maketitle
\newpage
\setlength{\parindent}{0pt}
\pagenumbering{arabic}

\section*{Deep Learning}
\subsection*{History}

Deep Learnig is a subfield of Machine Learning,
which itself is a subfield of Artificial Intelligence. \\

Deep Learning is not new, but advances in hardware
technology have only made it feasible recently. \\

\newpage

\subsection*{Neural Networks}

In regularized risk minimization, we have:
\[ \min_{f \in \ffrak} \dfrac{1}{n}
\sum_{i=1}^n \ell(f(x_i), y_i) + \rcal(f) \]
Where $f: \xcal \to \ycal$. \\

In deep learning, we just reparametrize $f$
such that it is the composition of many simple
functions. \\
These are the layers of the Neural Network. \\

In a feed-forward neural network, we have:
\[ f(x) = f_\theta(x) = (\lcal_{\theta_L}^{(L)}
\circ \lcal_{\theta_{L-1}}^{(L-1)} \circ \dots 
\lcal_{\theta_0}^{(0)} )(x)  \]
Where $\theta = (\theta_L, \dots \theta_0)$,
is the vector of parameters of each layer. \\
Here we have $L+1$ layers. \\

Each layer can have different dimensions.
\[ \lcal_{\theta_l}^{(l)}: \rbb^{d_{l-1}} \to \rbb^{d_l} \]
Layers take in input from the previous layer,
and output accoridng to their own dimension:

The input layer is:
\[\lcal_{\theta_0}^{(0)} = \lcal^{(0)} \]
It has no parameters, and just akes in the input. \\

The output of the neural network is the output of 
the last layer. \\

All layers but the first and last are called hidden
layers. \\

Each layer applies an affine function with certain
weights to the data, which is encoded in
a matrix $W_l \in \rbb^{d_l \times d_{l-1}}$. \\
Each neuron in layer $l$ apply these weights to all the
inputs they get from the previous layer: 
\[ W_l a_{l-1} \]
Here $W_l$ is the parameters $\theta_l$. \\
The way the matrix works is that each row in the
matrix is the weights of each neuron in the layer $l$,
which are then applies to the the input vector $a_{l-1}$,
through a dot product. \\
The computed value at this layer is called $z_l$:
\[ z_l  = W_l a_{l-1} \]
At each neuron, we can calculate this
using a dot product (and for the entire layer,
as a matrix multiplication):
\[ (z_l)_i = (W_l a_{l-1})_i = \sum_{j=1}^{d_{l-1}}
(W_l)_{ij} \, (a_{l-1})_j \]
We do this for each neuron, which is why we have a
matrix multiplication. \\

Each layer applies a coordinate-wise
function $\sigma_l$,
which means that each neuron in the layer applies
an activation function before outputting. \\
This means that after applying $W_l a_{l-1}$,
we then output $\sigma(W_l a_{l-1})$,
which is applied to each neuron's output independently
($\sigma$ is applied to each
neuron's compute dot product $(W_l a_{l-1})_i$). \\

The output of layer $l$ is thus the activation
\[ a_l = \sigma_l(W_l a_{l -1}) = \sigma_l(z_l) \]
Where $a_{l-1}$ is the output of the previous layer. \\

As mentioned before, we want to apply an affine function
onto the input $a_{l-1}$, so we want weights as well
as a bias; we can take care of that by
inputting $(x, 1)^T$ at each layer instead of just $x$,
so $W_l (x, 1)^T$ will have a bias term,
which just shifts the output (does not have $x$
in it). \\

Notice that the only thing that is tunable here are
the weight matrices $W_l$, so the optimization
will be with respect to:
\[ (W_1, W_2, \dots W_L) \in 
\rbb^{d_l \times d_{l-1} \times L} \]
There is no weight matrix for the input layer. \\

\newpage

\subsection*{Gradient Descent}

For a neural network with only $2$ layers, we
only have one weight matrix $W_1$. \\
The ojective to minimize is:
\[ \ell(\sigma_1(W_1 x), y) + \rcal(W_1) \]
Again we just used regularized empirical risk. \\
This is similar to non-deep machine learning models
because a neural network with $2$ layers actually
only has one layer, as the first is just the input. \\

But if we have more layers, we have:
\[ \min_{f \in \ffrak} \dfrac{1}{n}
\sum_{i=1}^n \ell(f_\theta(x_i), y_i) + \rcal(f\theta) \]
Where $f_\theta$ is a composition, which makes it more
complex. \\

If all the layers output smooth functions, then we can
optimize this objective using Gradient Descent
with respect to the weights. \\

For now we can ignore regularization. \\

In order to do Gradient Descent, we need the gradient
of the function $\ell(f_\theta(x), y)$,
which we can write as:
\[ \partialdd{\ell(f_\theta(x), y)}{\theta}
\qquad \OR \qquad \nabla_\theta \ell(f_\theta(x), y) \]
While these two values carry the same amount 
of information, they are not the exact same; 
one is the transpose of the other. \\

So how can we calculate the derivative? \\
We need the derivative of the loss
of $f_\theta$, 
which itself is a very lagre composite function. \\
We can do this derivative for each input $x_i$
independently. \\

We can approximate derivatives numerically:
\[ \partialdd{f}{x_i}(x) \approx 
\dfrac{f(x + h e_i) - f(x)}{h} \]
As $h$ approaches $0$, the limit of the above expression
is the derivative; but for a small $h$
we can numerically compute it. \\
We can think of this as a secant line between two very
close points, which approximates the tangent. \\

Note that the smaller $h$ is, the better. \\

What we saw earlier was a forward numerical
derivative. On the other hand:
\[ \partialdd{f}{x_i}(x) \approx 
\dfrac{f(x + h e_i) - f(x - h e_i)}{2h} \]
This is a centered one. \\

The issue with these numerical derivatives is
that they are slow and innacurate. \\
Recall that the Neural Network may be huge. \\

A better approach is to analytically
calculate the gradient of the function $f_\theta$. \\
This is difficult, but we can calculate the 
gradient at each layer, since $f_\theta$
is the composition of simple functions. \\
We can use the chain rule to then combine them. \\
The composition of $f_\theta$ into the loss is
just the last composition of the chain rule. \\

Recall that if $f = g \circ h$, then by the chain rule:
\[ \partialdd{f}{x}(x)
= \partialdd{g}{h}(h(x))\partialdd{f}{x}(x) \]
If $g$ and $h$ are simple, then analytically
calculating their derivative is quite fast
and easy. \\

If $f: \rbb \to \rbb$,
then ther derivative of $f$ is simple. \\

If $f: \rbb \to \rbb^n$, then the derivative
is just a vector:
\[ \dd{f}{x}
= \pmat{\dd{f_1}{x} \\ \vdots \\ \dd{f_n}{x}} \]
We can just compute the derivative
of $f$ component-wise. \\

If we have $f: \rbb^m \to \rbb$, then
the derivative is a row vector
with respect to each input variable:
\[ \partialdd{f}{x}
= \pmat{\dd{f}{x_1} & \dots & \partialdd{f}{x_m}} \]
Where we use partial derivatives since we have
multilple inputs. This is just the gradient. \\

Finally, if we have $f: \rbb^m \to \rbb^n$, then:
we will have to combine these two approaches,
to get a Jacobian for a derivative:
\[ Df(x) = J_f = \pmat{\partialdd{f_1}{x_1} & \dots & 
\partialdd{f_1}{x_m} \\ \vdots & \ddots & \vdots \\ 
\partialdd{f_n}{x_1} & \dots & \partialdd{f_n}{x_m}} \]
Where $J_p \in \rbb^{n \times m}$. \\

If we have $f = h \circ g$, then we
can calculate the Jacobian of $f(x)$ as:
\[ Df(x) = Dh(g(x))Dg(x) \]
Which is just the mutlivariate chain rule. \\
Notice that this just the single variable chain
rule, but with Jacobians. \\

Each column of $Df(x)$, which we can write as:
\[ Df(x)e_i = \partialdd{f(x)}{x_i} \]
Here $e_i$ is the $\nth(i)$ standard basis
vector:
\[ e_i = \bmat{0 \\ \dots \\ i \\ \dots \\ 0} \]
So this is the partial derivative of 
$f$ with respect to one variable $x_i$. \\

So if we specifically want to use the
multivariate chain rule to calculate
the partial derivative with respect to $x_i$:
\[ Df(x)e_i = Dh(g(x))[Dg(x)e_i] \]
We can use this formula.

We can also use the matrix $\nabla f(x) = Df(x)^T$,
the gradient matrix of $f$, which is just the transpose
of the Jacobian. \\
Both are used in the multi-variate chain rule. \\

As mentioned earlier, we need the chain rule
in order to combine each layer's derivative
into the derivative of the entire function. \\
The Jacobian is used for forward propagation,
from the input to output layer, while the gradient 
matrix is used for back propagation. \\

Forward propagation is good if $n < m$,
the output dimension is larger than the input's. \\

Back propagation mode is more efficient
in our case however.
This is because the final composition with 
the loss $\ell$ maps the entire input to one number.
The input on the other hand, may be very large. \\

In backpropagation, we use the following formula:
\[ \nabla f(x) \bar{y} = 
\nabla g(x)[\nabla h(g(x)\bar{y})] \]
Here $y = f(x)$ and $\bar{y} = \partialdd{f(x)}{x}$. \\

So we start at the output, where we have:
\[ \nabla_w \ell(f_\theta(x), y) \]
And then we back propagate an error term. \\


\end{document}
