
\documentclass[12pt]{article}

\usepackage[margin=1in]{geometry}

\input{../../variables.tex}

\title{%
    \Huge Machine Learning \\
    \Large Lecture I
}
\date{2025-04-26}
\author{Michael Saba}

\begin{document}
\pagenumbering{gobble}
\maketitle
\newpage
\setlength{\parindent}{0pt}
\pagenumbering{arabic}

\section*{Bayesian Learning Theory}

\subsection*{Supervised Learning}

In supervised learning,
our goal is to use a large,
labeled, sample of data points,
to train some algorithm
to make predictions on some new,
unseen data. \\

More formally,
given an input and output measurable
space
\[ (\xcal, \X) \qquad 
\text{ and } \qquad (\ycal, \Y) \]
we have set of input output 
observations
\[ (x_i, y_i) \in \xcal \times \ycal \]
we need to train some classifier $f$
to pick up on patterns in this data,
such that, if $f$
were given some input $x$
it has not seen before,
$f$ is able to predict the corresponding
label $y \in \ycal$.
If the label it has predicted
matches the label we have for $x$,
then the prediction was good.
We call $(x, y)$ the testing data. \\

We can assume that our data
$(x_i, y_i)$ is
generated by a joint probability
distribution $\Pb_{(X, Y)}$
on the measurable space
$\xcal \times \ycal$. \\
The random variables $X$ and $Y$
are independent,
and have the same distribution in the
testing and training data. \\

The classifer $f$ is a function
\[ f: \xcal \to \ycal \]
that maps some input data point $x$
to a label $y$
in a way maximizes some
measure of performance. \\
It is also called a learning rule,
prediction, algorithm... \\

The fact that $f$
has not yet seen this data point $(x, y)$
is integral to ensure it has generalized
to data beyond what we have trained it on,
and can be used to interpolate and
extrapolate information,
such as being able to make predictions
and classify real life data
that hasn't been labled by a human
beforehand. \\

Usually, the input $\xcal$
of the classifier is a Euclidian
vector space $\R^n$. \\
For instance, if the input 
is a $28$ by $28$ pixel image,
then we have a $\R^{784}$
input vector space. \\

As for the output,
it can be one of multiple
things depending on what we
are classifying:
\begin{enumerate}
    \item 
    Binary Classification:
    The output is $\ycal = \{0, 1\}$.
    \item 
    Multi-class Classification:
    The output is $\ycal = \{0, 1,
    \dots d_y \}$,
    where $d_y$ is the number of classes.
    \item 
    Regression:
    The output is continuous,
    meaning that
    the output is some Euclidian
    vector space $\ycal = \R^{d_y}$.
    \item 
    Structured Output:
    Sometimes, we may want
    an output that is a graph, 
    an image, a video... \\
\end{enumerate} 

Underfitting and overfitting
are issues that can take place
when training our classifier. \\
If the classifier is not complex
enough to capture the nuances
of the training data
(for instance, using a linear
classifier for data that is not linear),
then we have an underfitted classifier. \\
On the other hand,
overfitting takes place when the
classifier we are using is too complex,
and picks up on nuances we don't want
to be picked up on.
For instance, all real world data
is going to come accompanied by some
noise.
If the classifier were to internalize
that noise, then it will
become very good at making predictions
for the training data inputs,
but not so much for new unseen data.
It would generalize very badly. \\

More formally, for a single fixed
input $x$,
the output is not necessarily
a determinstic function of $x$
(a single constant value $f(x)$).
It can instead be a random variable 
\[ Y = f(x) + \varepsilon \]
where $\varepsilon$
is a zero-mean noise random variable. \\
When we say some function is not
deterministic of an input,
it means the output does not depend
only on the input,
but on some randomness. \\

Other issues for supervised
learning include
a large training data which
can cause computational issues,
weak link between input
and output data,
and the criterion of the performance
not being well defined
(the criterion is important
because it is used to measure
how well the prediction is,
and can thus be used to guide
the training process). \\

To solve these issues,
we need a rigorous mathematical model
that allows us to clearly analyze
performance and describe the setting. \\

A machine learning algorithm is
what is used to train the classifier
$f$ to label data points correctly.
It is a distinct algorithm from the one
used by $f$ to actually label the data. \\
The classifier $f$
takes an input $x \in \xcal$
and makes a prediction $y \in \ycal$. \\
On the other hand,
a machine learning algorithm $\ascr$
is a function that takes as input
$n$ training data sample points
$(x_i, y_i)$,
and returns a classifier $f$
that can make predictions.  \\

More formally, the machine learning
algorithm $\ascr$ is a function
\[ \ascr: (\xcal \times \ycal)^n
\to \ffrak \]
where $\ffrak$ is hypotheses class. \\
We can't possibly write an algorithm
that can generate any function
$f$ that can make predictions. \\
Instead, we usually provide a sort of
structure for the function we want,
in such a way that the machine learning
algorithm can refine the function
in a way that leads to accurate
predictions. \\

The hypotheses classes can be thought
of as the model we are training,
such as linear regression, SVM,
neural network... \\
The idea is that each hypotheses
class is a function
with a specific structure,
where each function $f$ differs only
by certain parameters $\omega_i$.
The job of the machine learnign algorithm
is to find the optimal parameters
for $f$ to accurately make
predictions. \\ 
So while the classifier $f$
takes as input $x$
and spits out $y$,
the machine learning algorithm $\fscr$
takes as input the parameters $\omega_i$
of the hypotheses class,
and returns the optimal parameters
for the function $f$ to make
predicitons. \\

So how can we train
our classifier to make predictions? \\
Suppose we have an input
random variable $X$
that represents the height of a person,
such that $\xcal = \R$,
and an output random variable $Y$
that represents the two sexes,
such that $\ycal = \{ M, F \}$. \\
Suppose that we know that
\[ \Pb_Y(F) = 0.6 \qquad \text{ and } 
\qquad \Pb_Y(M) = 0.4 \]
Our classifier $f$ takes $x$
as input,
and returns $M$ for male and $F$
for female. \\
Possible classifiers include:
\begin{enumerate}
    \item 
    If we only use the prior knowledge
    of how likely it is that a person
    is male or female,
    then we can make a prediction
    that $f(x) = F$
    if:
    \[ \Pb_Y(F) > \Pb_Y(M) \]
    This method is barely better
    than random luck; it only takes
    into account the distribution
    of the sexes, without even considering
    the height we have.
    \item
    The second method uses conditional
    density.
    We make a prediction that $f(x) = F$
    if:
    \[ p(X = x \mid Y = F)
    > p(X = x \mid Y = M) \]
    where $p$ is the density function. \\
    This method classifies the
    given person based
    of a woman or man has the given
    height.
    It is better than the last method,
    but it only takes into account
    the probability that someone
    has some height given that they
    are male or female,
    it does not take into consideration
    the prior knoweldge of how
    likely it is that a person
    is male or female to begin with. \\
    This a well known paradox;
    for instance,
    if the underlying probability
    that a person is male is low
    to begin with,
    then even if the probability
    that someone has some height
    is higher for a male,
    it might not be a good idea
    to predict that the person 
    is a male due to the low
    probability that anyone
    is a male to begin with. 
    \item
    The final method,
    which uses posterior probability,
    takes into account both
    the height and the prior knowledge
    of the sexes distribution. \\
    We predict that $f(x) = F$
    if:
    \[ \Pb(Y = F \mid X = x)
    > \Pb(Y = M \mid X = x) \]
    This compares the probability
    that someone is female or male
    given the height. \\
    Since $\Pb(Y = F \mid X = x)
    = \Pb(Y = F \cap X = x) 
    \cdot \Pb(Y = F)$,
    both the height and prior knowledge
    about the sexes distribution
    is taken into account,
    combining the first two approaches.
\end{enumerate}
We will be using the last method
for our classifiers. \\

Why did we use the conditional
density $p$ function
in the second method instead of
the conditional probability $\Pb$? \\
It is because the height random variable
$X$ is continuous,
so the probability of any one
height $X = x$ is just $0$,
which forces us to use the density. \\
The density can also be used to compare
the likelyhood of events. \\

Of course, the same argument
can be made for the third method:
doesn't the fact that the probability
$X = x$ is $0$
mean that the conditional
probability $\Pb(Y \mid X = x)$
is not well defined? \\
The answer is yes,
and we can solve this issue by using
the conditional density instead.
However, the density function
for the conditional distribution
may not always exist. \\
Luckily there are other ways to solve 
the issue that utilize the probability
distribution, and reformulate it in
such a way that it is always well
defined. \\
We will see them in the next section. \\

\newpage

\subsection*{Decision Theory}

Let us now formalize the ways
in which a classifier might
make a decision. \\
Recall that we settled on using
the posterior probability
$\Pb(Y \mid X = x)$
in making our classification
of the input $x$. \\

The first thing we have
is an underlying probability
space
$(\Omega, \fscr, \Pb)$. \\

We also have two measurable
spaces, an input space $\xcal$
and output space $\ycal$:
\[ (\xcal, \X) \AND (\ycal, \Y) \]
as well as two random variables
\[ X: \Omega \to \xcal \AND 
Y: \Omega \to \ycal \]
that represent the input
and output.
The random variables $X$ and $Y$
are independent. \\

The marginals
\[ \Pb_X = \Pb \circ X\inv 
\AND \Pb_Y = \Pb \circ Y\inv \]
are probability measures
in the input and output spaces. \\

If we join the two measurable spaces,
we get the joint space:
\[ (\xcal \times \ycal,
\X \otimes \Y) \]
which can be accompanied by the
joint probability measure
\[ \Pb_{(X, Y)}
= \Pb \circ (X, Y)\inv \]
which can also be written as
\[ \Pb_{(X, Y)} 
= \Pb_X \otimes \Pb_Y \]
since $X$ and $Y$ are independent. \\

We call the measure space
\[ (\xcal \times \ycal,
\X \otimes \Y, \Pb_{(X, Y)}) \]
the data space. \\

We can use the projections
\[ \pi_X: \xcal \times \ycal \to \xcal 
\AND 
\pi_Y: \xcal \times \ycal \to \ycal \]
where
\[ \pi_X(x, y) = x \AND \pi_Y(x, y) = y \]
to get the marginal distribution
from the joint distribution:
\[ \Pb_X = \Pb_{(X, Y)}
\circ (\pi_X)\inv \qquad 
\Pb_Y = \Pb_{(X, Y)} \circ 
(\pi_Y)\inv \] 
Here $(\pi_X)\inv$ and $(\pi_Y)\inv$
return the whole set of
outcomes $(x, y_i)$
and $(x_i, y)$
respectively (since the projections
are not injective). \\

Putting it all together in one
diagram, we have:
\tikzFigure{
    \node at (0, 0) 
    {$(\Omega, \fscr, \Pb)$};
    \node at (5, 3) 
    {$(\xcal, \X, \Pb_X)$};
    \node at (5, -3) 
    {$(\ycal, \Y, \Pb_Y)$};
    \node at (10, 0) 
    {$(\xcal \times \ycal,
    \X \otimes \Y, \Pb_{(X, Y)})$};

    \draw [->, shorten >=25, shorten <=25]
    (0, 0) -- (5, 3)
    node[midway, yshift=10, xshift=-10] {$X$};
    \draw [->, shorten >=25, shorten <=25]
    (0, 0) -- (5, -3)
    node[midway, yshift=-10, xshift=-10] {$Y$};
    \draw [->, shorten >=65, shorten <=30]
    (0, 0) -- (10, 0)
    node[midway, yshift=10] {$(X, Y)$};
    \draw [->, shorten >=25, shorten <=25]
    (10, 0) -- (5, 3)
    node[midway, yshift=10, xshift=10]
    {$\pi_X$};
    \draw [->, shorten >=25, shorten <=25]
    (10, 0) -- (5, -3)
    node[midway, yshift=-10, xshift=10]
    {$\pi_Y$};
}

Our goal is to use the posterior
probability $\Pb(Y \mid \X = x)$
to make a decision.
However, because $X$
may be continuous,
the probability that any one event
will take place is $0$,
which will cause issues. \\
So we are better off using the 
conditional density. \\

If we have a joint density function
\[p_{(X, Y)}: (\xcal, \ycal) \to \R^+ \]
then we can use it to calculate
the conditional density. \\

First, we need the marginal density $p_X$,
which can be computed from the joint
density as follows
\[ p_X = \int_{\ycal} p_{(X, Y)}(x, y)dy \]
which satisfies the conditions
for being the density of $\Pb_X$. \\

We can then calculate the conditional
density as follows:
\[ p_{Y \mid X = x}(y)
= \piecewise{
    \dfrac{p_{(X, Y)}(x, y)}{p_X(x)},
    \quad \text { if } p_X(x) > 0
    \vcal{10pt}
    }{
        0, \qquad \qquad
        \quad \,\, \text{ otherwise }
    }
\]
This is useful if we can find the
joint density. \\
If not, then we are forced 
to use the probability distribution,
which as we mentioned earlier,
carries a risk,
since $X$ is continuous,
and $\Pb_X(X = x) = 0$. \\

We can define this conditional
probability using the conditional
density:
\[ \Pb_{Y \mid X = x}(A)
= \int_{A} p_{Y \mid X = x}(y)dy
\qquad \forall A \, \in Y \]
this is just the standard defintion
of a distirbution over some density function. \\

Note that we can infer that
\[ p_{(Y, X)}(x, y) = p_{Y \mid X = x}(y)
\otimes p_{X}(x)  
\implies \Pb_{(Y, X)}(x, y)
= \Pb_{Y \mid X = x}(y) \otimes \Pb_{X}(x) \]
which is just Baye's Rule. \\

The posterior $\Pb(Y \mid X = x)$,
is not well defined when $X$ is continuous,
since the probability that $X = x$
is $0$,
and 
\[ \Pb(Y \mid X = x) = 
\dfrac{\Pb(Y \cap X = x)}
{\Pb(X = x)}  \]
which leads to division by $0$.
We will see how we can fix this
(assuming we don't have the density)
in the next section. \\

\newpage

\subsection*{Conditional Probability}

This section is optional. \\
In pratice we only need to know
that we are using the conditional
probability, which may not always be
well behaved, 
and as such we may need an alternative
method to express it. \\

Suppose we have two events 
$A, B \in \fscr$,
then we can define the conditional
probabilty as follows:
\[ \Pb(B \mid A)
= \dfrac{\Pb(B \mid A)}{\Pb(A)} \]
which is a well defined probability measure,
but only so long as $\Pb(A) \neq 0$. \\

We can condition on a random variable
\[ X: \Omega \to \xcal \]
with space $(\xcal, \X)$
by just using the inverse $X\inv$. \\
So if want to condition on a set $C \in \X$,
such that $A = X\inv(C)$,
we have $\Pb(X \in C) = \Pb(A)$. \\

The issue described above would thus occur
when we try to condition over a single event:
$X\inv(x) = \emptyset$,
which gives us $\Pb(X = x) = 0$. \\

We need to control the nullsets
$X\inv(x)$ in a consistent way
in order to condition on single events. \\

The first technique we can use
is to define the conditional probability
in terms of the expected value of
an indicator function. \\

As we know:
\[ \Pb(A) = \E[\indicator_A] \]
holds for any measurable subset $A$. \\

We can extend this to conditional
probabilities. \\

First we define Borel $\sigma$-algebra
generated by the random variable $X$:
\[ \sigma(X) = 
\{X\inv(C) \mid C \in \X\} \]
which is basically
the preimage of all measurable
events in $\xcal$,
but in the probability space $\Omega$. \\

Now, the conditional probability
of some event $B \in \fscr$
conditioned on the random variable:
\[ \Pb(B \mid X) \]
can be written as
\[ \Pb(B \mid \sigma(X)) \]
which is more consistent
as both $B$ and $\sigma(X)$
belong to the probability space. \\
This means that:
\[ \Pb(B \mid X) 
= \Pb(B \mid \sigma(X))
= \E[\indicator_B \mid \sigma(X)] \]
The issue with the expected
value approach however is that it is
not well defined for all values $B \in \fscr$
and $X = x$. \\

The second, better approach
that is actually used in practice is
probability kernels. \\

Given two measurable spaces
\[ (\ucal, \U) \AND (\vcal, \V) \]
then there exists a mapping
\[ \kappa: \ucal \times \V \to [0, 1] \]
that maps one outcomes from $\ucal$
and one measurable subset in $\vcal$
to a probability. \\
For a fixed value $B \in \V$,
the function is measurable,
and for a fixed $u \in \ucal$,
the function is a measure,
called the probability kernel.
This gives us the measure space
$(\vcal, \V, \kappa(u, \cdot))$,
which actually fulfills the
conditions of a probability space,
meaning that $\kappa(u, \vcal) = 1$. \\

So, to put things into perspective,
if we fix one element $u \in \ucal$,
we get a probability measure
$\kappa(u, \cdot)$
that returns the measure of
some single outcome $u$
and a measurable subset $B \in \V$. \\

On the other hand,
if we were to fix some measurable
subset $B \in \V$ on the other hand,
we would get a measurable function
$\kappa(\cdot, B)$.
This allows us to intgegrate over
multiple values of $u \in \ucal$.
This is useful in this case for example:
\[ \Pb(B) = \int_{\ucal} \kappa(u, B)
\Pb_U(du) \]
which is necessary in order
to calculate the probability
of some event $B$
(which is the sum of all
the conditional probabilities
of $B$). \\

The kernel can represent any probability,
but in our case we need it to represent
the conditional probability. \\
So if a kernel:
\[ \kappa: \xcal \times \Y
\to [0, 1] \]
satisfies the equation
of the conditional probability:
\[ \Pb_{(X, Y)}
= \Pb_{X} \otimes \kappa \]
then we will denote the kernel as
\[ \kappa_{X \mid Y} \]
as it will represent a well behaved
conditional probability measure. \\
This function will satisfy:
\[ \Pb_{Y \in B \mid X}(\omega)
= \kappa_{X \mid Y}(X(\omega), B) \]
for any $B \in \Y$. \\
It will also satisfy the equation:
\[ \E[f(X, Y) \mid X](\omega)
= \int_{\ycal} f(X(\omega), y)
\kappa_{X \mid Y}(X(\omega), dy) \]
for any measurable function
$f: \xcal \times \ycal 
\to [0, \infty)$. \\

The first equation just tells us that
the kernel is the same as the conditional,
but is better behaved
(is well defined even for
single events in $X$). \\
The second equation is the following
property of expected values:
\[ \E[f(X)] 
= \int_{\xcal}f(x)\Pb_X(dx) \]
but used on the conditional probability. \\
The point is to show that the kernel
can be treated the same way as the
actual conditional probability. \\

We won't get into how to find the
kernel for now, but it sufficed to know
it exists and solved our problem. \\

\newpage

\subsection*{Law of Total Expectation}

The law of total expectation
gives us a formula for the expectation
of $f(X, Y)$ in terms of the conditional
expectation. \\

For a measurable function:
\[ f: \xcal \times \ycal \to \R \]
as a consequence of Fubini's Theorem,
as well as the definition of
conditional probability:
\[ \Pb_{(X, Y)} = \Pb_{Y|X}
\otimes \Pb_X \]
We can then evaluate the following
expected value
\[ \E[f(X, Y)] \]
First we use the definition
of the expected value:
\[ \E[f(X, Y)]
= \int_{\Omega}f(X(\omega), Y(\omega))
\cdot \Pb(d\omega) \]
Now, we know that
for a measurable function $f$
that maps $\xcal \times \ycal$
to $\R$,
the expected value can be rewritten
as such:
\[ \E[f(X)] = \E_X[f] \]
Which means that we can change from
the probability space $\Omega$ to the
measure space $\xcal \times \ycal$:
\[ \E[f(X, Y)]
= \int_{\xcal \times \ycal} f(x, y)
\cdot \Pb_{(X, Y)}(dx, dy) \]
By Fubini's Theorem,
we can decompose the joint
measure space with measure $\mu \otimes v$
into two probability spaces
with measures $\mu$ and $v$. \\
This can be done for the joint
probability $\Pb_{(X, Y)}$,
since we know that 
\[ \Pb_{(X, Y)} = \Pb_{Y|X}
\otimes \Pb_X \]
which means that:
\[ \E[f(X, Y)]
= \int_{\xcal} \int_{\ycal} f(x, y)
\cdot \Pb_{Y \mid X = x}(dy)
\Pb_X(dx) \]
Now as we know, the expected
value of a probability distribution
is just the sum (integral)
of all events weighted by the
probability of their taking place.
So the intergal
\[ \int_{\ycal} f(x, y)
\cdot \Pb_{Y \mid X = x}(dy) \]
is just the expected value of $f$
of the
conditional probability for a fixed $x$.
This means that we have:
\[ \E[f(X, Y)]
= \int_{\xcal} \E[f(x, Y)
\mid X = x]
\Pb_X(dx) \]
which can be brought back to the
probability space:
\[ \E[f(X, Y)]
= \int_{\Omega} \E[f(X, Y \mid X)](\omega)
\Pb(d\omega) \]
This integral is also a sum
over a distribution weighted
by a probability.
So it can also be expressed as
an expected value:
\[ \E[f(X, Y)]
= \E[\E[f(X, Y \mid X)]] \]
And with that we are done. \\

\newpage

\subsection*{Decision Theory Continuation}

Now going back to the idea of classifiers.
We know that we are using the conditional
probability
to measure the likelihood of some event
$B$:
\[ \Pb_{Y \mid X = x}(B) \]
But what to do once we get this value? \\

If we have a binary classifier $f$
(for example, like the one we saw
with labels $Y = M$ or $Y = F$,
representing the sexes),
then we could use a threshold of $0.5$. \\
If $\Pb_{Y \mid X = x}(F) > 0.5$
then $f$ predicts female, else it
predicts male. \\

In a more general classification case,
we want a classfier $f$ such that:
\[ f(x) \in \text{argmax}_{y \in \ycal} \;
\Pb_{Y \mid X = x}(y) \]
which means we want to pick the event $y$
such that the conditional probability
is maximal. \\

This approach is usually our main goal,
though it is sometimes flawed. \\
For instance, similar to our issue
with $X$,
if $Y$ is continuous, the probability
of any one single event is $0$
(can't be remedied with a kernel
since it is not the variable being
conditioned over). \\
Sometimes the maximum is also not always
the right choice (in a multimodal
distribution, there may be several peaks). \\

A better way is to use a loss function
as a measure of performance for a
classifier on one sample point. \\
A loss function $l$ is a mapping:
\[ \ell: \ycal \times \ycal
\to \R^+ \]
which maps tuples $(z, y)$,
where $z$ is the predicted label $f(x)$,
and $y$ is the actual label of $x$,
to a positive number. \\
The smaller the number is,
the better $f$'s prediction is. \\

In the case of classification
(discrete output label $\ycal$,
regardless of whether it is binary
or multiclass),
the loss function is the same as
the indicator function:
\[ \ell(z, y) = \indicator_{z \neq y} \]
That is to say,
if $\ell(z, y) = 1$,
then the two labels are not equal,
which makes the classification bas (large
loss). \\
On the other hand, if $\ell(z, y) = 0$,
then the predicted label is the same as
the actual label,
which means the classification is good
(low loss). \\
We call this the 0-1-loss. \\

Note that the 0-1-loss $\ell(a, b)$ is
just the indicator function
$\indicator_{a \neq b}$
(we use $\neq$ since it's 1 when they're
not equal). \\

On the other hand, in the case of
regression (where $\ycal = \R$
is continious),
then the best way to measure
loss is by finding the distance
between the predicted label and the
actual label:
\[ \ell(z, y) = (z - y)^2 \]
here we take the distance square
because the loss must be positive
(and we don't care whether the
$z$ is smaller or larger than $y$,
we only care for the absolute distance).
We use the square and not the absolute
value because the square is differentiable
on the entire domain of $\ell$,
a good property to have when the goal
is optimization. \\
So as the absolute distance
grows between $z$ and $y$,
so does the loss.
If the prediction is so good that
$z \sim y$,
then $\ell(z, y) \sim 0$. \\
We call this the squared loss. \\

We can generalize this for 
multivariate regression
where the output is a vector space
$\R^n$. \\
We can just use the norm
of the difference vector $z - y$
which measure the distance
between $z$ and $y$:
\[ \ell(z, y) = \|z - y\|^2 \]
We use the norm squared. \\

Given a measurable prediction function
\[ f:\xcal \to \ycal \]
and a loss function:
\[ \ell: \ycal \times \ycal \to \R^+ \]
and a probability distribution
$\Pb_{(X, Y)}$ on $\xcal \times \ycal$,
the expected risk of $f$
is defined as:
\[ \rscr_\ell(f) = \E[\ell(f(X), Y)] \]
It is the expected value of the loss,
which compares the outputs of the classifier 
and the actual label. \\
So we can think of the risk as an evaluation
of how good of a predictor it is,
but this time over the entire distribution,
not just one point $(x, y)$. \\

We can write:
\[ \rscr_\ell(f) = \E[\ell(f(X), Y)] 
= \int_{\Omega} \ell(f(X(\omega)), Y(\omega)) 
\Pb(d\omega) \]
which means that
\[ \rscr_\ell(f) = \E[\ell(f(X), Y)] 
= \int_{\xcal \times \ycal}
\ell(f(x), y) \Pb_{(X, Y)}(dx, dy) \] \\

The risk is a deterministic quality 
of the the classifier,
meaning that for some classifier $f$,
the risk does not depend on randomness,
but is just a single number
(not a random variable) that depends only
on $f$. \\

Now, one issue is that in real
life we don't have the full distribution.
We have some sample called the training data,
on which $f$ is trained. \\
That means that the classifier $f_Z$
learnt from random data $Z$
(our choice of a training data sample),
which means that the risk $\rscr_\ell(f_Z)$
also depends on the random data $Z$. \\
Tangibly, what this means is that
the risk $\rscr_\ell(f_Z)$
doesn't just depend on $f$,
but also on $Z$,
a random variable of datasets
(a random variable where each element 
it a choice of one dataset),
which makes $\rscr_\ell(f_Z)$
itself a random variable,
a function $h(Z)$ of the random variable $Z$. \\

When we measure the risk of some
classifier $f_Z$,
we use the random variable $(X, Y)$
(testing data)
in the expected value of the risk formula. \\
We need $Z$ to be independent from 
the random variable $(X, Y)$.
This means that we need the training and testing
data to be seperate.
This has already been mentioned,
but doing so ensures that $f_Z$
generalizes beyond the training data $Z$,
which means it is not overfitted to $Z$. \\
If we use $Z$ in the testing process
as well,
then the risk would be low
(indicating a good classifier)
regardless of whether the classifier $f_Z$
actually generalizes to unsees data,
or just overfits $Z$. \\


The infimum of a set $S$:
\[ \inf(S) \]
is the largest number smaller
or equal to every number in that set. \\
If the set has a minimum,
then it is just the minimum:
\[ \inf(\{3, 1, 5\}) = 1 \]
But the infimum is also defined
when there is no minimum,
in which case the infimum
is not part of the set:
\[ \inf(\{ x \in \R \mid x > 0 \})
= 0 \]
Here $0 \notin \R^+$. \\

Baye's optimal risk,
denoted $\rscr_\ell^*$,
and given by:
\[ \rscr_\ell^*
= \inf(\{ \rscr_\ell(f) \mid f
\text{ is measurable } \}) \]
It is basically the best possible
classifier $f$
(one which has the lowest loss). \\
We use the infimum since
the risk is a positive real number,
so the best possible risk may not be
part of the set. \\

The function $f^*$
that attains this infimum
is called Baye's predictor,
or Baye's optimal learning rule. \\
Note that it is not always 
defined everywhere,
and may not be uniqe. \\

The excess risk of a
measurable learning rule 
$f:\xcal \to \ycal$ is: 
\[ \rscr_\ell^* - \rscr_\ell(f) \]
which is basically a measure
of how far the learning rule
is from being optimal
as a predictor. \\

\end{document}
