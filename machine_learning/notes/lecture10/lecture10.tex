
\documentclass[12pt]{article}

\usepackage[margin=1in]{geometry}

\input{../../../variables.tex}

\title{%
    \Huge Machine Learning \\
    \Large Lecture X, XI
}
\date{2025-06-02}
\author{Michael Saba}

\begin{document}
\pagenumbering{gobble}
\maketitle
\newpage
\setlength{\parindent}{0pt}
\pagenumbering{arabic}

\section*{Convex Optimization}
\subsection*{Convexity}

We've thus far seen multiple classification and regression
problems, including linear classification,
linear classification with ridge or LASSO regression,
Linear Discriminant Analysis, Logistic Regression,
and Support Vector Machine with hard and soft margins. \\

Note that we can also use least 1-norm minimization
where we use the 1-norm instead of the 2-norm 
for the empirical risk, which penalizes large
errors and small errors proportionally. \\

Sometimes there exists a closed form expression for 
the optimal solution, but other times we ar forced to use
numerical algorithms. \\

Recall that a convex function is a function:
\[ f: \rbb^d \to \rbb \]
such that for all $x, y \in \rbb^d$ and $\lambda \in [0, 1]$:
\[ f((1-\lambda)x + \lambda y) 
\leq (1-\lambda)f(x) + \lambda f(y)  \]
A function being convex implies that any local minimum
is a global minimum. \\

The pointwise supremum (minimum or maximum) of two 
convex functions, their sum, and non-negative
scaling of a convex function all yield a convex function. \\

One way to check that a function is convex
is to look at the second derivative.
In $\rbb$, that means that
$f''(x) > 0$ implies the function is convex. \\
More generally, in $\rbb^d$, 
a function is convex if the hessian $\nabla^2 f$
is twice differentiable. \\
This check only works if the function 
is twice-differentiable. \\

If it's only once-differentiable, we can also
check that:
\[ \ang{x_1 - x_0, \nabla f(x_1) \nabla f(x_0)} \geq 0 \]
which means the first derivative is increasing
(means the second derivative is positive). \\

A function is convex if it is convex on all line segments
(each one dimensional function). \\

A line segment between $x$ and $y$ is defined by:
\[ [x, y] = \{(1-\lambda)x + \lambda y 
\mid \lambda \in [0, 1] \} \]
We can also define half-open and open line segments
$[x, y)$, $(x, y]$, and $(x, y)$. \\

We say a set $C$ is a Convex Set if for any two
points $x$ and $y$ in the set, the line segment
$[x, y]$ is on the set:
\[ \forall x, y \in C \quad [x, y] \subset C \]
For example, the area defined by $x^2 - y \leq 0$
is a convex set. \\

\newpage

\subsection*{Optimization}

A convex optimization problem (including all problems
listed in the last section) looks like:
\[ \min_{\theta \in \Theta} f(\theta) \]
Where we try to find the optimal parameters $\theta$.
We call $\theta$ the optimization variable. \\

The problem being a convex optimization problem
assume that the objective (function) $f$:
\[ f: \rbb^d \to \rbb \]
is convex.
It also assumes that the constraint (parameter space)
$\Theta \in \rbb^d$ is a convex set. \\

An optimal solution $\theta^*$ such that
\[ f(\theta^*) = \inf_{\theta \in \Theta} f(\theta) \]
may or may not exist. 
Note that the reason we use infimum is that the
minimum may not exist (for example, $e^x$
doesn't have a minimum, but the infimum is $0$). \\
The solution may or may not be unique. \\
We need to know how to check for optimality. \\
There may or may not be a 
closed form expression of the solution $\theta^*$. \\
We also need to know what randomness the solution depends
on (the noise, the data...). \\

There are several classes of optimization problems:
\begin{enumerate}
    \item Linear Programming: The function $f$
    is linear and the constraint $\Theta$
    is given by a linear system $A\theta = b$.
    \item Non-Linear Programming: One of the conditions
    of linear programming fails.
    \item Unconstrained optimization, where the parameters
    can be any numbers $\Theta = \rbb^d$. 
    \item Smooth optimization: We can characterize classes
    by whether they are differentiable (smooth),
    including higher order of differentiability.
    \item Non-smooth optimization: The function is not
    smooth on its entire domain.
    \item Structure optimization.
\end{enumerate}
It is often easier to study them and try to solve them
individually. \\

If there exists a solution to a convex optimization
problem, but no closed-form expression,
then we will have to use an iterative numerical algorithm
that converges towards the solution as the number of
iterations increases. \\

We use an interative algorithm $\acal$
to solve problems from some 
(family) of optimization problems $\cscr$. \\
We expect that $\acal$ solves all problems $\pcal \in \cscr$. \\
The algorithm constructs an improving sequence
$(\theta^{(k)})_{k \in \nbb}$ that converges towards
the solution $\theta^*$ of $\pcal$. \\
The class of problems $\cscr$ is the set of rules
that the problems $\pcal$ follow. \\
The algorithm as access to some information about $\pcal$
at any point $\olsi{\theta}$;
this is called an orcal. For example,
$\acal$ can have access to $f(\olsi{\theta})$,
the derivative $\nabla f(\olsi{\theta})$,
and the second derivative $\nabla^2 f(\olsi{\theta})$.
A zero order algorithm only uses the function value, 
a first order algorithm only uses the first order
derivative... \\

We say that $\acal$ solves $\pcal$ 
if $(\theta^{(k)})_{k \in \nbb}$
converges towards the solution as $k \to \infty$. \\
We are often interested in the convergence rate
of a given algortithm. \\
\colorText{red}{TODO: epsilon.} \\

We can only really compare the performance of algorithms
that follow the same rules (solve problems from the
same class). \\

A zero-order algorithm is not expensive at all,
but is also inaccurate and slow to converge. Second-order
algorithms are faster and better but also more expensive
to compure. First-order algorithms are a good
tradeoff between the two. \\

\newpage

\subsection*{Convex Quadratic Minimization}

The class of Convex Quadratic Minimization problems
all look like:
\[ \min_{\theta \in \rbb^d} f(\theta)
\quad f(\theta) = \|\Phi \theta - y\|^2 \]
This class includes linear regression, regularized linear 
regression with ridge regression, and linear
discriminant analysis. \\

Notice that there is no constraint on the 
optimization variable $\theta$,
so this is an unconstrained optimization problem. \\

\colorText{red}{TODO: Properties.} \\

There are several algorithms that we can use to solve
problems of this class, including
a direct solver using a closed form epxression (as we
have seen before), gradient descent, and conjugate
gradient method. \\

\colorText{red}{TODO: Closed form solution.} \\

\newpage

\subsection*{Gradient Descent}

It is also called the steepest descent algorithm,
because at each iteration, we walk in the direction
of steepest descent. \\

This is a first-order algorithm, as we need the gradient
in order to find the direction $d$ of steepest descent
(we minimize the inner product). \\

We choose some step length $\tau$ to move in the direction
of steepest descent:
\[ \theta^{(k+1)} = \theta^{(k)} -\tau \nabla f(\theta^{(k)}) \]
We repeat these steps until the error is acceptable. \\

We choose some starting point $\theta^{(0)}$. \\

A heuristic for the step size is $\dfrac{2}{L + m}$. \\

\colorText{red}{TODO: Algorithm and convergence rate.} \\

We have a linear rate of convergence, where $\omega^k$
decreases since $\omega < 1$. \\
Note that the convergence rate we have
here does depend on the step size, but here the formula
assumes that we use $\dfrac{2}{L + m}$. \\

\colorText{red}{TODO: Condition Number.} \\

The condition number $\kappa = \dfrac{L}{m}$
impacts convergence. \\

The idea behind gradient-descent is that we are making
locally optimal decisions that lead us towards the 
optimal solution. If we use a bad step size, we get a
zig-zag behavior that slows down convergence. \\

\newpage

\subsection*{Conjugate Gradient Method}

This method was first created to solve linear systems
\[ Q\theta = b \]
where $Q$ is positive semi-definite and $b \in \rbb^d$. \\

This can be shown to be equivalent to solving
thq quadratic minimization problem:

\colorText{red}{TODO: All of this.} \\

This can be shown to be the best first-order algorithm
to solve convex quadratic minimization problems. \\

\colorText{red}{TODO: Conjugate direction method.} \\

The idea is that we generate directions for our
descent that are optimal in order to get to the solution
in an optimal number of iterations. \\

The generated directions span the whole space.
We can show the chosen directions are optimal. \\

We have a main update step, which uses several
parameters that are also computed each step. \\
We also compute a new direction. \\

\colorText{red}{TODO: Cost and convergence rate.} \\

The convergence rate is also linear, but it is much faster
since $\sqrt{\dfrac{L}{m}}$ is much smaller 
than $\dfrac{L}{m}$. \\

It does not generalize to non-linear problems. \\

\newpage

\subsection*{Smoothness}

Can we generalize Gradient Descent to non-quadratic
functions? \\

The first algorithm that can do that is called
gradient descent with line search,
however, we avoid it because it is too
costly, has no convergence rate because it
does not have a constant step size. \

The other methos is to 
bound the function at each point
by two quadratics, one below and one above.
This is done using strong convexity
and Lipschitz continuity. \\

L-smoothness gives us a bound on the function
by the Descent Lemma. 
So we rephrase it as a function
bounded by two quadratics. \\

The constant $L$ is used to control the
constant. \\

Note that if the original function $f$ was
convex, which we assume it is by default,
then it is bounded from above by a quadratic,
and from below by a linear function! \\

And if $f$ is strongly convex, then it is
bounded from above and below by quadratics
that are both turned up (convex)
instead of having the lower bound turned down. \\

The slides have the cases. \\

We don't need to do line search for the step
size, and can instead just use a fixed
size, so long as $0 < \tau < \dfrac{2}{L}$. \\

We now have a convergence rate
of $O\para{\dfrac{1}{k}}$. \\

This becomes a linear rate if the function
is strongly convex. \\

\newpage

\subsection*{Lower Bound for the Convergence Rate}

We can show, non-constructively (only in theory)
what the optimal first order iterative
numerical method is for solving
optimization problems. \\

Note that $\kappa = \dfrac{L}{m}$
is called the conditional. \\

After that we have lower bounds on the
function given some prior assumptions. \\

Optimal methods are those that have a convergence
rate proportional to the lower bound. \\

Conjugate gradient and heavy-ball method
are both optimal (in the strongly convex and 
L-smooth cases). \\

Heavy ball method is gradient descent 
with momentum. \\

Gradient descent is not optimal
in the convex and L-smooth case,
but Nesterov Accelerated Gradient method
is optimal. \\

We obviously don't consider the non-convex case. \\

\newpage

\subsection*{Nesterov's Accelerated Gradient
Method}

It is at least as costly as gradient descent
since it also uses the gradient. \\

It has an extrapolation step where
we find $\eta^{(k)}$, which is like
a small extra step
we take in a specific direction
we take each iteration. \\

Here we can also choose $t_k = \dfrac{k + 1}{2}$
which gets rid of
the first step in the iteration where we calculate
$t_{k+1}$, which is what gives
us the convergence rate. \\

This $t_k$ is not optimal, but it allows
us to have a new, tighter convergence rate. \\

The gradient is the only costly part of this
algorithm. \\

Note that it works for even non quadratic-functions,
so long as the smoothness and convexity
conditions are met. \\

\newpage

\subsection*{Proximal mapping}

But what about non-smooth problems,
where $f$ is not smooth?
Can we generalize the methods that
we already have? \\

In gradient descent we have:
\[ \theta^{(k+1)} = 
\theta^{(k)} - \tau\nabla f(\theta^{(k)}) \]
Which we can consider it to be the
discretization of the dynamical system:
\[ \theta(t) = -\nabla f(\theta(t)) \]
We can imagine that as the time step
becomes smaller and smaller, this defines
a curve each time walking in th direction
of the gradient. \\

This is called explicit euler discretization. \\

\colorText{red}{We can do implicit euler discretization
where we instead have a point $\theta^{(k)}$,
and we want to find a $\theta^{(k+1)}$
in terms of only $\theta^{(k)}$.} \\

We can then take our step:
\[ \theta^{(k+1)} = 
\theta^{(k)} - \tau\nabla f(\theta^{(k)}) \]
and set it equal to $0$,
and reinterpret it in a way that lets
us do steps that don't require the gradient,
and thus works for non smooth functions. \\
It instead uses the proximal mapping. \\

Note that $\delta_{C}(\theta)$
means the outputs are all $\theta$ is in $C$. \\

The intuitition is that we want to find the
closest point in a set $C$ to a point $\theta^{(k)}$,
which is just a projection. \\

The proximal mapping method is nice
is we can solve the optimization problem
we have instead of the gradient nicely. \\

We now have some examples of simple
proximal mappings. We can derive the solutions
of these proximal mappings. \\

\newpage

\subsection*{Proximal Gradient Descent}

Structured/Additive Composite optimization is 
when we assume the function we are trying
to optimize or minimize is split into
$f(\theta) + g(\theta)$
where the first is L-smooth and convex,
and the second is convex. \\

This fits nicely with the empirical risk
+ regularization model we have. \\

So we can do two steps, one for $f$
with gradient descent, one for $g$
with proximal mapping.
This is called proximal gradient descent. \\

We now have an example with the LASSO problem. \\
The 1-norm proximal mapping is 
some shrinkage operator we are given. \\

Then we are given the full algorithm. \\

Then we have convergence rates. \\

We can also use accelerated gradient descent 
here, where we use Nestorov instead of vanilla 
gradient descent. \\

Note that we don't call these
descent methods because there is no descent;
there is convergence, but not every step
gets us closer. \\

It is also called FISTA. \\

Note that while $\|\theta\|_1$
is prox-friendly, and can have proximal mapping
done to it,
this isn't true for $\|A \theta\|_1$
for some arbitrary matrix $A$. \\

\newpage

\subsection*{Duality}

Now we still have to find a way to solve
Hard and soft margin SVM, as well as regularized
least 1-norm minimization,
since they are not prox-friendly or are
non-smooth. \\

A disk is just a collection of points in space. \\

Or we can think of it as the intersection of
several half-spaces (all tangent to the circle). \\

We can call a closed set $C$
convex if it is the intersection of
closed half-spaces that contain it. \\

The same applies to convex functions:
a convex function $f$ is always the
pointwise supremum (max or min) of the
collection of affine functions $h$
such that $h \leq f$. \\
Think of it as lines of tangents below a graph
of $f$. \\

The conjugate of $f$ is $f^*$,
and it can be thought of as a function that 
operates on slopes;
it takes in a slope as input, and returns
the part where the affine function
intersects the $y$-axis.
It does so by sort of sliding the affine
function with the slope $\xi$
until it becomes tangent to $f(\theta)$. \\
We do that by maximizing the distance
between $f(\theta)$
and $\ang{\xi, \theta}$, 
which is the affine function. \\

The bi-conjugate is the conjugate of the
conjugate $f^{**}$. \\

The conjugate $f^*$ is alwasy convex. \\

If $f$ is convex, then $f = f^{**}$. \\

We then have some examples of conjugates. \\

The Fenchel-Rockafellar duality tells
us that if we have some optimization problem
with some structure:
\[ \min_\theta f(A\theta) + g(\theta) \]
then solving the primal problem corresponds
to solving the dual problem:
\[ \max_\xi - g^*(-A^T\xi) - f^*(\xi) \]
We do so by computing the conjugates of $f$
and $g$ seperately. \\
Notice that we ignore the $A$
in the primal problem. \\
Note that $f$ and $g$ must be convex
and $A$ must be a linear mapping. \\

The slides tell us how to derive
Fenchel-Rockafellar duality. \\

So we may be able to use proximal gradient
methods on the dual problem.
First we get the dual, which 
gets us a form solvable by proximal gradient
methods. \\

\colorText{red}{Moreau's identity.} \\

Then we are given an example using the
$\ell$-1 loss minimization with $\ell$-2
regularization, which is not solvable
in primal form since $\ell$-1 is non-smooth. \\

We calculate $f^*$ and $g^*$,
then we get a $\max$ dual problem, 
then a $\min$ of the negation. \\

Then we have a conclusion slide. \\

Then we are given how to solve soft,
hard margin SVM, and logistic regression
using the dual. \\

Note that we can write $\delta_{\rbb_-}(1-t)$
as a loss function for the hard-margin SVM. \\

So we just compute the conjugates
using the $\sup$ of inner product - $f\theta$,
then use Fenchel-Rockafellar duality. \\

\end{document}
