
\documentclass[12pt]{article}

\usepackage[margin=1in]{geometry}

\input{../../../variables.tex}

\title{%
    \Huge Machine Learning \\
    \Large Lecture X
}
\date{2025-06-02}
\author{Michael Saba}

\begin{document}
\pagenumbering{gobble}
\maketitle
\newpage
\setlength{\parindent}{0pt}
\pagenumbering{arabic}

\section*{Convex Optimization}
\subsection*{Convexity}

We've thus far seen multiple classification and regression
problems, including linear classification,
linear classification with ridge or LASSO regression,
Linear Discriminant Analysis, Logistic Regression,
and Support Vector Machine with hard and soft margins. \\

Note that we can also use least 1-norm minimization
where we use the 1-norm instead of the 2-norm 
for the empirical risk, which penalizes large
errors and small errors proportionally. \\

Sometimes there exists a closed form expression for 
the optimal solution, but other times we ar forced to use
numerical algorithms. \\

Recall that a convex function is a function:
\[ f: \rbb^d \to \rbb \]
such that for all $x, y \in \rbb^d$ and $\lambda \in [0, 1]$:
\[ f((1-\lambda)x + \lambda y) 
\leq (1-\lambda)f(x) + \lambda f(y)  \]
A function being convex implies that any local minimum
is a global minimum. \\

The pointwise supremum (minimum or maximum) of two 
convex functions, their sum, and non-negative
scaling of a convex function all yield a convex function. \\

One way to check that a function is convex
is to look at the second derivative.
In $\rbb$, that means that
$f''(x) > 0$ implies the function is convex. \\
More generally, in $\rbb^d$, 
a function is convex if the hessian $\nabla^2 f$
is twice differentiable. \\
This check only works if the function 
is twice-differentiable. \\

If it's only once-differentiable, we can also
check that:
\[ \ang{x_1 - x_0, \nabla f(x_1) \nabla f(x_0)} \geq 0 \]
which means the first derivative is increasing
(means the second derivative is positive). \\

A function is convex if it is convex on all line segments
(each one dimensional function). \\

A line segment between $x$ and $y$ is defined by:
\[ [x, y] = \{(1-\lambda)x + \lambda y 
\mid \lambda \in [0, 1] \} \]
We can also define half-open and open line segments
$[x, y)$, $(x, y]$, and $(x, y)$. \\

We say a set $C$ is a Convex Set if for any two
points $x$ and $y$ in the set, the line segment
$[x, y]$ is on the set:
\[ \forall x, y \in C \quad [x, y] \subset C \]
For example, the area defined by $x^2 - y \leq 0$
is a convex set. \\

\newpage

\subsection*{Optimization}

A convex optimization problem (including all problems
listed in the last section) looks like:
\[ \min_{\theta \in \Theta} f(\theta) \]
Where we try to find the optimal parameters $\theta$.
We call $\theta$ the optimization variable. \\

The problem being a convex optimization problem
assume that the objective (function) $f$:
\[ f: \rbb^d \to \rbb \]
is convex.
It also assumes that the constraint (parameter space)
$\Theta \in \rbb^d$ is a convex set. \\

An optimal solution $\theta^*$ such that
\[ f(\theta^*) = \inf_{\theta \in \Theta} f(\theta) \]
may or may not exist. 
Note that the reason we use infimum is that the
minimum may not exist (for example, $e^x$
doesn't have a minimum, but the infimum is $0$). \\
The solution may or may not be unique. \\
We need to know how to check for optimality. \\
There may or may not be a 
closed form expression of the solution $\theta^*$. \\
We also need to know what randomness the solution depends
on (the noise, the data...). \\

There are several classes of optimization problems:
\begin{enumerate}
    \item Linear Programming: The function $f$
    is linear and the constraint $\Theta$
    is given by a linear system $A\theta = b$.
    \item Non-Linear Programming: One of the conditions
    of linear programming fails.
    \item Unconstrained optimization, where the parameters
    can be any numbers $\Theta = \rbb^d$. 
    \item Smooth optimization: We can characterize classes
    by whether they are differentiable (smooth),
    including higher order of differentiability.
    \item Non-smooth optimization: The function is not
    smooth on its entire domain.
    \item Structure optimization.
\end{enumerate}
It is often easier to study them and try to solve them
individually. \\

If there exists a solution to a convex optimization
problem, but no closed-form expression,
then we will have to use an iterative numerical algorithm
that converges towards the solution as the number of
iterations increases. \\

We use an interative algorithm $\acal$
to solve problems from some 
(family) of optimization problems $\cscr$. \\
We expect that $\acal$ solves all problems $\pcal \in \cscr$. \\
The algorithm constructs an improving sequence
$(\theta^{(k)})_{k \in \nbb}$ that converges towards
the solution $\theta^*$ of $\pcal$. \\
The class of problems $\cscr$ is the set of rules
that the problems $\pcal$ follow. \\
The algorithm as access to some information about $\pcal$
at any point $\olsi{\theta}$;
this is called an orcal. For example,
$\acal$ can have access to $f(\olsi{\theta})$,
the derivative $\nabla f(\olsi{\theta})$,
and the second derivative $\nabla^2 f(\olsi{\theta})$.
A zero order algorithm only uses the function value, 
a first order algorithm only uses the first order
derivative... \\

We say that $\acal$ solves $\pcal$ 
if $(\theta^{(k)})_{k \in \nbb}$
converges towards the solution as $k \to \infty$. \\
We are often interested in the convergence rate
of a given algortithm. \\

An $\eps$-solution is a solution that is at
most $\eps$ off from the true global minimizer:
\[ \olsi{\theta} \leq \theta^* + \eps \]
This can be used to induce the convergence rate. \\

We can only really compare the performance of algorithms
that follow the same rules (solve problems from the
same class). \\

A zero-order algorithm is not expensive at all,
but is also inaccurate and slow to converge. Second-order
algorithms are faster and better but also more expensive
to compure. First-order algorithms are a good
tradeoff between the two. \\

\newpage

\subsection*{Convex Quadratic Minimization}

The class of Convex Quadratic Minimization problems
all look like:
\[ \min_{\theta \in \rbb^d} f(\theta)
\quad f(\theta) = \|\Phi \theta - y\|^2 \]
This class includes linear regression, regularized linear 
regression with ridge regression, and linear
discriminant analysis. \\

Notice that there is no constraint on the 
optimization variable $\theta$,
so this is an unconstrained optimization problem. \\

We will call:
\[ L = \|\Phi^T \Phi\| \quad \AND
\quad m = \lambda_{\min}(\Phi^T\Phi) \geq 0 \]
Which are respecively the largest and smallest
eigenvalues of $\Phi^T\Phi$. \\

There are several algorithms that we can use to solve
problems of this class, including
a direct solver using a closed form epxression (as we
have seen before), gradient descent, and conjugate
gradient method. \\

The direct solver has a closed form solution
as we saw in the last two chapters. \\

\newpage

\subsection*{Gradient Descent}

It is also called the steepest descent algorithm,
because at each iteration, we walk in the direction
of steepest descent. \\

This is a first-order algorithm, as we need the gradient
in order to find the direction $d$ of steepest descent
(we minimize the inner product). \\

Recall that the speed of descent at $\theta^{(k)}$
in some direction $d$ is the directional
derivative:
\[ \frac{f(\theta^{(k)} + t d) - 
f(\theta^{(k)})}{|t|} = 
\langle \nabla f(\theta^{(k)}), 
d \rangle + \frac{o(|t|)}{|t|} \]
We can then solve for the direction $d^{(k)}$
that maximizes the descent speed:
\[ \min_{d \in \mathbb{R}^d} 
\langle \nabla f(\theta^{(k)}), 
d \rangle \quad \SUCHTHAT \quad \|d\| = 1\]
\[ d^{(k)} = -\nabla f(\theta^{(k)}) \]
So this is the direction of steepest descent,
and it is given by the gradient. \\

We choose some step length $\tau$ to move in the direction
of steepest descent:
\[ \theta^{(k+1)} = \theta^{(k)} -\tau \nabla f(\theta^{(k)}) \]
We repeat these steps until the error is acceptable. \\

We choose some starting point $\theta^{(0)}$. \\

A heuristic for the step size is $\tau = \dfrac{2}{L + m}$. \\

The update step is thus:
\[ \theta^{(k+1)} = \theta^{(k)} - \tau 
\nabla f(\theta^{(k)})
\quad \WHERE \quad \nabla 
f(\theta^{(k)}) = \Phi^\top 
(\Phi \theta^{(k)} - y)\]
For $k \geq 1$. \\

The convergence rate for the $\tau$
we heuristically set is:
\[ \|\theta^{(k)} - \theta^*\| 
\leq \omega^k \|\theta^{(0)} - \theta^*\| 
= O(\omega^k) \quad \WHERE
\quad \omega = \frac{L/m - 1}{L/m + 1} \]
We have a linear rate of convergence, 
where $\omega^k$
decreases since $\omega < 1$. \\
Note that the convergence rate we have
here does depend on the step size, but here the formula
assumes that we use $\dfrac{2}{L + m}$. \\

The condition number $\kappa = \dfrac{L}{m}$
impacts convergence. \\

The idea behind gradient-descent is that we are making
locally optimal decisions that lead us towards the 
optimal solution. If we use a bad step size, we get a
zig-zag behavior that slows down convergence. \\

\newpage

\subsection*{Conjugate Gradient Method}

This method was first created to solve linear systems
\[ Q\theta = b \]
where $Q$ is positive semi-definite and $b \in \rbb^d$. \\

This can be shown to be equivalent to solving
the quadratic minimization problem:
\[ \theta \in \mathbb{R}^d 
\quad \frac{1}{2} \langle \theta 
Q \theta \rangle - \langle b, \theta \rangle \]
This will help us accelerate gradient descent
but only using first order information. \\

Our problem is equivalent to the form
we showed above, with $Q = \Phi^\top \Phi$ and 
$b = \Phi^\top y$:
\[ \frac{1}{2} \|\Phi \theta - y\|^2 
= \frac{1}{2} \langle \theta 
\Phi^\top \Phi \theta \rangle 
- \langle \theta, \Phi^\top y 
\rangle + \frac{1}{2} \|y\|^2 \]
Which is the same form. \\

The idea behind the algorithm that minimizes
this is to move in directions that are conjugates,
which ensures faster convergence. \\

For the first point $\theta^{(0)}$,
we start with a direction:
\[ d^{(0)} = r^{(0)} = b - Q \theta^{(0)}\]
Then the update step
updates the direction $d^{(k)}$ 
and $\theta^{(k)}$:
\[ \alpha_k = \frac{\langle r^{(k)}, r^{(k)} 
\rangle}{\langle d^{(k)}, Q d^{(k)} \rangle} 
\quad \text{(optimal step size)}\]
\[ \theta^{(k+1)} = \theta^{(k)} + \alpha_k 
d^{(k)} \quad \text{(main update step)} \]
\[ r^{(k+1)} = r^{(k)} - \alpha_k Q d^{(k)} 
\quad \text{(compute gradient)} \]
\[ \beta_{k+1} = \frac{\langle r^{(k+1)}, 
r^{(k+1)} \rangle}{\langle r^{(k)}, r^{(k)} 
\rangle} \quad \text{(projection coefficient)} \]
\[ d^{(k+1)} = r^{(k+1)} + \beta_{k+1} 
d^{(k)} \quad \text{(project onto Q-conjugate 
direction)} \] \\
The idea is that we generate directions for our
descent that are optimal in order to get to the solution
in an optimal number of iterations. \\

The generated directions span the whole space.
We can show the chosen directions are optimal. \\

The convergence rate is:
\[ \|\theta^{(k)} - \theta^*\| \leq 2 
\sqrt{\frac{L}{m}} \, \omega^k \|\theta^{(0)} 
- \theta^*\| = O(\omega^k) 
\quad \WHERE \quad \omega = 
\frac{\sqrt{L/m} - 1}{\sqrt{L/m} + 1} \]
The convergence rate is also linear, but it is much faster
since $\sqrt{\dfrac{L}{m}}$ is much smaller 
than $\dfrac{L}{m}$. \\
\newpage

\subsection*{Gradient Descent Generalization}

We can't generalize the conjugate graident method
for non-quadratic methods. \\
There are non-linear generalizations, 
but they won't be nearly as powerful. \\

We can however, generalize Gradient Descent
for any problem, since it only needs
to compute the gradient. \\

The goal will be to minimize any function $f$:
\[ \min_{\theta \in \mathbb{R}^d} f(\theta) \]
where $f$ is convex and continuously 
differentiable. \\

The first algorithm that can do that is called
gradient descent with line search. \\
The goal is to find a step size $\tau_k > 0$
each step such that the update step:
\[ \theta^{(k+1)} = \theta^{(k)} 
- \tau_k \nabla f(\theta^{(k)})\]
satisfies the Armijo condition for $\gamma$:
\[ f(\theta^{(k+1)}) \leq f(\theta^{(k)}) 
- \gamma \tau_k \|\nabla f(\theta^{(k)})\|^2 \]
however, we avoid it because it is too
costly; it has no convergence rate because it
does not have a constant step size. \\

The other methos is to 
bound the function at each point
by two quadratics, one below and one above. \\

We will make use of two properties
a function can have. \\

The first is $L$-smoothness,
which is the same as saying the gradient
of the function $\nabla f$ is $L$-Lipschitz
continuous:
\[ \|\nabla f(\theta) - \nabla f(\bar{\theta})\| 
\leq L \|\theta - \bar{\theta}\| 
\quad \forall \theta, \bar{\theta}\]
This will be a necessary condition. \\

The second condition is that $f$
is $m$-strongly convex, which means that:
\[ f(m) - \dfrac{m}{2} \|\theta\|^2 \]
is convex (we assume that $m \geq 0$). \\
This condition is not necessary, but gives
desirable results. \\

If we assume the first condition
is true for all $\olsi{\theta} \in \rbb^d$,
then by the descent lemma:
\[ \left| f(\theta) - \left( f(\bar{\theta}) 
+ \langle \nabla f(\bar{\theta}), \theta - 
\bar{\theta} \rangle \right) \right| 
\leq \frac{L}{2} \|\theta - \bar{\theta}\|^2 
\quad \forall \theta \in \mathbb{R}^d \]
Which we can reformulate as:
\[ f(\bar{\theta}) + \langle \nabla 
f(\bar{\theta}), \theta - \bar{\theta} 
\rangle - \frac{L}{2} \|\theta - 
\bar{\theta}\|^2 \leq f(\theta) \leq 
\underbrace{
f(\bar{\theta}) + \langle \nabla f(\bar{\theta}), 
\theta - \bar{\theta} \rangle + \frac{L}{2} 
\|\theta - \bar{\theta}\|^2}
_{=: f_L(\theta; \bar{\theta})}\]
This gives us an upper and lower bound at
each point $\olsi{\theta}$ that are both
quadratic functions. \\

Now what this means is that we can use
gradient descent to minimize any function
$f$, so long as $\nabla f$
is $L$-Lipschitz continuous,
and $f$ is at least convex. \\
We again choose any $\theta^{(0)}$
to start. \\
We don't need to do line search for the step
size, and can instead just use a fixed
size, so long as
\[ 0 < \tau < \dfrac{2}{L}\]
which yields the same update step as before:
\[ \theta^{(k+1)} = \theta^{(k)} 
- \tau \nabla f(\theta^{(k)}) \]
Which now works for any function $f$
so long as it is convex and $\nabla f$
is $L$-Lipschitz continuous. \\

This will give us a sub linear convergence rate:
\[ f(\theta^{(k)}) - \inf f \leq 
\frac{\|\theta^{(0)} - \theta^*\|^2}{2 \tau k} 
= O\left(\frac{1}{k}\right) \]
When we assume that $f$ is convex. \\

On the other hand, if we further assume
that $f$ is $m$-strongly convex, then
we get a linear rate of convergence
to a global minimizer $\theta^*$:
\[ \|x^{(k)} - x^*\|^2 \leq \omega^k 
\|x^{(0)} - x^*\|^2 = O(\omega^k) 
\quad \WHERE \quad \omega = 1 - \tau m \]
Which can be improved to:
\[ \omega = \frac{L/m - 1}{L/m + 1}\]
If we further assume that
$\tau = \dfrac{2}{L + m}$. \\

Note that if the original function $f$ was
not convex, then it is bounded from 
above by a convex quadratic (cup),
and from below by a concave quadratic (cap). \\

If the original function $f$ was
convex, which we assume it is by default,
then it is bounded from above by a quadratic,
and from below by a linear function. \\

And if $f$ is strongly convex, then it is
bounded from above and below by quadratics
that are both convex (cups)
instead of having the lower bound turned down. \\

\newpage


\end{document}
