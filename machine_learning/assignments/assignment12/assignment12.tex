

\documentclass[12pt]{article}

\usepackage[margin=1in]{geometry}

\input{../../../variables.tex}

\title{
    \Huge Machine Learning \\
    \Large Assignment XI
}
\date{2025-07-03}
\author{Michael Saba}

\begin{document}
\pagenumbering{gobble}
\maketitle
\newpage
\setlength{\parindent}{0pt}
\pagenumbering{arabic}

\subsection*{Ex 1}

\begin{enumerate}[label = \letters]
    \item
    We have a unitary graph $(\vcal, \ecal, w)$ where:
    \[ \vcal = \pmat{1 & 2 & 3 & 4 & 5} \]
    \[ \ecal = \pmat{(1, 2) & (2,3) & (3, 4) & 
    (4, 5) & (5, 1), (2, 4)} \]
    \[ w_{ij} = \piecewise{
        1 \quad \IF (i, j) \in \ecal \\
        0 \quad \text{otherwise}       
    }  \]
    \tikzGraphic{
        \node[circle, draw, inner sep=2pt] (1) at (90:1) {1};
        \node[circle, draw, inner sep=2pt] (2) at (162:1) {2};
        \node[circle, draw, inner sep=2pt] (3) at (234:1) {3};
        \node[circle, draw, inner sep=2pt] (4) at (306:1) {4};
        \node[circle, draw, inner sep=2pt] (5) at (18:1) {5};
        \draw (1) -- (2);
        \draw (2) -- (3);
        \draw (3) -- (4);
        \draw (4) -- (5);
        \draw (5) -- (1);
        \draw (2) -- (4);      
    }
    \item 
    Here, we have unitary weights between nodes,
    so the weight $w_{ij} = 1$ where there is an edge
    between node $i$ and $j$, and a weight of $0$
    otherwise:
    \[ W = \bmat{
        0 & 1 & 0 & 0 & 1 \\
        1 & 0 & 1 & 1 & 0 \\
        0 & 1 & 0 & 1 & 0 \\
        0 & 1 & 1 & 0 & 1 \\
        1 & 0 & 0 & 1 & 0 \\
    } \]
    The diagonal matrix has the sums of the weights
    connecting to each node:
     \[ D = \bmat{
        2 & 0 & 0 & 0 & 0 \\
        0 & 3 & 0 & 0 & 0 \\
        0 & 0 & 3 & 0 & 0 \\
        0 & 0 & 0 & 3 & 0 \\
        0 & 0 & 0 & 0 & 2 \\
    } \]
    So the Graph Laplacian is:
    \[ \Delta = D - W = \bmat{
        2 & -1 & 0 & 0 & -1 \\
        -1 & 3 & -1 & -1 & 0 \\
        0 & -1 & 2 & -1 & 0 \\
        0 & -1 & -1 & 3 & -1 \\
        -1 & 0 & 0 & -1 & 2 \\
    } \]
    \item 
    The eigenvalues, which are calculated
    in Python, are:
    \[ \pmat{0 & 1.382 & 2.382 & 3.618 & 4.618} \]
    \item 
    Since there are $5$ eigenvalues, and the 
    eigenvalue $0$ appears once, it means that it has a 
    multiplicity of $1$. \\
    What this means is that the graph has one connected
    region, which looking at the graph, seems true.
    \item 
    The eigenvector of the first non-zero eigenvalue
    is that of $1.382$, which is
    (calculated in Python):
    \[ v = \bmat{ 0.512 & -0.195 & -0.632 & -0.195 & 0.512} \]
    Thresholding it at $t = 0$ allows us to group
    vertices into two clusters.
    Vertex $i$ goes into $\csf_{1}$
    if $v_i \geq 0$, and it goes into $\csf_{2}$
    otherwise:
    \[ \csf_1 = \{ 1, 5 \}
    \qquad \csf_2 = \{2, 3, 4\} \]
    \item 
    We know that:
    \[\text{RatioCut}(\csf_1, \dots, \csf_K) = 
    \sum_{i=1}^{K} \dfrac{
    \text{cut}(\csf_i, \bar{\csf}_i)}{|\csf_i|} \]
    And that in our case:
    \[\text{RatioCut}(\csf_1, \csf_2) = 
    \dfrac{\text{cut}(\csf_1, \csf_2)}{|\csf_1|}
    +  \dfrac{\text{cut}(\csf_1, \csf_2)}{|\csf_2|} \]
    Where $|\csf|$ is the number of elements in a cluster 
    $\csf$. \\
    Also, $\text{cut}(\csf_1, \csf_2)$
    is the sum of weights connecting $\csf_1$
    to $\csf_2$. \\
    We want to find the partition that optimizes
    (minimizes) the RatioCut.
    We can pick thresholds that change the way
    the elements are grouped into clusters, and then
    pick the one with the lowest RatioCut:
    \[ v = \bmat{ 0.512 & -0.195 & -0.632 & -0.195 & 0.512} \]
    There are four thresholds we can pick,
    which yield different clusterings:
    \multiline{ t_2 < -0.632 \qquad &\implies \qquad
    \csf_1 = \{1, 2, 3, 4, 5\} \AND \csf_2 = \emptyset \\
    -0.632 < t_2 < -0.195 \quad &\implies \qquad
    \csf_1 = \{1, 2, 4, 5\} \AND \csf_2 = \{3\} \\ 
    -0.195 < t_3 < 0.512 \quad &\implies \qquad
    \csf_1 = \{1, 5\} \AND \csf_2 = \{2, 3, 4\} \\   
    0.512 < t_4 \quad &\implies \qquad
    \csf_1 = \emptyset \AND \csf_2 = \{1, 2, 3, 4, 5\} \\ }
    We can ignore the cases where we only have
    one cluster, since it will have a RatioCut of $0$:
    \[ t_2 \implies \text{RatioCut}(\csf_1, \csf_2)
    = \dfrac{2}{1} + \dfrac{2}{4} = 2.5 \]
    \[ t_3 \implies \text{RatioCut}(\csf_1, \csf_2)
    = \dfrac{2}{3} + \dfrac{2}{2} \approx 1.666 \]
    So $t_3$ is the optimal threshold as it is the 
    smallest. \\
    \item We just draw it.
    \item No.
\end{enumerate}

\colorText{red}{
    \subsection*{Correction}
    The eigenvector he got was different,
    but the principle is still the same,
    and the clustering is the same. \\
}

\newpage

\subsection*{Ex 2}
\begin{enumerate}[label = \letters]
    \item 
    As mentioned earlier, when we have an optimization
    problem:
    \[ \min_{\theta \in \Theta} f(\theta) \]
    In order for it to be a convex problem,
    we need $f$ to be a convex function,
    and $\Theta$ to be a convex set. \\
    Here we have the problem:
    \[ \min_{U, A} \quad \frac{1}{n} \| X - U A \|_F^2 \quad 
    \text{such that} \quad U^\top U = I \]
    It is not a convex optimization problem because
    the set:
    \[ C = \{U \in \rbb^{d \times m} \mid U^TU = I\} \]
    is not a convex set (The set of all orthogonal
    matrices). \\
    Treating the matrices as large vectors in 
    $\rbb^{d \times m}$, we can define a line going 
    from one matrix to another in $C$ as:
    \[ [X, Y] = 
    \{ (1 - \lambda) X + \lambda Y \in 
    \mathbb{R}^{d \times m} \mid \lambda \in [0,1] \} \]
    However, notice that, for matrices:
    \[ X = \bmat{I_m \\ 0} \AND Y = \bmat{-I_m \\ 0} \]
    Or (depending on whether $d \geq m$ or not):
    \[ X = \bmat{I_d & 0} \AND Y = \bmat{-I_d & 0} \]
    Notice that $X^TX = Y^TY = I$, so $X, Y \in C$,
    but the midpoint of the line connecting them,
    that is, when $\lambda = 5$,
    is not, since it is the $0$ matrix,
    which is not orthogonal. \\
    So the set $C$ is not convex, which means that
    the problem is not. \\
    \item 
    Now, if we fix $U$, then the optimization
    problem no longer has a constraint, and is just:
    \[ \min_{A} \quad \frac{1}{n} \| X - U A \|_F^2 \]
    Which means that we just need to make
    sure that the function:
    \[ f(A) = \frac{1}{n}\| X - U A \|_F^2 \]
    is convex. \\
    This is a quadratic function, so the answer is yes. \\
    We can also derive the second derivative
    (using the derivative of the Frobenius norm):
    \[ \partial_A f = -\dfrac{2}{n}U^T(X - UA) \]
    \[ \partial_{AA} f = \dfrac{2}{n}U^TU \]
    Here this matrix is clearly positive-semi
    definite since $U^TU$ is obviously positive-semi
    definite and $\dfrac{2}{n} > 0$. \\ 
    We can compute the optimal $A*$
    by setting the derivative to $0$:
    \[ -\dfrac{2}{n}U^T(X - UA) = 0 \]
    \[ \dfrac{2}{n}U^TUA = \dfrac{2}{n}U^TX \]
    \[ A^* = (U^TU)\inv U^TX \]
    \item 
    Plugging it back in, we have:
    \[ \min_{U} \quad \frac{1}{n} \| X - 
    U(U^TU)\inv U^TX \|_F^2 \quad 
    \text{such that} \quad U^\top U = I \]
    Since $U^TU = I$, then:
    \[ \min_{U} \quad \frac{1}{n} \| X - 
    UU^TX \|_F^2 \quad 
    \text{such that} \quad U^\top U = I \]
    Here $UU^TX$ is the projection of the points
    onto the subspace of dimension $n$,
    expressed as a linear combination of
    basis vectors (columns of $U$).
    \item 

\end{enumerate}

\colorText{red}{
    \subsection*{Correction}
    \begin{enumerate}[label = \letters]
        \item 
        We can show that either the
        objective is not a convex function,
        or the constraint is not a convex set. \\
        To show that the objective is not convex,
        we can just show that Jensen's inequality
        doesn't hold. \\
        For both cases, we can just use simple exmaples,
        since one counterexample is enough.
        It's usually easiest to choose $\lambda 
        = \sfrac{1}{2}$.
        \item 
        We can use the trace definition of the
        matrix norm in order to derive its derivative,
        since the derivative of the trace
        is the trace of the derivative (linear). \\
        Also we can use the property that
        $U^TU = I$ in this part, and not wait
        till the next part.
        \item 
        It's an orthogonal projection. \\
        Here $U^TX$ is the points in the subspace
        $\ucal$ expressed in the basis $U$
        (so they are $m$-dimensional).
        Then $UU^TX$ mutliplies this vector,
        so we get a linear combination of the 
        columns of $U$, which gives us the projection
        in $\ucal$ expressed in the original basis.
        \item 
        
    \end{enumerate}
}

\newpage

\subsection*{Ex 3}
\begin{enumerate}[label = \letters]
    \item 
    The covariance matrix is:
    \[ \hat{\Sigma} = \dfrac{1}{n}XX^T
    = \dfrac{1}{4}\bmat{8 & 0 \\ 0 & 8}
    = \bmat{2 & 0 \\ 0 & 2} \]
    \item 
    The eigenvalues are given by:
    \[ \det(\hat{\Sigma} - \lambda I) = 0 \]
    \[ (2-\lambda)^2 = 0 \]
    \[ \lambda = 2 \]
    So the eigenvectors are:
    \[ v_1 = \bmat{0 \\ 1} \qquad v_2 = \bmat{1 \\ 0} \]
    \item 
    Either $v_1$ or $v_2$ can be the principal component.
    Usually the $m$ principal components
    for a reduction to an $m$-dimensional
    are the eigenvectors of $m$ largest eigenvalues,
    which will become teh basis vectors (columns of $U$). \\
    In this case we want to do a reduction to one dimension,
    so we only have a single eigenvector. \\ 
    \item 
    To project the points, we do $A = U^TX$,
    that is:
    \[ \bmat{0 & 1} \bmat{2 & 0 & -2 & 0 \\
    0 & 2 & 0 & -2} = \bmat{0 & 2 & 0 & -2} \]
    The result is the matrix with one dimensional
    columns for the points in the $\ucal$ subspace. \\
    We would have also picked the other eigenvector. \\
\end{enumerate}

\colorText{red}{
    \subsection*{Correction}
    We can always use the fact that for a diagonal
    matrix, th eiegnevalues are just the
    diagonal entries. \\
}


\end{document}