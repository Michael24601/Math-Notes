

\documentclass[12pt]{article}

\usepackage[margin=1in]{geometry}

\input{../../../variables.tex}

\title{
    \Huge Machine Learning \\
    \Large Assignment IX
}
\date{2025-06-13}
\author{Michael Saba}

\begin{document}
\pagenumbering{gobble}
\maketitle
\newpage
\setlength{\parindent}{0pt}
\pagenumbering{arabic}

\subsection*{Ex 1}

\begin{enumerate}[label = \letters]
    \item 
    First we need to show that the Huber Function
    $f: \rbb \to \rbb$:
    \[ f(\theta) = \piecewise{
        \dfrac{\theta^2}{2\eps} \quad & \IF 
        |\theta| < \eps \vs{10} \\ 
        |\theta| - \dfrac{\eps}{2} \quad & \ELSE
    } \]
    Now, to show that $f(\theta)$
    is convex, we need to show that all of its
    pieces are convex, and that $f$ is continuous
    and differentiable at the boundaries
    between the pieces, that is at $|\theta| = \eps$. \\
    To start, both pieces of $f(\theta)$
    are diffrentiable on their domain,
    so we can use the second derivative test
    to show they are convex:
    \[ \partial_{\theta \theta} \dfrac{\theta^2}{2\eps}
    = \partial_{\theta} \dfrac{\theta}{\eps} 
    = \dfrac{1}{\eps} \]
    And:
    \[ \partial_{\theta \theta} \para{ |\theta| 
    - \dfrac{\eps}{2}}
    = \partial_{\theta} \sign(\theta) = 0 \]
    Both are non-negative, so both functions are convex. \\
    Now let's look at the boundaries:
    \begin{enumerate}[label = \numbers]
        \item For $\theta = \eps$: \\
        We have:
        \[ \limit{\theta}{\eps^-}{f(\theta)}
        = \dfrac{\eps^2}{2\eps} = \dfrac{\eps}{2} \]
        \[ \limit{\theta}{\eps^+}{f(\theta)}
        = \eps - \dfrac{\eps}{2} = \dfrac{\eps}{2} \]
        \[ f(\eps) = \eps - \dfrac{\eps}{2} 
        = \dfrac{\eps}{2} \]
        Since the limits from both sides are equal
        to $f(\eps)$, the function is continuous
        at $\theta = \eps$. \\
        It is also differentiable since the
        derivative of $f$ at $\eps$ 
        from the left and right side are equal:
        \[ f'(\eps, -1) = 
        \para{\dfrac{\theta^2}{2\eps}}'(\eps)
        = \dfrac{\eps}{\eps} = 1 \] 
        \[ f'(\eps, +1) = \para{|\theta| - 
        \dfrac{\eps}{2}}'(\eps) = \sign(\eps) = 1 \]
        Since both are equal,
        the function is continuous
        at $\theta = \eps$.
        \item For $\theta = -\eps$:
        We have:
        \[ \limit{\theta}{\eps^-}{f(-\theta)}
        = |-\eps| - \dfrac{\eps}{2} = \dfrac{\eps}{2} \]
        \[ \limit{\theta}{\eps^+}{f(-\theta)}
        = \dfrac{(-\eps)^2}{2\eps} = \dfrac{\eps}{2} \]
        \[ f(-\eps) = |-\eps| - \dfrac{\eps}{2} 
        = \dfrac{\eps}{2} \]
        Since the limits from both sides are equal
        to $f(-\eps)$, the function is continuous
        at $\theta = \eps$. \\
        It is also differentiable since the
        derivative of $f$ at $-\eps$ 
        from the left and right side are equal:
        \[ f'(-\eps, -1) = \para{|\theta| - 
        \dfrac{\eps}{2}}'(-\eps) = \sign(-\eps) = -1 \]
        \[ f'(-\eps, +1) = 
        \para{\dfrac{\theta^2}{2\eps}}'(-\eps)
        = -\dfrac{\eps}{\eps} = -1 \] 
        Since both are equal,
        the function is continuous
        at $\theta = -\eps$.
    \end{enumerate}
    So $f$ is convex.
    \item 
    We want to find the dual of $f$,
    which means calculating:
    \[ f^*(\xi) = \sup_\theta 
    \brac{ \ang{\xi, \theta} - f(\theta)} \]
    Now, since $f(\theta)$ is convex, that
    means that $-f(\theta)$ is concave,
    which means that:
    \[ \ang{\xi, \theta} - f(\theta) \]
    is concave. \\
    And since it is differentiable everywhere,
    that means we can find the supremum, or maximum
    in this case, by differentiating and setting the
    derivative equal to $0$:
    We consider $3$ seperate cases:
    \begin{enumerate}[label = \numbers]
        \item If $|\theta| \leq \eps$:
        \[ \partial_\theta 
        \brac{ \ang{\xi, \theta} - f(\theta)} \]
        \[ \partial_\theta \brac{ \ang{\xi, \theta} 
        - \dfrac{\theta^2}{2\eps}} \]
        \[ \xi - \dfrac{\theta}{\eps} \]
        Setting it equal to $0$:
        \[ \xi - \dfrac{\theta}{\eps} = 0 \]
        \[ \theta = \eps \xi \]
        This is valid for all $|\theta| \leq \eps$,
        meaning $|\eps\xi| \leq \eps$,
        meaning that $|\xi| \leq 1$.
        \item If $\theta > \eps$,
        notice that the function becomes linear,
        with a constant derivative:
        \[ \partial_\theta 
        \brac{ \ang{\xi, \theta} - f(\theta)} \]
        \[ \partial_\theta \brac{ \ang{\xi, \theta} 
        - \theta + \dfrac{\eps}{2}} \]
        \[ \xi - 1 \]
        This means that the function will
        only have a maximum
        on this interval if the linear function
        was increasing, which gives us a maximum
        which we can reach at infinity $\theta = \infty$.
        This happens if
        the derivative is positive,
        that is, if $\xi - 1 > 0$,
        that is $\xi > 1$. \\
        \item If $\theta < -\eps$,
        notice that the function becomes linear,
        with a constant derivative:
        \[ \partial_\theta 
        \brac{ \ang{\xi, \theta} - f(\theta)} \]
        \[ \partial_\theta \brac{ \ang{\xi, \theta} 
        + \theta + \dfrac{\eps}{2}} \]
        \[ \xi + 1 \]
        This means that the function will
        only have a maximum
        on this interval if the linear function
        was decreasing, which gives us a maximum
        at minus infinity $\theta = -\infty$.
        This happens if
        the derivative is negative,
        that is, if $\xi + 1 < 0$,
        that is $\xi < -1$. \\
    \end{enumerate}
    So, we have:
    \[ \theta^* = \piecewise{
        \eps \xi \quad &\IF |\xi| \leq 1 \\
        \infty \quad &\ELSE \IF \xi > 1 \\
        -\infty \quad &\ELSE \IF \xi < -1 \\
    } \]
    So replacing that into the conjuagte, we get:
    \[ f^*(\xi) = \piecewise{
        \ang{\xi, \eps\xi} - \dfrac{\eps^2\|\xi\|^2}{2\eps}
        = \dfrac{\eps\|\xi\|^2}{2} 
        \quad &\IF |\xi| \leq 1 \\
        \infty \IF |\xi| > 1
    } \]
    Which we can write as:
    \[\dfrac{\eps}{2}\|\xi\|^2 + \delta_{B_1(0)}(\xi) \]
    Where $B_1(0) = \|\cdot\|_2 \leq 1$.
    \item 
    Usually, when we want to optimize a regularized
    empirical risk, we use proximal gradient descent,
    with the proximal mapping applies to the 
    regularization term. But in this case, the Huber
    function is not prox-friendly. This is different
    from its dual:
    \[ f^*(\xi) = \frac{\eps}{2} \|\xi\|^2 
    + \delta_{B_1(0)}(\xi)\]
    Which has a simple proximal mapping (a projection
    onto the unit sphere). 
\end{enumerate}

\colorText{red}{
    \subsection*{Correction}
    
    \begin{enumerate}[label = \letters]
        \item 
        \item She did a special case for $|\theta| = 1$,
        but I included it in the first case 
        $|\theta| \leq 1$, which we can do.
        \item We should probably say that it is
        useful for regularization as it
        imposes certain restrictipns on the
        term $\xi$, by constraining it to the unit ball
        (the second term).
    \end{enumerate}
}

\newpage

\subsection*{Ex 2}

We have the margin-based logistic loss function:
\[ \tilde{\ell}(t) = \log(1 + e^{-t}) \]
And we want to find the conjugate:
\[ \tilde{\ell}^*(s) = \sup_{t}\brac{\ang{s, t} 
- \tilde{\ell}(t)} \]
Notice that this function is convex.
We can show that it is convex by doing the second
derivative test (since $\tilde{\ell}$
is twice differentiable):
\[\partial_{tt}\log(1 + e^{-t}) \]
\[\partial_{t} \para{\dfrac{-e^{-t}}{1 + e^{-t}}} \]
\[\dfrac{e^{-t}(1 + e^{-t}) - (-e^{-t})(-e^{-t})}
{(1 + e^{-t})^2}\]
\[\dfrac{e^{-t}}{(1 + e^{-t})^2} \geq 0 \]
Now, since $\tilde{\ell}(t)$
is convex, that means that $-\tilde{\ell}(t)$
is concave. \\
This means that the following function is concave
(with respect to $t$):
\[ \ang{s, t} - \tilde{\ell}(t) \]
This is because the first term is concave
(all linear functions are convex and concave
at once), and the second term is concave,
and the sum of two concave functions is concave. \\
Now a concave function is known to always have
a unique global maximizer, and since 
$\ang{s, t} - \tilde{\ell}(t)$ is differentiable,
we can find it by differentiating and setting
the derivative equal to $0$:
\[ \partial_t\para{\ang{s, t} - \log(1 + e^{-t})} \]
\[ s - \dfrac{-e^{-t}}{1 + e^{-t}} \]
\[ s + \dfrac{1}{e^{t} + 1} \]
Now setting it equal to $0$, we get:
\[ s + \dfrac{1}{e^t + 1} = 0\]
\[ -s = \dfrac{1}{e^t + 1} \]
\[ -\dfrac{1}{s} = e^t + 1 \]
\[ -\dfrac{1}{s} - 1 = e^t \]
\[ \log\para{-\dfrac{1}{s} - 1} = t \]
\[ \log\para{\dfrac{-1 - s}{s}} = t \]
We can plug this optimal $t$ back into
the formula to get the supremum:
\[ \tilde{\ell}^*(s)
= \ang{s, \log\para{\dfrac{-1 - s}{s}}}
- \log\para{1+ \dfrac{-1 - s}{s}} \]
This is only valid, or finite, for:
\[ \dfrac{-1 - s}{s} > 0 \]
We now need to multiply both sides by $s$,
so we consider two cases:
\begin{enumerate}
    \item If $s > 0$:
        \[ \dfrac{-1 - s}{s} > 0 \]
        \[ -1 - s > 0 \]
        \[ s < -1 \]
    \item If $s < 0$:
        \[ \dfrac{-1 - s}{s} > 0 \]
        \[ -1-s < 0 \]
        \[ s > -1 \]
\end{enumerate}
So we have the interval:
\[ s \in ((0, \infty) \cap (-\infty, -1))
\cup ((-\infty, 0) \cap (-1, \infty)) \]
\[ s \in \emptyset
\cup (-1, 0) \]
\[ s \in (-1, 0) \]
So that is the domain where it is finite. \\


\colorText{red}{
    We can just mention that outisde the range
    $(-1, 0)$, $\tilde{\ell}^*(s)$ becomes infinity.
}


\newpage

\subsection*{Ex 3}
We will assume that $k: \xcal \times \xcal \to \rbb$
is a positive definite kernel, and
will show that $k'(x, y)$ is also positive definite:
\begin{enumerate}[label = \letters]
    \item 
    For $k'(x, y) = \sum_{r=0}^\infty
    \alpha_r k(x, y)^r$ where $\alpha_r \geq 0$: \\
    The sum looks like:
    \[ \sum_{j = 1}^m\sum_{i=1}^m c_ic_j k'(x_i, x_j)
    = \sum_{j = 1}^m\sum_{i=1}^m c_ic_j 
    \brac{\sum_{r=0}^\infty \alpha_r k(x_i, x_j)^r} \]
    Which we can rewrite as:
    \[ \sum_{j = 1}^m\sum_{i=1}^m
    \brac{\sum_{r=0}^\infty \alpha_r c_ic_j k(x_i, x_j)^r} \]
    \[ \sum_{r=0}^\infty \alpha_r \brac{
     \sum_{j = 1}^m\sum_{i=1}^m
    \brac{ c_ic_j k(x_i, x_j)^r}} \]
    The inner sum is non-negative since the pointwise
    multiple of two positive definite kernels is
    a positive deinite kenrel, so
    $k(x, y)^r$ is positive definite. And
    the outer sum has positive weights, 
    so the entire term is non-negative.
    So $k'(x, y)$ is positive definite. \\
    \item 
    We have:
    \[ k'(x, y) =  e^{-\lambda(k(x, x) + k(y, y) 
    - 2k(x, y))} \]
    So it will have a gram matrix:
    \[ K_{ij} = \exp\para{-\lambda(k(x_i, x_i) 
    + k(x_j, x_j) - 2k(x_i, x_j))} \]
    \[ K_{ij} = e^{\para{-\lambda k(x_i, x_i)}}
    e^{\para{-\lambda k(x_j, x_j)}}
    e^{\para{2\lambda k(x_i, x_j)}} \]
    So the matrix can be written
    as the product of two diagonal matrices
    and a matrix:
    \[ K = \diag\para{e^{\para{-\lambda k(x_i, x_i)}}} 
    \cdot M \cdot
    \diag\para{e^{\para{-\lambda k(x_j, x_j)}}} \]
    Where:
    \[ M_{ij} = \exp\para{2\lambda k(x_i, x_j)} \]
    We know that the kernel $k$ is positive
    definite, and we know that:
    \[ e^{2\lambda k(x, y)}
    = \sum_{i=1}^{\infty} \dfrac{(2\lambda)^i}{i!}k(x, y)^i \]
    must also be a positive definite kernel,
    which we proved in part a),
    so $M$ is a positive semi-definite matrix. \\
    Finally, note that we have:
    \[ K = DMD \]
    where $D$ is diagonal, which means that:
    \[ w^TDMDw = w^TD^TMDw = (Dw)^TM(Dw)  \]
    Since $M$ is positive semi-definite, the
    above expression must be positive, so
    $DMD$ is also positive semi-definite. 
    This means that $k'$ is a positive definite 
    kernel. \\
    \item 
    For the Guassian kernel:
    \[ k'(x, y) = e^{-\dfrac{\|x - y\|^2}{2\sigma}} \]
    We can just say that this is a special case
    of part $b$ where $k(x, y) = \ang{x, y} = x^Ty$
    and $\lambda = -\dfrac{1}{2\sigma}$.
    We know that $k$ is a positive definite
    kernel since its Gram matrix can be written
    as $X^TX$. \\
\end{enumerate}

\colorText{red}{
    \begin{enumerate}[label = \letters]
        \item 
        We can combine the fact that
        the finite sum, positive scaling,
        pointwise product, and pointwise limit
        to infinity of positive definite kernels
        leads to a positive definite kernel.
        We use all four properties. \\
        What I wrote was correct, but for an easier
        time we can just use the properties from
        the slides.
        \item 
        She didn't mention anything regarding
        the exponetial terms with just $x$
        and $y$, and how they turn into diagonal
        matrices. \\ 
        Instead, she used the fact that
        $f(x)f(z)k(x, z)$ is a positive definite
        kernel if $k(x, z)$ is positive definite,
        and if $f: \xcal \to \rcal$.
        \item
    \end{enumerate}
}

\end{document}